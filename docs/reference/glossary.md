# Glossary: Physical AI & Humanoid Robotics

## A

**Action Planning**: The process of determining a sequence of actions to achieve a specific goal in robotics.

**ASR (Automatic Speech Recognition)**: Technology that converts spoken language into text, enabling voice command processing for robots.

## B

**Behavior Tree**: A hierarchical structure used to organize and control the execution of actions in robotics and AI systems.

## C

**Cognitive Architecture**: A framework that defines the structure and function of an intelligent agent's cognitive system.

## D

**Digital Twin**: A virtual representation of a physical object or system that mirrors its real-world counterpart.

## E

**Embodied AI**: Artificial intelligence systems that interact with the physical world through robotic bodies.

## F

**Forward Kinematics**: The process of determining the position and orientation of a robot's end effector based on its joint angles.

## G

**Gazebo**: A 3D simulation environment for robotics that provides realistic physics and rendering capabilities.

## H

**Humanoid Robot**: A robot with a body structure that resembles that of a human, typically featuring a head, torso, and two arms and legs.

## I

**Inverse Kinematics**: The process of determining the joint angles required to position a robot's end effector at a desired location and orientation.

**Isaac Sim**: NVIDIA's robotics simulation platform for developing and testing AI-powered robots.

## L

**LIDAR (Light Detection and Ranging)**: A sensing technology that uses laser light to measure distances and create 3D maps of the environment.

**LLM (Large Language Model)**: A type of artificial intelligence model designed to understand and generate human language.

## M

**Motion Planning**: The process of determining a sequence of movements for a robot to navigate from a start state to a goal state while avoiding obstacles.

## N

**Nav2**: The navigation stack for ROS 2, providing path planning and execution capabilities for autonomous mobile robots.

## P

**Perception Pipeline**: A sequence of processing steps that transforms raw sensor data into meaningful information for robot decision-making.

**PDDL (Planning Domain Definition Language)**: A formal language used to describe planning problems in robotics and AI.

## R

**ROS (Robot Operating System)**: A flexible framework for writing robot software that provides hardware abstraction, device drivers, libraries, and tools.

**ROS 2**: The second generation Robot Operating System providing communication infrastructure for robotic applications.

## S

**SLAM (Simultaneous Localization and Mapping)**: The computational problem of constructing or updating a map of an unknown environment while simultaneously keeping track of an agent's location within it.

**SDF (Simulation Description Format)**: An XML format for representing robot models and environments in Gazebo.

## T

**Teleoperation**: The remote control of a robot by a human operator.

## U

**URDF (Unified Robot Description Format)**: An XML format for representing robot models in ROS.

## V

**VLA (Vision-Language-Action)**: Systems that integrate visual perception, natural language understanding, and action execution for robotic applications.

**VSLAM (Visual Simultaneous Localization and Mapping)**: SLAM using visual sensors as the primary input.

## W

**Whisper**: An automatic speech recognition (ASR) system developed by OpenAI that can recognize and transcribe spoken language.

## X

**Xacro**: An XML macro language for generating URDF files with parameterized robot descriptions.

## Y

## Z