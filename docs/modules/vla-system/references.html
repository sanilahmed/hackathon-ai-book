<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-modules/vla-system/references" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.1.0">
<title data-rh="true">Vision-Language-Action (VLA) System References | Physical AI &amp; Humanoid Robotics Technical Book</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://sanilahmed.github.io/hackathon-ai-book/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://sanilahmed.github.io/hackathon-ai-book/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://sanilahmed.github.io/hackathon-ai-book/modules/vla-system/references"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Vision-Language-Action (VLA) System References | Physical AI &amp; Humanoid Robotics Technical Book"><meta data-rh="true" name="description" content="Academic Papers and Research"><meta data-rh="true" property="og:description" content="Academic Papers and Research"><link data-rh="true" rel="icon" href="/hackathon-ai-book/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://sanilahmed.github.io/hackathon-ai-book/modules/vla-system/references"><link data-rh="true" rel="alternate" href="https://sanilahmed.github.io/hackathon-ai-book/modules/vla-system/references" hreflang="en"><link data-rh="true" rel="alternate" href="https://sanilahmed.github.io/hackathon-ai-book/modules/vla-system/references" hreflang="x-default"><link rel="stylesheet" href="/hackathon-ai-book/assets/css/styles.0d3dc5f1.css">
<script src="/hackathon-ai-book/assets/js/runtime~main.0a81519c.js" defer="defer"></script>
<script src="/hackathon-ai-book/assets/js/main.53d47e2d.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/hackathon-ai-book/"><div class="navbar__logo"><img src="/hackathon-ai-book/img/logo.svg" alt="Physical AI Book Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/hackathon-ai-book/img/logo.svg" alt="Physical AI Book Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Physical AI Book</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/hackathon-ai-book/modules/ros2-nervous-system">Modules</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/sanilahmed/hackathon-ai-book" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/hackathon-ai-book/modules/ros2-nervous-system">Getting Started</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/hackathon-ai-book/modules/ros2-nervous-system">Module 1: ROS 2 Robotic Nervous System</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/hackathon-ai-book/modules/digital-twin">Module 2: Digital Twin (Gazebo + Unity)</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/hackathon-ai-book/modules/ai-robot-brain">Module 3: AI-Robot Brain (NVIDIA Isaac)</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/hackathon-ai-book/modules/vla-system">Module 4: Vision-Language-Action (VLA)</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-ai-book/modules/vla-system">Module 4: Vision-Language-Action (VLA) System</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-ai-book/modules/vla-system/vla-fundamentals">VLA Fundamentals</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-ai-book/modules/vla-system/vla-architecture">VLA Architecture</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-ai-book/modules/vla-system/multimodal-perception">Multimodal Perception</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-ai-book/modules/vla-system/language-action-mapping">Language-Action Mapping</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-ai-book/modules/vla-system/training-vla-models">Training VLA Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-ai-book/modules/vla-system/vla-integration">VLA Integration</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/hackathon-ai-book/modules/lab-exercises/lab-4-1-vla-fundamentals">Lab Exercises</a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/hackathon-ai-book/modules/vla-system/references">Vision-Language-Action (VLA) System References</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-ai-book/modules/vla-system/safety-evaluation">Safety and Evaluation</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/hackathon-ai-book/modules/lab-exercises">Reference</a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/hackathon-ai-book/roadmap">Project Roadmap &amp; Next Steps</a></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/hackathon-ai-book/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Module 4: Vision-Language-Action (VLA)</span><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Vision-Language-Action (VLA) System References</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Vision-Language-Action (VLA) System References</h1>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="academic-papers-and-research">Academic Papers and Research<a href="#academic-papers-and-research" class="hash-link" aria-label="Direct link to Academic Papers and Research" title="Direct link to Academic Papers and Research">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="foundational-vla-research">Foundational VLA Research<a href="#foundational-vla-research" class="hash-link" aria-label="Direct link to Foundational VLA Research" title="Direct link to Foundational VLA Research">​</a></h3>
<ul>
<li>Zhu, Y., Zeng, A., Joshi, S., Chen, S., Dogra, K., Welle, J., ... &amp; Tellex, S. (2022). RT-1: Robotics transformer for real-world control at scale. <em>arXiv preprint arXiv:2208.01871</em>.</li>
<li>Brohan, A., Brown, J., Carbajal, J., Chebotar, Y., Dabis, J., Finn, C., ... &amp; Zhu, Y. (2022). RVT: Robotic view transformation learning from unimodal demonstrations. <em>arXiv preprint arXiv:2209.11133</em>.</li>
<li>Chen, M., Mishra, A., Gupta, A., Devin, C., Zhu, Y., Turck, L., ... &amp; Levine, S. (2021). Learning transferable visual models from natural language supervision. <em>International Conference on Machine Learning</em>.</li>
<li>Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., ... &amp; Sutskever, I. (2021). Learning transferable visual models from natural language supervision. <em>International Conference on Machine Learning</em>.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="vision-language-models-for-robotics">Vision-Language Models for Robotics<a href="#vision-language-models-for-robotics" class="hash-link" aria-label="Direct link to Vision-Language Models for Robotics" title="Direct link to Vision-Language Models for Robotics">​</a></h3>
<ul>
<li>Radford, A., et al. (2021). Learning transferable visual models from natural language supervision. <em>Proceedings of the International Conference on Machine Learning (ICML)</em>.</li>
<li>Li, J., Li, D., Savarese, S., &amp; Hoi, S. (2022). BLIP: Bootstrapping language-image pre-training for unified vision-language understanding and generation. <em>International Conference on Machine Learning</em>.</li>
<li>Liu, J., Jia, J., Liu, Y., &amp; Tuzel, O. (2019). Visual semantic search with vision-language models. <em>arXiv preprint arXiv:1905.12384</em>.</li>
<li>Chen, X., et al. (2020). An empirical study of training end-to-end vision-and-language transformers. <em>arXiv preprint arXiv:2112.05253</em>.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="multimodal-learning-for-robotics">Multimodal Learning for Robotics<a href="#multimodal-learning-for-robotics" class="hash-link" aria-label="Direct link to Multimodal Learning for Robotics" title="Direct link to Multimodal Learning for Robotics">​</a></h3>
<ul>
<li>Misra, I., Maaten, L. V. D., Efros, A. A., &amp; He, K. (2018). Cross-modal alignment of video and language. <em>Proceedings of the European Conference on Computer Vision (ECCV)</em>.</li>
<li>Alayrac, J. B., et al. (2022). Self-supervised learning of image and video representations by predicting future visual representations. <em>arXiv preprint arXiv:2204.07152</em>.</li>
<li>Kolve, E., et al. (2017). AI2-THOR: An interactive 3D environment for visual AI. <em>arXiv preprint arXiv:1712.05474</em>.</li>
<li>Zhu, Y., et al. (2017). Target-driven visual navigation in indoor scenes using deep reinforcement learning. <em>IEEE International Conference on Robotics and Automation (ICRA)</em>.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="language-guided-robot-control">Language-Guided Robot Control<a href="#language-guided-robot-control" class="hash-link" aria-label="Direct link to Language-Guided Robot Control" title="Direct link to Language-Guided Robot Control">​</a></h3>
<ul>
<li>Hermann, K., et al. (2017). Grounded language learning in a simulated 3D world. <em>arXiv preprint arXiv:1706.06551</em>.</li>
<li>Misra, D., Lang, A., &amp; Artzi, Y. (2018). Mapping instructions and visual observations to actions with reinforcement learning. <em>Conference on Robot Learning (CoRL)</em>.</li>
<li>Chen, X., et al. (2019). Task-oriented active learning for robot-assisted dressing. <em>IEEE International Conference on Robotics and Automation (ICRA)</em>.</li>
<li>Tellex, S., et al. (2011). Understanding natural language commands for robotic navigation and mobile manipulation. <em>AAAI Conference on Artificial Intelligence</em>.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="technical-documentation">Technical Documentation<a href="#technical-documentation" class="hash-link" aria-label="Direct link to Technical Documentation" title="Direct link to Technical Documentation">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="ros-2-integration">ROS 2 Integration<a href="#ros-2-integration" class="hash-link" aria-label="Direct link to ROS 2 Integration" title="Direct link to ROS 2 Integration">​</a></h3>
<ul>
<li>ROS 2 Documentation. (2023). Robot Operating System 2. Retrieved from <a href="https://docs.ros.org/en/humble/" target="_blank" rel="noopener noreferrer">https://docs.ros.org/en/humble/</a></li>
<li>ROS 2 Navigation. (2023). Navigation2 System Documentation. Retrieved from <a href="https://navigation.ros.org/" target="_blank" rel="noopener noreferrer">https://navigation.ros.org/</a></li>
<li>ROS 2 Control. (2023). ROS 2 Control Framework. Retrieved from <a href="https://control.ros.org/" target="_blank" rel="noopener noreferrer">https://control.ros.org/</a></li>
<li>ROS 2 Safety Working Group. (2023). Safety Guidelines for ROS 2. Retrieved from <a href="https://github.com/ros-safety" target="_blank" rel="noopener noreferrer">https://github.com/ros-safety</a></li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="deep-learning-frameworks">Deep Learning Frameworks<a href="#deep-learning-frameworks" class="hash-link" aria-label="Direct link to Deep Learning Frameworks" title="Direct link to Deep Learning Frameworks">​</a></h3>
<ul>
<li>PyTorch Documentation. (2023). PyTorch: Tensors and Dynamic neural networks in Python with GPU acceleration. Retrieved from <a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener noreferrer">https://pytorch.org/docs/stable/index.html</a></li>
<li>TensorFlow Documentation. (2023). TensorFlow: Large-scale machine learning on heterogeneous systems. Retrieved from <a href="https://www.tensorflow.org/api_docs" target="_blank" rel="noopener noreferrer">https://www.tensorflow.org/api_docs</a></li>
<li>Hugging Face Transformers. (2023). State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow. Retrieved from <a href="https://huggingface.co/docs/transformers/index" target="_blank" rel="noopener noreferrer">https://huggingface.co/docs/transformers/index</a></li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="vision-language-models">Vision-Language Models<a href="#vision-language-models" class="hash-link" aria-label="Direct link to Vision-Language Models" title="Direct link to Vision-Language Models">​</a></h3>
<ul>
<li>OpenAI CLIP Documentation. (2023). Contrastive Language-Image Pre-training. Retrieved from <a href="https://github.com/openai/CLIP" target="_blank" rel="noopener noreferrer">https://github.com/openai/CLIP</a></li>
<li>BLIP Documentation. (2023). Bootstrapping Language-Image Pre-training. Retrieved from <a href="https://github.com/salesforce/BLIP" target="_blank" rel="noopener noreferrer">https://github.com/salesforce/BLIP</a></li>
<li>Vision Transformer (ViT) Documentation. (2023). An Image is Worth 16x16 Words. Retrieved from <a href="https://github.com/google-research/vision_transformer" target="_blank" rel="noopener noreferrer">https://github.com/google-research/vision_transformer</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="simulation-and-development-tools">Simulation and Development Tools<a href="#simulation-and-development-tools" class="hash-link" aria-label="Direct link to Simulation and Development Tools" title="Direct link to Simulation and Development Tools">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="isaac-sim-and-nvidia-tools">Isaac Sim and NVIDIA Tools<a href="#isaac-sim-and-nvidia-tools" class="hash-link" aria-label="Direct link to Isaac Sim and NVIDIA Tools" title="Direct link to Isaac Sim and NVIDIA Tools">​</a></h3>
<ul>
<li>NVIDIA Isaac Sim Documentation. (2023). NVIDIA Isaac Sim. Retrieved from <a href="https://docs.omniverse.nvidia.com/isaacsim/latest/" target="_blank" rel="noopener noreferrer">https://docs.omniverse.nvidia.com/isaacsim/latest/</a></li>
<li>Isaac ROS Documentation. (2023). NVIDIA Isaac ROS. Retrieved from <a href="https://nvidia-isaac-ros.github.io/" target="_blank" rel="noopener noreferrer">https://nvidia-isaac-ros.github.io/</a></li>
<li>Isaac Lab Documentation. (2023). NVIDIA Isaac Lab. Retrieved from <a href="https://isaac-sim.github.io/IsaacLab/" target="_blank" rel="noopener noreferrer">https://isaac-sim.github.io/IsaacLab/</a></li>
<li>NVIDIA Omniverse Documentation. (2023). NVIDIA Omniverse Platform. Retrieved from <a href="https://docs.omniverse.nvidia.com/" target="_blank" rel="noopener noreferrer">https://docs.omniverse.nvidia.com/</a></li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="gazebo-and-simulation">Gazebo and Simulation<a href="#gazebo-and-simulation" class="hash-link" aria-label="Direct link to Gazebo and Simulation" title="Direct link to Gazebo and Simulation">​</a></h3>
<ul>
<li>Gazebo Documentation. (2023). Gazebo Robot Simulator. Retrieved from <a href="http://gazebosim.org/" target="_blank" rel="noopener noreferrer">http://gazebosim.org/</a></li>
<li>Gazebo Garden. (2023). Gazebo Garden User Guide. Retrieved from <a href="https://gazebosim.org/api/garden/" target="_blank" rel="noopener noreferrer">https://gazebosim.org/api/garden/</a></li>
<li>Robot Web Tools. (2023). Web-based robot interfaces. Retrieved from <a href="http://robotwebtools.org/" target="_blank" rel="noopener noreferrer">http://robotwebtools.org/</a></li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="unity-and-perception">Unity and Perception<a href="#unity-and-perception" class="hash-link" aria-label="Direct link to Unity and Perception" title="Direct link to Unity and Perception">​</a></h3>
<ul>
<li>Unity Robotics Hub. (2023). Unity for Robotics. Retrieved from <a href="https://unity.com/solutions/robotics" target="_blank" rel="noopener noreferrer">https://unity.com/solutions/robotics</a></li>
<li>Unity Perception Package. (2023). Synthetic Data Generation for Computer Vision. Retrieved from <a href="https://docs.unity3d.com/Packages/com.unity.perception@latest" target="_blank" rel="noopener noreferrer">https://docs.unity3d.com/Packages/com.unity.perception@latest</a></li>
<li>Unity ML-Agents Toolkit. (2023). Unity Machine Learning Agents Toolkit. Retrieved from <a href="https://github.com/Unity-Technologies/ml-agents" target="_blank" rel="noopener noreferrer">https://github.com/Unity-Technologies/ml-agents</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="standards-and-best-practices">Standards and Best Practices<a href="#standards-and-best-practices" class="hash-link" aria-label="Direct link to Standards and Best Practices" title="Direct link to Standards and Best Practices">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="robotics-standards">Robotics Standards<a href="#robotics-standards" class="hash-link" aria-label="Direct link to Robotics Standards" title="Direct link to Robotics Standards">​</a></h3>
<ul>
<li>IEEE Standards Association. (2023). IEEE Standards for Robotics. Retrieved from <a href="https://standards.ieee.org/industry-applications/robotics/" target="_blank" rel="noopener noreferrer">https://standards.ieee.org/industry-applications/robotics/</a></li>
<li>ISO Standards for Robotics. (2023). International Organization for Standardization - Robotics Standards. Retrieved from <a href="https://www.iso.org/standards.html" target="_blank" rel="noopener noreferrer">https://www.iso.org/standards.html</a></li>
<li>ROS Enhancement Proposals (REPs). (2023). ROS Standards and Conventions. Retrieved from <a href="https://ros.org/reps/" target="_blank" rel="noopener noreferrer">https://ros.org/reps/</a></li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="ai-safety-and-ethics">AI Safety and Ethics<a href="#ai-safety-and-ethics" class="hash-link" aria-label="Direct link to AI Safety and Ethics" title="Direct link to AI Safety and Ethics">​</a></h3>
<ul>
<li>Partnership on AI. (2023). AI Safety Guidelines. Retrieved from <a href="https://www.partnershiponai.org/" target="_blank" rel="noopener noreferrer">https://www.partnershiponai.org/</a></li>
<li>IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. (2023). Ethically Aligned Design. Retrieved from <a href="https://ethicsinaction.ieee.org/" target="_blank" rel="noopener noreferrer">https://ethicsinaction.ieee.org/</a></li>
<li>Future of Life Institute. (2023). AI Safety Research. Retrieved from <a href="https://futureoflife.org/background/benefits-risks-of-artificial-intelligence/" target="_blank" rel="noopener noreferrer">https://futureoflife.org/background/benefits-risks-of-artificial-intelligence/</a></li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="software-engineering-best-practices">Software Engineering Best Practices<a href="#software-engineering-best-practices" class="hash-link" aria-label="Direct link to Software Engineering Best Practices" title="Direct link to Software Engineering Best Practices">​</a></h3>
<ul>
<li>PEP 8 - Style Guide for Python Code. (2023). Python Software Foundation. Retrieved from <a href="https://peps.python.org/pep-0008/" target="_blank" rel="noopener noreferrer">https://peps.python.org/pep-0008/</a></li>
<li>Google Python Style Guide. (2023). Google Engineering Practices. Retrieved from <a href="https://google.github.io/styleguide/pyguide.html" target="_blank" rel="noopener noreferrer">https://google.github.io/styleguide/pyguide.html</a></li>
<li>ROS 2 Developer Guide. (2023). Best practices for ROS 2 development. Retrieved from <a href="https://docs.ros.org/en/humble/The-ROS2-Project/Contributing/Developer-Guide.html" target="_blank" rel="noopener noreferrer">https://docs.ros.org/en/humble/The-ROS2-Project/Contributing/Developer-Guide.html</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="open-source-projects-and-libraries">Open Source Projects and Libraries<a href="#open-source-projects-and-libraries" class="hash-link" aria-label="Direct link to Open Source Projects and Libraries" title="Direct link to Open Source Projects and Libraries">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="vision-language-action-libraries">Vision-Language-Action Libraries<a href="#vision-language-action-libraries" class="hash-link" aria-label="Direct link to Vision-Language-Action Libraries" title="Direct link to Vision-Language-Action Libraries">​</a></h3>
<ul>
<li>OpenVLA: Open Vision-Language-Action Models. (2023). Retrieved from <a href="https://github.com/openvla/openvla" target="_blank" rel="noopener noreferrer">https://github.com/openvla/openvla</a></li>
<li>Hugging Face Transformers. (2023). State-of-the-art Natural Language Processing. Retrieved from <a href="https://github.com/huggingface/transformers" target="_blank" rel="noopener noreferrer">https://github.com/huggingface/transformers</a></li>
<li>LAVIS: A Library for Language-and-Vision Intelligence. (2023). Retrieved from <a href="https://github.com/salesforce/LAVIS" target="_blank" rel="noopener noreferrer">https://github.com/salesforce/LAVIS</a></li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="robotics-libraries">Robotics Libraries<a href="#robotics-libraries" class="hash-link" aria-label="Direct link to Robotics Libraries" title="Direct link to Robotics Libraries">​</a></h3>
<ul>
<li>MoveIt! Motion Planning Framework. (2023). Retrieved from <a href="https://moveit.ros.org/" target="_blank" rel="noopener noreferrer">https://moveit.ros.org/</a></li>
<li>PyRobot: A Software Framework for Robot Learning Research. (2023). Retrieved from <a href="https://github.com/facebookresearch/pyrobot" target="_blank" rel="noopener noreferrer">https://github.com/facebookresearch/pyrobot</a></li>
<li>Habitat-Sim: 3D Simulator for Embodied AI. (2023). Retrieved from <a href="https://aihabitat.org/" target="_blank" rel="noopener noreferrer">https://aihabitat.org/</a></li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="deep-learning-libraries">Deep Learning Libraries<a href="#deep-learning-libraries" class="hash-link" aria-label="Direct link to Deep Learning Libraries" title="Direct link to Deep Learning Libraries">​</a></h3>
<ul>
<li>Detectron2: FAIR&#x27;s Next-Generation Platform for Object Detection and Segmentation. (2023). Retrieved from <a href="https://github.com/facebookresearch/detectron2" target="_blank" rel="noopener noreferrer">https://github.com/facebookresearch/detectron2</a></li>
<li>OpenCV: Open Source Computer Vision Library. (2023). Retrieved from <a href="https://opencv.org/" target="_blank" rel="noopener noreferrer">https://opencv.org/</a></li>
<li>PyTorch Geometric: Geometric Deep Learning Extension Library for PyTorch. (2023). Retrieved from <a href="https://pytorch-geometric.readthedocs.io/" target="_blank" rel="noopener noreferrer">https://pytorch-geometric.readthedocs.io/</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="tutorials-and-learning-resources">Tutorials and Learning Resources<a href="#tutorials-and-learning-resources" class="hash-link" aria-label="Direct link to Tutorials and Learning Resources" title="Direct link to Tutorials and Learning Resources">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="vla-specific-tutorials">VLA-Specific Tutorials<a href="#vla-specific-tutorials" class="hash-link" aria-label="Direct link to VLA-Specific Tutorials" title="Direct link to VLA-Specific Tutorials">​</a></h3>
<ul>
<li>NVIDIA Isaac Lab Tutorials. (2023). Retrieved from <a href="https://isaac-sim.github.io/IsaacLab/" target="_blank" rel="noopener noreferrer">https://isaac-sim.github.io/IsaacLab/</a></li>
<li>Hugging Face Course on Vision-Language Models. (2023). Retrieved from <a href="https://huggingface.co/course/chapter7/1" target="_blank" rel="noopener noreferrer">https://huggingface.co/course/chapter7/1</a></li>
<li>PyTorch Tutorial on Vision-Language Models. (2023). Retrieved from <a href="https://pytorch.org/tutorials/beginner/transformer_tutorial.html" target="_blank" rel="noopener noreferrer">https://pytorch.org/tutorials/beginner/transformer_tutorial.html</a></li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="robotics-education">Robotics Education<a href="#robotics-education" class="hash-link" aria-label="Direct link to Robotics Education" title="Direct link to Robotics Education">​</a></h3>
<ul>
<li>Modern Robotics: Mechanics, Planning, and Control by Lynch and Park. Cambridge University Press, 2017.</li>
<li>Probabilistic Robotics by Thrun, Burgard, and Fox. MIT Press, 2005.</li>
<li>Robot Learning by Sutton and Barto. MIT Press, 2018.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="deep-learning-for-robotics">Deep Learning for Robotics<a href="#deep-learning-for-robotics" class="hash-link" aria-label="Direct link to Deep Learning for Robotics" title="Direct link to Deep Learning for Robotics">​</a></h3>
<ul>
<li>Deep Learning by Goodfellow, Bengio, and Courville. MIT Press, 2016.</li>
<li>Reinforcement Learning: An Introduction by Sutton and Barto. MIT Press, 2018.</li>
<li>Computer Vision: Algorithms and Applications by Szeliski. Springer, 2010.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="industry-applications-and-case-studies">Industry Applications and Case Studies<a href="#industry-applications-and-case-studies" class="hash-link" aria-label="Direct link to Industry Applications and Case Studies" title="Direct link to Industry Applications and Case Studies">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="commercial-vla-systems">Commercial VLA Systems<a href="#commercial-vla-systems" class="hash-link" aria-label="Direct link to Commercial VLA Systems" title="Direct link to Commercial VLA Systems">​</a></h3>
<ul>
<li>Boston Dynamics. (2023). Spot and Atlas Robots. Retrieved from <a href="https://www.bostondynamics.com/" target="_blank" rel="noopener noreferrer">https://www.bostondynamics.com/</a></li>
<li>Amazon Robotics. (2023). Robotic Systems for Warehousing. Retrieved from <a href="https://www.aboutamazon.com/company/innovation-at-amazon/amazon-robotics" target="_blank" rel="noopener noreferrer">https://www.aboutamazon.com/company/innovation-at-amazon/amazon-robotics</a></li>
<li>Fetch Robotics. (2023). Autonomous Mobile Robots. Retrieved from <a href="https://www.fetchrobotics.com/" target="_blank" rel="noopener noreferrer">https://www.fetchrobotics.com/</a></li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="research-institutions">Research Institutions<a href="#research-institutions" class="hash-link" aria-label="Direct link to Research Institutions" title="Direct link to Research Institutions">​</a></h3>
<ul>
<li>MIT Computer Science and Artificial Intelligence Laboratory (CSAIL). (2023). Robotics Research. Retrieved from <a href="https://www.csail.mit.edu/" target="_blank" rel="noopener noreferrer">https://www.csail.mit.edu/</a></li>
<li>Stanford AI Lab. (2023). Vision and Learning Research. Retrieved from <a href="https://ai.stanford.edu/" target="_blank" rel="noopener noreferrer">https://ai.stanford.edu/</a></li>
<li>Google AI Robotics. (2023). Learning for Robotics. Retrieved from <a href="https://ai.google/research/teams/brain/robotics/" target="_blank" rel="noopener noreferrer">https://ai.google/research/teams/brain/robotics/</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="evaluation-and-benchmarking">Evaluation and Benchmarking<a href="#evaluation-and-benchmarking" class="hash-link" aria-label="Direct link to Evaluation and Benchmarking" title="Direct link to Evaluation and Benchmarking">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="robotics-benchmarks">Robotics Benchmarks<a href="#robotics-benchmarks" class="hash-link" aria-label="Direct link to Robotics Benchmarks" title="Direct link to Robotics Benchmarks">​</a></h3>
<ul>
<li>RoboCup@Home. (2023). Benchmarking Service and Domestic Robots. Retrieved from <a href="https://athome.robocup.org/" target="_blank" rel="noopener noreferrer">https://athome.robocup.org/</a></li>
<li>Amazon Picking Challenge. (2023). Robotic Manipulation Benchmark. Retrieved from <a href="https://amazonpickingchallenge.org/" target="_blank" rel="noopener noreferrer">https://amazonpickingchallenge.org/</a></li>
<li>ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks. (2023). Retrieved from <a href="https://askforalfred.com/" target="_blank" rel="noopener noreferrer">https://askforalfred.com/</a></li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="vision-language-benchmarks">Vision-Language Benchmarks<a href="#vision-language-benchmarks" class="hash-link" aria-label="Direct link to Vision-Language Benchmarks" title="Direct link to Vision-Language Benchmarks">​</a></h3>
<ul>
<li>COCO Dataset. (2023). Common Objects in Context. Retrieved from <a href="https://cocodataset.org/" target="_blank" rel="noopener noreferrer">https://cocodataset.org/</a></li>
<li>VQA Dataset. (2023). Visual Question Answering. Retrieved from <a href="https://visualqa.org/" target="_blank" rel="noopener noreferrer">https://visualqa.org/</a></li>
<li>NLVR2 Dataset. (2023). Natural Language for Visual Reasoning. Retrieved from <a href="https://lil.nlp.cornell.edu/nlvr/" target="_blank" rel="noopener noreferrer">https://lil.nlp.cornell.edu/nlvr/</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="safety-and-security">Safety and Security<a href="#safety-and-security" class="hash-link" aria-label="Direct link to Safety and Security" title="Direct link to Safety and Security">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="safety-standards">Safety Standards<a href="#safety-standards" class="hash-link" aria-label="Direct link to Safety Standards" title="Direct link to Safety Standards">​</a></h3>
<ul>
<li>ISO 10218-1:2011 - Robots and robotic devices — Safety requirements for industrial robots. International Organization for Standardization.</li>
<li>ISO 13482:2014 - Robots and robotic devices — Safety requirements for personal care robots. International Organization for Standardization.</li>
<li>IEC 61508: Functional safety of electrical/electronic/programmable electronic safety-related systems. International Electrotechnical Commission.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="security-guidelines">Security Guidelines<a href="#security-guidelines" class="hash-link" aria-label="Direct link to Security Guidelines" title="Direct link to Security Guidelines">​</a></h3>
<ul>
<li>NIST Cybersecurity Framework. (2023). National Institute of Standards and Technology. Retrieved from <a href="https://www.nist.gov/cyberframework" target="_blank" rel="noopener noreferrer">https://www.nist.gov/cyberframework</a></li>
<li>OWASP Top 10 for IoT. (2023). Open Web Application Security Project. Retrieved from <a href="https://owasp.org/www-project-internet-of-things/" target="_blank" rel="noopener noreferrer">https://owasp.org/www-project-internet-of-things/</a></li>
<li>ROS 2 Security Working Group. (2023). Security Best Practices. Retrieved from <a href="https://github.com/ros-security" target="_blank" rel="noopener noreferrer">https://github.com/ros-security</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conferences-and-journals">Conferences and Journals<a href="#conferences-and-journals" class="hash-link" aria-label="Direct link to Conferences and Journals" title="Direct link to Conferences and Journals">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="top-robotics-conferences">Top Robotics Conferences<a href="#top-robotics-conferences" class="hash-link" aria-label="Direct link to Top Robotics Conferences" title="Direct link to Top Robotics Conferences">​</a></h3>
<ul>
<li>IEEE International Conference on Robotics and Automation (ICRA)</li>
<li>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</li>
<li>Robotics: Science and Systems (RSS)</li>
<li>Conference on Robot Learning (CoRL)</li>
<li>International Symposium of Robotics Research (ISRR)</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="top-aimachine-learning-conferences">Top AI/Machine Learning Conferences<a href="#top-aimachine-learning-conferences" class="hash-link" aria-label="Direct link to Top AI/Machine Learning Conferences" title="Direct link to Top AI/Machine Learning Conferences">​</a></h3>
<ul>
<li>Conference on Neural Information Processing Systems (NeurIPS)</li>
<li>International Conference on Machine Learning (ICML)</li>
<li>International Conference on Learning Representations (ICLR)</li>
<li>AAAI Conference on Artificial Intelligence</li>
<li>International Joint Conference on Artificial Intelligence (IJCAI)</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="top-journals">Top Journals<a href="#top-journals" class="hash-link" aria-label="Direct link to Top Journals" title="Direct link to Top Journals">​</a></h3>
<ul>
<li>IEEE Transactions on Robotics</li>
<li>The International Journal of Robotics Research</li>
<li>Autonomous Robots</li>
<li>Journal of Field Robotics</li>
<li>IEEE Robotics and Automation Letters</li>
<li>Nature Machine Intelligence</li>
<li>Science Robotics</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="additional-resources">Additional Resources<a href="#additional-resources" class="hash-link" aria-label="Direct link to Additional Resources" title="Direct link to Additional Resources">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="online-courses-and-moocs">Online Courses and MOOCs<a href="#online-courses-and-moocs" class="hash-link" aria-label="Direct link to Online Courses and MOOCs" title="Direct link to Online Courses and MOOCs">​</a></h3>
<ul>
<li>CS229: Machine Learning - Stanford University</li>
<li>CS231n: Convolutional Neural Networks for Visual Recognition - Stanford University</li>
<li>MIT 6.034 Artificial Intelligence - MIT OpenCourseWare</li>
<li>Robotics Specialization - University of Pennsylvania (Coursera)</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="community-resources">Community Resources<a href="#community-resources" class="hash-link" aria-label="Direct link to Community Resources" title="Direct link to Community Resources">​</a></h3>
<ul>
<li>ROS Discourse. (2023). Community forum for ROS users. Retrieved from <a href="https://discourse.ros.org/" target="_blank" rel="noopener noreferrer">https://discourse.ros.org/</a></li>
<li>PyTorch Discuss. (2023). Community forum for PyTorch users. Retrieved from <a href="https://discuss.pytorch.org/" target="_blank" rel="noopener noreferrer">https://discuss.pytorch.org/</a></li>
<li>Reddit r/MachineLearning. (2023). Community for machine learning discussions. Retrieved from <a href="https://www.reddit.com/r/MachineLearning/" target="_blank" rel="noopener noreferrer">https://www.reddit.com/r/MachineLearning/</a></li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/sanilahmed/hackathon-ai-book/tree/main/docs/modules/vla-system/references.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/hackathon-ai-book/modules/lab-exercises/lab-4-3-action-mapping"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Lab 4.3: Action Mapping in Vision-Language-Action Systems</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/hackathon-ai-book/modules/vla-system/safety-evaluation"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Safety and Evaluation</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#academic-papers-and-research" class="table-of-contents__link toc-highlight">Academic Papers and Research</a><ul><li><a href="#foundational-vla-research" class="table-of-contents__link toc-highlight">Foundational VLA Research</a></li><li><a href="#vision-language-models-for-robotics" class="table-of-contents__link toc-highlight">Vision-Language Models for Robotics</a></li><li><a href="#multimodal-learning-for-robotics" class="table-of-contents__link toc-highlight">Multimodal Learning for Robotics</a></li><li><a href="#language-guided-robot-control" class="table-of-contents__link toc-highlight">Language-Guided Robot Control</a></li></ul></li><li><a href="#technical-documentation" class="table-of-contents__link toc-highlight">Technical Documentation</a><ul><li><a href="#ros-2-integration" class="table-of-contents__link toc-highlight">ROS 2 Integration</a></li><li><a href="#deep-learning-frameworks" class="table-of-contents__link toc-highlight">Deep Learning Frameworks</a></li><li><a href="#vision-language-models" class="table-of-contents__link toc-highlight">Vision-Language Models</a></li></ul></li><li><a href="#simulation-and-development-tools" class="table-of-contents__link toc-highlight">Simulation and Development Tools</a><ul><li><a href="#isaac-sim-and-nvidia-tools" class="table-of-contents__link toc-highlight">Isaac Sim and NVIDIA Tools</a></li><li><a href="#gazebo-and-simulation" class="table-of-contents__link toc-highlight">Gazebo and Simulation</a></li><li><a href="#unity-and-perception" class="table-of-contents__link toc-highlight">Unity and Perception</a></li></ul></li><li><a href="#standards-and-best-practices" class="table-of-contents__link toc-highlight">Standards and Best Practices</a><ul><li><a href="#robotics-standards" class="table-of-contents__link toc-highlight">Robotics Standards</a></li><li><a href="#ai-safety-and-ethics" class="table-of-contents__link toc-highlight">AI Safety and Ethics</a></li><li><a href="#software-engineering-best-practices" class="table-of-contents__link toc-highlight">Software Engineering Best Practices</a></li></ul></li><li><a href="#open-source-projects-and-libraries" class="table-of-contents__link toc-highlight">Open Source Projects and Libraries</a><ul><li><a href="#vision-language-action-libraries" class="table-of-contents__link toc-highlight">Vision-Language-Action Libraries</a></li><li><a href="#robotics-libraries" class="table-of-contents__link toc-highlight">Robotics Libraries</a></li><li><a href="#deep-learning-libraries" class="table-of-contents__link toc-highlight">Deep Learning Libraries</a></li></ul></li><li><a href="#tutorials-and-learning-resources" class="table-of-contents__link toc-highlight">Tutorials and Learning Resources</a><ul><li><a href="#vla-specific-tutorials" class="table-of-contents__link toc-highlight">VLA-Specific Tutorials</a></li><li><a href="#robotics-education" class="table-of-contents__link toc-highlight">Robotics Education</a></li><li><a href="#deep-learning-for-robotics" class="table-of-contents__link toc-highlight">Deep Learning for Robotics</a></li></ul></li><li><a href="#industry-applications-and-case-studies" class="table-of-contents__link toc-highlight">Industry Applications and Case Studies</a><ul><li><a href="#commercial-vla-systems" class="table-of-contents__link toc-highlight">Commercial VLA Systems</a></li><li><a href="#research-institutions" class="table-of-contents__link toc-highlight">Research Institutions</a></li></ul></li><li><a href="#evaluation-and-benchmarking" class="table-of-contents__link toc-highlight">Evaluation and Benchmarking</a><ul><li><a href="#robotics-benchmarks" class="table-of-contents__link toc-highlight">Robotics Benchmarks</a></li><li><a href="#vision-language-benchmarks" class="table-of-contents__link toc-highlight">Vision-Language Benchmarks</a></li></ul></li><li><a href="#safety-and-security" class="table-of-contents__link toc-highlight">Safety and Security</a><ul><li><a href="#safety-standards" class="table-of-contents__link toc-highlight">Safety Standards</a></li><li><a href="#security-guidelines" class="table-of-contents__link toc-highlight">Security Guidelines</a></li></ul></li><li><a href="#conferences-and-journals" class="table-of-contents__link toc-highlight">Conferences and Journals</a><ul><li><a href="#top-robotics-conferences" class="table-of-contents__link toc-highlight">Top Robotics Conferences</a></li><li><a href="#top-aimachine-learning-conferences" class="table-of-contents__link toc-highlight">Top AI/Machine Learning Conferences</a></li><li><a href="#top-journals" class="table-of-contents__link toc-highlight">Top Journals</a></li></ul></li><li><a href="#additional-resources" class="table-of-contents__link toc-highlight">Additional Resources</a><ul><li><a href="#online-courses-and-moocs" class="table-of-contents__link toc-highlight">Online Courses and MOOCs</a></li><li><a href="#community-resources" class="table-of-contents__link toc-highlight">Community Resources</a></li></ul></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Modules</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/hackathon-ai-book/modules/ros2-nervous-system">ROS 2 Nervous System</a></li><li class="footer__item"><a class="footer__link-item" href="/hackathon-ai-book/modules/digital-twin">Digital Twin</a></li><li class="footer__item"><a class="footer__link-item" href="/hackathon-ai-book/modules/ai-robot-brain">AI-Robot Brain</a></li><li class="footer__item"><a class="footer__link-item" href="/hackathon-ai-book/modules/vla-system">Vision-Language-Action</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/ros2" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/sanilahmed/hackathon-ai-book" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Physical AI Book. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>