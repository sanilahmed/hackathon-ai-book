"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[4972],{4563:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>s,metadata:()=>r,toc:()=>c});var i=t(4848),a=t(8453);const s={},o="Lab 4.1: Vision-Language-Action (VLA) Fundamentals",r={id:"modules/lab-exercises/lab-4-1-vla-fundamentals",title:"Lab 4.1: Vision-Language-Action (VLA) Fundamentals",description:"Overview",source:"@site/docs/modules/lab-exercises/lab-4-1-vla-fundamentals.md",sourceDirName:"modules/lab-exercises",slug:"/modules/lab-exercises/lab-4-1-vla-fundamentals",permalink:"/ai-robotic-book/modules/lab-exercises/lab-4-1-vla-fundamentals",draft:!1,unlisted:!1,editUrl:"https://github.com/your-org/physical-ai-book/tree/main/docs/modules/lab-exercises/lab-4-1-vla-fundamentals.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"VLA Integration",permalink:"/ai-robotic-book/modules/vla-system/vla-integration"},next:{title:"Lab 4.2: Multimodal Perception in Vision-Language-Action Systems",permalink:"/ai-robotic-book/modules/lab-exercises/lab-4-2-multimodal-perception"}},l={},c=[{value:"Overview",id:"overview",level:2},{value:"Objectives",id:"objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Duration",id:"duration",level:2},{value:"Exercise 1: Understanding VLA Architecture",id:"exercise-1-understanding-vla-architecture",level:2},{value:"Step 1: Create VLA architecture overview",id:"step-1-create-vla-architecture-overview",level:3},{value:"Exercise 2: Working with Pre-trained Vision-Language Models",id:"exercise-2-working-with-pre-trained-vision-language-models",level:2},{value:"Step 1: Implement CLIP integration",id:"step-1-implement-clip-integration",level:3},{value:"Exercise 3: Multimodal Fusion Techniques",id:"exercise-3-multimodal-fusion-techniques",level:2},{value:"Step 1: Implement advanced fusion methods",id:"step-1-implement-advanced-fusion-methods",level:3},{value:"Exercise 4: Action Mapping and Execution",id:"exercise-4-action-mapping-and-execution",level:2},{value:"Step 1: Create action mapping system",id:"step-1-create-action-mapping-system",level:3},{value:"Exercise 5: VLA Evaluation and Validation",id:"exercise-5-vla-evaluation-and-validation",level:2},{value:"Step 1: Create evaluation metrics",id:"step-1-create-evaluation-metrics",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues and Solutions",id:"common-issues-and-solutions",level:3},{value:"Assessment Questions",id:"assessment-questions",level:2},{value:"Extension Exercises",id:"extension-exercises",level:2},{value:"Summary",id:"summary",level:2}];function u(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.h1,{id:"lab-41-vision-language-action-vla-fundamentals",children:"Lab 4.1: Vision-Language-Action (VLA) Fundamentals"}),"\n",(0,i.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,i.jsx)(e.p,{children:"In this lab, you will learn the fundamentals of Vision-Language-Action (VLA) systems for robotics. You'll explore the core concepts of multimodal learning, implement basic vision-language models, and understand how to connect perception, language understanding, and action execution in robotic systems. This includes working with pre-trained models like CLIP, implementing multimodal fusion, and creating simple VLA systems."}),"\n",(0,i.jsx)(e.h2,{id:"objectives",children:"Objectives"}),"\n",(0,i.jsx)(e.p,{children:"By the end of this lab, you will be able to:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Understand the core concepts of Vision-Language-Action systems"}),"\n",(0,i.jsx)(e.li,{children:"Implement basic vision and language encoders"}),"\n",(0,i.jsx)(e.li,{children:"Create multimodal fusion mechanisms"}),"\n",(0,i.jsx)(e.li,{children:"Integrate VLA systems with robot control"}),"\n",(0,i.jsx)(e.li,{children:"Work with pre-trained vision-language models"}),"\n",(0,i.jsx)(e.li,{children:"Implement simple action mapping from language to robot actions"}),"\n",(0,i.jsx)(e.li,{children:"Evaluate VLA system performance"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Completion of Module 1-3 (ROS 2, Digital Twin, AI-Robot Brain)"}),"\n",(0,i.jsx)(e.li,{children:"Understanding of deep learning fundamentals"}),"\n",(0,i.jsx)(e.li,{children:"Experience with PyTorch and neural networks"}),"\n",(0,i.jsx)(e.li,{children:"Basic knowledge of computer vision and NLP"}),"\n",(0,i.jsx)(e.li,{children:"Experience with Isaac Sim or similar simulation environments"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"duration",children:"Duration"}),"\n",(0,i.jsx)(e.p,{children:"4-5 hours"}),"\n",(0,i.jsx)(e.h2,{id:"exercise-1-understanding-vla-architecture",children:"Exercise 1: Understanding VLA Architecture"}),"\n",(0,i.jsx)(e.h3,{id:"step-1-create-vla-architecture-overview",children:"Step 1: Create VLA architecture overview"}),"\n",(0,i.jsxs)(e.p,{children:["Create ",(0,i.jsx)(e.code,{children:"~/vla_examples/vla_architecture.py"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n# vla_architecture.py\n"""Vision-Language-Action system architecture."""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\nclass VisionEncoder(nn.Module):\n    """Vision encoder for VLA systems."""\n\n    def __init__(self, backbone=\'resnet50\', pretrained=True):\n        super(VisionEncoder, self).__init__()\n\n        if backbone == \'resnet50\':\n            from torchvision.models import resnet50\n            self.backbone = resnet50(pretrained=pretrained)\n            # Remove the final classification layer\n            self.feature_dim = self.backbone.fc.in_features\n            self.backbone.fc = nn.Identity()\n        elif backbone == \'clip_vision\':\n            # Use CLIP\'s vision encoder\n            import clip\n            model, preprocess = clip.load("ViT-B/32", device="cpu")\n            self.backbone = model.visual\n            self.feature_dim = model.visual.output_dim\n        else:\n            raise ValueError(f"Unknown backbone: {backbone}")\n\n        # Projection layer to common space\n        self.projection = nn.Linear(self.feature_dim, 512)\n\n    def forward(self, images):\n        """Forward pass through vision encoder."""\n        features = self.backbone(images)\n        projected_features = self.projection(features)\n        return projected_features\n\n\nclass LanguageEncoder(nn.Module):\n    """Language encoder for VLA systems."""\n\n    def __init__(self, model_name=\'bert-base-uncased\'):\n        super(LanguageEncoder, self).__init__()\n\n        from transformers import AutoTokenizer, AutoModel\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModel.from_pretrained(model_name)\n\n        # Projection layer to common space\n        self.projection = nn.Linear(self.model.config.hidden_size, 512)\n\n    def forward(self, texts):\n        """Forward pass through language encoder."""\n        # Tokenize input texts\n        if isinstance(texts, str):\n            texts = [texts]\n\n        inputs = self.tokenizer(\n            texts,\n            return_tensors=\'pt\',\n            padding=True,\n            truncation=True,\n            max_length=128\n        )\n\n        # Get language embeddings\n        outputs = self.model(**inputs)\n        # Use [CLS] token representation\n        embeddings = outputs.last_hidden_state[:, 0, :]\n\n        # Project to common space\n        projected_embeddings = self.projection(embeddings)\n        return projected_embeddings\n\n\nclass MultimodalFusion(nn.Module):\n    """Multimodal fusion for vision-language integration."""\n\n    def __init__(self, feature_dim=512):\n        super(MultimodalFusion, self).__init__()\n        self.feature_dim = feature_dim\n\n        # Cross-attention mechanism\n        self.cross_attention = nn.MultiheadAttention(\n            embed_dim=feature_dim,\n            num_heads=8,\n            batch_first=True\n        )\n\n        # Fusion layers\n        self.fusion_mlp = nn.Sequential(\n            nn.Linear(feature_dim * 2, feature_dim * 4),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(feature_dim * 4, feature_dim * 2),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(feature_dim * 2, feature_dim)\n        )\n\n        # Normalization\n        self.norm = nn.LayerNorm(feature_dim)\n\n    def forward(self, vision_features, language_features):\n        """Fuse vision and language features."""\n        # Ensure proper dimensions\n        if len(vision_features.shape) == 2:\n            vision_features = vision_features.unsqueeze(1)  # [B, 1, D]\n        if len(language_features.shape) == 2:\n            language_features = language_features.unsqueeze(1)  # [B, 1, D]\n\n        # Cross-attention: vision attends to language and vice versa\n        attended_vision, _ = self.cross_attention(\n            vision_features, language_features, language_features\n        )\n        attended_language, _ = self.cross_attention(\n            language_features, vision_features, vision_features\n        )\n\n        # Concatenate attended features\n        combined_features = torch.cat([\n            attended_vision.squeeze(1),\n            attended_language.squeeze(1)\n        ], dim=-1)\n\n        # Apply fusion MLP\n        fused_features = self.fusion_mlp(combined_features)\n        fused_features = self.norm(fused_features)\n\n        return fused_features\n\n\nclass ActionDecoder(nn.Module):\n    """Action decoder for converting fused features to robot actions."""\n\n    def __init__(self, action_space_dim=12):\n        super(ActionDecoder, self).__init__()\n        self.action_space_dim = action_space_dim\n\n        # Decode fused features to action space\n        self.decoder = nn.Sequential(\n            nn.Linear(512, 1024),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(512, action_space_dim)\n        )\n\n        # Separate heads for different action types\n        self.navigation_head = nn.Linear(512, 3)  # x, y, theta\n        self.manipulation_head = nn.Linear(512, 6)  # joint positions\n        self.gripper_head = nn.Linear(512, 1)      # gripper position\n\n    def forward(self, fused_features):\n        """Decode fused features to actions."""\n        # Single action prediction\n        actions = self.decoder(fused_features)\n\n        # Alternative: structured action prediction\n        structured_actions = {\n            \'navigation\': self.navigation_head(fused_features),\n            \'manipulation\': self.manipulation_head(fused_features),\n            \'gripper\': torch.sigmoid(self.gripper_head(fused_features)),\n            \'full_action\': actions\n        }\n\n        return structured_actions\n\n\nclass VLAModel(nn.Module):\n    """Complete VLA model combining vision, language, and action components."""\n\n    def __init__(self, action_space_dim=12):\n        super(VLAModel, self).__init__()\n\n        # Initialize components\n        self.vision_encoder = VisionEncoder()\n        self.language_encoder = LanguageEncoder()\n        self.multimodal_fusion = MultimodalFusion()\n        self.action_decoder = ActionDecoder(action_space_dim)\n\n    def forward(self, images, texts):\n        """Forward pass through complete VLA model."""\n        # Encode vision\n        vision_features = self.vision_encoder(images)\n\n        # Encode language\n        language_features = self.language_encoder(texts)\n\n        # Fuse modalities\n        fused_features = self.multimodal_fusion(vision_features, language_features)\n\n        # Decode to actions\n        actions = self.action_decoder(fused_features)\n\n        return actions\n\n    def process_instruction(self, image, instruction):\n        """Process single instruction with image."""\n        with torch.no_grad():\n            actions = self.forward(image.unsqueeze(0), [instruction])\n            return actions\n\n\n# Example usage and testing\nif __name__ == "__main__":\n    print("VLA Architecture Example")\n\n    # Create VLA model\n    vla_model = VLAModel(action_space_dim=12)\n    print(f"VLA model created with {sum(p.numel() for p in vla_model.parameters()):,} parameters")\n\n    # Test with dummy inputs\n    dummy_image = torch.randn(1, 3, 224, 224)  # Batch of 1, 3-channel, 224x224 image\n    dummy_text = ["Move forward"]\n\n    print("Testing VLA model with dummy inputs...")\n    with torch.no_grad():\n        output = vla_model(dummy_image, dummy_text)\n\n    print(f"Model output keys: {output.keys()}")\n    for key, value in output.items():\n        if isinstance(value, torch.Tensor):\n            print(f"  {key}: shape {value.shape}")\n\n    print("VLA architecture test completed successfully")\n'})}),"\n",(0,i.jsx)(e.h2,{id:"exercise-2-working-with-pre-trained-vision-language-models",children:"Exercise 2: Working with Pre-trained Vision-Language Models"}),"\n",(0,i.jsx)(e.h3,{id:"step-1-implement-clip-integration",children:"Step 1: Implement CLIP integration"}),"\n",(0,i.jsxs)(e.p,{children:["Create ",(0,i.jsx)(e.code,{children:"~/vla_examples/clip_integration.py"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\n# clip_integration.py\n\"\"\"CLIP integration for VLA systems.\"\"\"\n\nimport torch\nimport clip\nfrom PIL import Image\nimport numpy as np\nimport torchvision.transforms as transforms\n\nclass CLIPBasedVLA:\n    \"\"\"VLA system based on CLIP model.\"\"\"\n\n    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):\n        self.device = device\n        print(f\"Loading CLIP model on {device}...\")\n\n        # Load CLIP model\n        self.model, self.preprocess = clip.load(\"ViT-B/32\", device=device)\n        self.model.eval()  # Set to evaluation mode\n\n        # Action space definition\n        self.action_space = {\n            'navigation': ['move forward', 'move backward', 'turn left', 'turn right', 'stop'],\n            'manipulation': ['pick up', 'put down', 'grasp', 'release', 'move to'],\n            'general': ['go to', 'bring', 'fetch', 'avoid', 'follow']\n        }\n\n        print(\"CLIP model loaded successfully\")\n\n    def encode_image(self, image_path):\n        \"\"\"Encode image using CLIP.\"\"\"\n        if isinstance(image_path, str):\n            image = self.preprocess(Image.open(image_path)).unsqueeze(0).to(self.device)\n        else:\n            # Assume image is already a PIL Image\n            image = self.preprocess(image_path).unsqueeze(0).to(self.device)\n\n        with torch.no_grad():\n            image_features = self.model.encode_image(image)\n            image_features /= image_features.norm(dim=-1, keepdim=True)  # Normalize\n\n        return image_features\n\n    def encode_text(self, text):\n        \"\"\"Encode text using CLIP.\"\"\"\n        if isinstance(text, list):\n            text_tokens = clip.tokenize(text).to(self.device)\n        else:\n            text_tokens = clip.tokenize([text]).to(self.device)\n\n        with torch.no_grad():\n            text_features = self.model.encode_text(text_tokens)\n            text_features /= text_features.norm(dim=-1, keepdim=True)  # Normalize\n\n        return text_features\n\n    def compute_similarity(self, image_features, text_features):\n        \"\"\"Compute similarity between image and text.\"\"\"\n        similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n        return similarity\n\n    def rank_candidates(self, image_path, candidate_texts):\n        \"\"\"Rank candidate texts by similarity to image.\"\"\"\n        image_features = self.encode_image(image_path)\n        text_features = self.encode_text(candidate_texts)\n\n        similarity = self.compute_similarity(image_features, text_features)\n        sorted_indices = torch.argsort(similarity[0], descending=True)\n\n        results = []\n        for idx in sorted_indices:\n            results.append({\n                'text': candidate_texts[idx.item()],\n                'similarity': similarity[0][idx].item()\n            })\n\n        return results\n\n    def infer_action_from_instruction(self, image_path, instruction):\n        \"\"\"Infer action from image and instruction using CLIP.\"\"\"\n        # Combine navigation and manipulation actions\n        all_actions = self.action_space['navigation'] + self.action_space['manipulation'] + self.action_space['general']\n\n        # Add the instruction to the list of candidates\n        candidates = all_actions + [instruction]\n\n        # Rank all candidates\n        rankings = self.rank_candidates(image_path, candidates)\n\n        # Return top-ranked action\n        top_action = rankings[0]\n        return top_action\n\n    def process_vla_task(self, image_path, instruction):\n        \"\"\"Process complete VLA task: image + instruction -> action.\"\"\"\n        print(f\"Processing VLA task: '{instruction}' with image {image_path}\")\n\n        # Get action recommendation\n        action_recommendation = self.infer_action_from_instruction(image_path, instruction)\n\n        # Convert to robot command\n        robot_command = self.convert_to_robot_command(action_recommendation['text'])\n\n        result = {\n            'instruction': instruction,\n            'recommended_action': action_recommendation,\n            'robot_command': robot_command,\n            'confidence': action_recommendation['similarity']\n        }\n\n        return result\n\n    def convert_to_robot_command(self, action_text):\n        \"\"\"Convert natural language action to robot command.\"\"\"\n        # This is a simplified mapping - in practice, this would be more complex\n        action_mapping = {\n            'move forward': [1.0, 0.0, 0.0],  # [x_vel, y_vel, angular_vel]\n            'move backward': [-1.0, 0.0, 0.0],\n            'turn left': [0.0, 0.0, 1.0],\n            'turn right': [0.0, 0.0, -1.0],\n            'stop': [0.0, 0.0, 0.0],\n            'pick up': [0.0, 0.0, 0.0],  # Placeholder for manipulation\n            'put down': [0.0, 0.0, 0.0],\n            'grasp': [0.0, 0.0, 0.0],\n            'release': [0.0, 0.0, 0.0],\n            'go to': [0.0, 0.0, 0.0],\n            'bring': [0.0, 0.0, 0.0],\n            'fetch': [0.0, 0.0, 0.0],\n            'avoid': [0.0, 0.0, 0.0],\n            'follow': [0.0, 0.0, 0.0]\n        }\n\n        return action_mapping.get(action_text.lower(), [0.0, 0.0, 0.0])\n\n\nclass CLIPEnhancedVLA(CLIPBasedVLA):\n    \"\"\"Enhanced VLA system with improved capabilities.\"\"\"\n\n    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):\n        super().__init__(device)\n\n        # Enhanced action space with more specific actions\n        self.enhanced_action_space = {\n            'navigation': [\n                'move forward 1 meter',\n                'move backward 0.5 meters',\n                'turn left 90 degrees',\n                'turn right 90 degrees',\n                'move forward slowly',\n                'move forward quickly',\n                'navigate around obstacle',\n                'approach red object',\n                'move toward blue object',\n                'stop moving'\n            ],\n            'manipulation': [\n                'pick up the red cup',\n                'put the cup on the table',\n                'grasp the blue ball',\n                'release the object',\n                'move object to left',\n                'move object to right',\n                'lift object gently',\n                'place object carefully'\n            ],\n            'complex': [\n                'go to the kitchen and bring the cup',\n                'navigate to the table and place the object',\n                'find the red cube and move it to the box',\n                'avoid obstacles while moving forward'\n            ]\n        }\n\n    def semantic_search(self, image_path, query, top_k=5):\n        \"\"\"Perform semantic search using CLIP.\"\"\"\n        # Create a more comprehensive set of candidates\n        all_candidates = []\n        for category, actions in self.enhanced_action_space.items():\n            all_candidates.extend(actions)\n\n        # Add the query itself\n        all_candidates.append(query)\n\n        # Rank candidates\n        rankings = self.rank_candidates(image_path, all_candidates)\n\n        # Return top-k results\n        return rankings[:top_k]\n\n    def generate_action_sequence(self, image_path, high_level_task):\n        \"\"\"Generate sequence of actions for complex tasks.\"\"\"\n        print(f\"Generating action sequence for: '{high_level_task}'\")\n\n        # For complex tasks, break them down into subtasks\n        if 'and' in high_level_task.lower():\n            subtasks = high_level_task.lower().split(' and ')\n        else:\n            subtasks = [high_level_task]\n\n        action_sequence = []\n        for subtask in subtasks:\n            # Get relevant actions for this subtask\n            top_actions = self.semantic_search(image_path, subtask, top_k=3)\n            action_sequence.append({\n                'subtask': subtask,\n                'recommended_actions': top_actions[:2]  # Top 2 recommendations\n            })\n\n        return action_sequence\n\n\n# Example usage\nif __name__ == \"__main__\":\n    print(\"CLIP Integration Example\")\n\n    # Create CLIP-based VLA system\n    vla_system = CLIPBasedVLA()\n\n    # Example: Simulate processing an image with instruction\n    # Note: In practice, you would use real image paths\n    print(\"\\nTesting with dummy image and instruction...\")\n\n    # Since we don't have a real image, we'll simulate the process\n    dummy_instruction = \"Move forward\"\n    print(f\"Instruction: {dummy_instruction}\")\n\n    # This would normally process a real image\n    # result = vla_system.process_vla_task(\"dummy_image.jpg\", dummy_instruction)\n    # print(f\"Result: {result}\")\n\n    print(\"\\nTesting enhanced VLA system...\")\n    enhanced_vla = CLIPEnhancedVLA()\n\n    complex_task = \"go to the kitchen and bring the cup\"\n    sequence = enhanced_vla.generate_action_sequence(\"dummy_image.jpg\", complex_task)\n    print(f\"Action sequence for '{complex_task}':\")\n    for i, step in enumerate(sequence):\n        print(f\"  Step {i+1}: {step['subtask']}\")\n        for action in step['recommended_actions']:\n            print(f\"    - {action['text']}: {action['similarity']:.3f}\")\n\n    print(\"\\nCLIP integration example completed successfully\")\n"})}),"\n",(0,i.jsx)(e.h2,{id:"exercise-3-multimodal-fusion-techniques",children:"Exercise 3: Multimodal Fusion Techniques"}),"\n",(0,i.jsx)(e.h3,{id:"step-1-implement-advanced-fusion-methods",children:"Step 1: Implement advanced fusion methods"}),"\n",(0,i.jsxs)(e.p,{children:["Create ",(0,i.jsx)(e.code,{children:"~/vla_examples/multimodal_fusion.py"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n# multimodal_fusion.py\n"""Advanced multimodal fusion techniques for VLA systems."""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\nclass CrossModalAttention(nn.Module):\n    """Cross-modal attention for vision-language fusion."""\n\n    def __init__(self, feature_dim=512, num_heads=8):\n        super(CrossModalAttention, self).__init__()\n        self.feature_dim = feature_dim\n        self.num_heads = num_heads\n        self.head_dim = feature_dim // num_heads\n\n        # Linear projections\n        self.vision_proj = nn.Linear(feature_dim, feature_dim)\n        self.language_proj = nn.Linear(feature_dim, feature_dim)\n\n        # Multi-head attention\n        self.attention = nn.MultiheadAttention(\n            embed_dim=feature_dim,\n            num_heads=num_heads,\n            batch_first=True\n        )\n\n        # Output projection\n        self.output_proj = nn.Linear(feature_dim, feature_dim)\n\n        # Layer normalization\n        self.norm = nn.LayerNorm(feature_dim)\n\n    def forward(self, vision_features, language_features):\n        """Apply cross-modal attention."""\n        # Project features\n        vision_proj = self.vision_proj(vision_features.unsqueeze(1))\n        language_proj = self.language_proj(language_features.unsqueeze(1))\n\n        # Cross-attention: vision attends to language\n        attended_vision, attention_weights_vision = self.attention(\n            vision_proj, language_proj, language_proj\n        )\n\n        # Cross-attention: language attends to vision\n        attended_language, attention_weights_language = self.attention(\n            language_proj, vision_proj, vision_proj\n        )\n\n        # Concatenate and project\n        combined = torch.cat([\n            attended_vision.squeeze(1),\n            attended_language.squeeze(1)\n        ], dim=-1)\n\n        output = self.output_proj(combined)\n        output = self.norm(output)\n\n        return output, {\n            \'vision_attention\': attention_weights_vision,\n            \'language_attention\': attention_weights_language\n        }\n\n\nclass CoAttentionFusion(nn.Module):\n    """Co-attention fusion mechanism."""\n\n    def __init__(self, feature_dim=512):\n        super(CoAttentionFusion, self).__init__()\n        self.feature_dim = feature_dim\n\n        # Attention mechanisms\n        self.vision_attention = nn.Sequential(\n            nn.Linear(feature_dim, feature_dim // 2),\n            nn.ReLU(),\n            nn.Linear(feature_dim // 2, feature_dim),\n            nn.Softmax(dim=-1)\n        )\n\n        self.language_attention = nn.Sequential(\n            nn.Linear(feature_dim, feature_dim // 2),\n            nn.ReLU(),\n            nn.Linear(feature_dim // 2, feature_dim),\n            nn.Softmax(dim=-1)\n        )\n\n        # Fusion network\n        self.fusion_net = nn.Sequential(\n            nn.Linear(feature_dim * 2, feature_dim * 2),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(feature_dim * 2, feature_dim)\n        )\n\n    def forward(self, vision_features, language_features):\n        """Apply co-attention fusion."""\n        # Compute attention weights\n        vision_att_weights = self.vision_attention(vision_features)\n        language_att_weights = self.language_attention(language_features)\n\n        # Apply attention\n        attended_vision = vision_features * vision_att_weights\n        attended_language = language_features * language_att_weights\n\n        # Concatenate and fuse\n        combined = torch.cat([attended_vision, attended_language], dim=-1)\n        fused_features = self.fusion_net(combined)\n\n        return fused_features\n\n\nclass HierarchicalFusion(nn.Module):\n    """Hierarchical fusion for multi-level feature integration."""\n\n    def __init__(self, feature_dims=[256, 512, 1024]):\n        super(HierarchicalFusion, self).__init__()\n        self.feature_dims = feature_dims\n\n        # Level-specific fusion modules\n        self.level_fusions = nn.ModuleList()\n        for i, dim in enumerate(feature_dims):\n            if i == 0:\n                # First level: simple fusion\n                self.level_fusions.append(\n                    nn.Sequential(\n                        nn.Linear(dim * 2, dim * 2),\n                        nn.ReLU(),\n                        nn.Linear(dim * 2, dim)\n                    )\n                )\n            else:\n                # Subsequent levels: incorporate previous level\n                self.level_fusions.append(\n                    nn.Sequential(\n                        nn.Linear(feature_dims[i-1] + dim * 2, dim * 2),\n                        nn.ReLU(),\n                        nn.Linear(dim * 2, dim)\n                    )\n                )\n\n        # Final fusion layer\n        self.final_fusion = nn.Sequential(\n            nn.Linear(sum(feature_dims), 1024),\n            nn.ReLU(),\n            nn.Linear(1024, 512)\n        )\n\n    def forward(self, vision_features_list, language_features):\n        """Apply hierarchical fusion across multiple levels."""\n        fused_outputs = []\n\n        for i, (vision_features, fusion_module) in enumerate(zip(vision_features_list, self.level_fusions)):\n            if i == 0:\n                # First level: just fuse vision and language\n                level_input = torch.cat([vision_features, language_features], dim=-1)\n            else:\n                # Subsequent levels: include previous fused output\n                prev_fused = fused_outputs[-1]\n                level_input = torch.cat([prev_fused, vision_features, language_features], dim=-1)\n\n            level_output = fusion_module(level_input)\n            fused_outputs.append(level_output)\n\n        # Final fusion of all levels\n        final_input = torch.cat(fused_outputs, dim=-1)\n        final_output = self.final_fusion(final_input)\n\n        return final_output\n\n\nclass AdaptiveFusion(nn.Module):\n    """Adaptive fusion that learns to weight different modalities."""\n\n    def __init__(self, feature_dim=512):\n        super(AdaptiveFusion, self).__init__()\n        self.feature_dim = feature_dim\n\n        # Gate networks to compute modality weights\n        self.vision_gate = nn.Sequential(\n            nn.Linear(feature_dim, feature_dim // 4),\n            nn.ReLU(),\n            nn.Linear(feature_dim // 4, 1),\n            nn.Sigmoid()\n        )\n\n        self.language_gate = nn.Sequential(\n            nn.Linear(feature_dim, feature_dim // 4),\n            nn.ReLU(),\n            nn.Linear(feature_dim // 4, 1),\n            nn.Sigmoid()\n        )\n\n        # Fusion network\n        self.fusion = nn.Sequential(\n            nn.Linear(feature_dim * 2, feature_dim * 2),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(feature_dim * 2, feature_dim)\n        )\n\n    def forward(self, vision_features, language_features):\n        """Apply adaptive fusion with learned weights."""\n        # Compute adaptive weights\n        vision_weight = self.vision_gate(vision_features)\n        language_weight = self.language_gate(language_features)\n\n        # Apply weights\n        weighted_vision = vision_features * vision_weight\n        weighted_language = language_features * language_weight\n\n        # Concatenate and fuse\n        combined = torch.cat([weighted_vision, weighted_language], dim=-1)\n        fused_output = self.fusion(combined)\n\n        return fused_output, {\n            \'vision_weight\': vision_weight.mean().item(),\n            \'language_weight\': language_weight.mean().item()\n        }\n\n\nclass FusionComparison:\n    """Compare different fusion techniques."""\n\n    def __init__(self):\n        self.fusion_methods = {\n            \'cross_attention\': CrossModalAttention(),\n            \'co_attention\': CoAttentionFusion(),\n            \'adaptive\': AdaptiveFusion(),\n        }\n\n    def test_fusion_methods(self, vision_features, language_features):\n        """Test different fusion methods."""\n        results = {}\n\n        for name, fusion_module in self.fusion_methods.items():\n            if name == \'adaptive\':\n                output, weights = fusion_module(vision_features, language_features)\n                results[name] = {\n                    \'output\': output,\n                    \'weights\': weights\n                }\n            else:\n                output = fusion_module(vision_features, language_features)\n                results[name] = {\n                    \'output\': output\n                }\n\n        return results\n\n    def analyze_fusion_performance(self, results):\n        """Analyze fusion method performance."""\n        analysis = {}\n\n        for method_name, result in results.items():\n            output = result[\'output\']\n\n            # Compute various metrics\n            analysis[method_name] = {\n                \'output_norm\': torch.norm(output).item(),\n                \'output_variance\': torch.var(output).item(),\n                \'output_mean\': torch.mean(output).item(),\n                \'output_std\': torch.std(output).item()\n            }\n\n            # Add method-specific metrics\n            if \'weights\' in result:\n                analysis[method_name].update(result[\'weights\'])\n\n        return analysis\n\n\n# Example usage and testing\nif __name__ == "__main__":\n    print("Multimodal Fusion Techniques Example")\n\n    # Create fusion comparison system\n    fusion_comparison = FusionComparison()\n\n    # Generate dummy features for testing\n    batch_size = 4\n    feature_dim = 512\n\n    vision_features = torch.randn(batch_size, feature_dim)\n    language_features = torch.randn(batch_size, feature_dim)\n\n    print(f"Testing fusion methods with features of shape: {vision_features.shape}")\n\n    # Test all fusion methods\n    results = fusion_comparison.test_fusion_methods(vision_features, language_features)\n\n    # Analyze results\n    analysis = fusion_comparison.analyze_fusion_performance(results)\n\n    print("\\nFusion Method Analysis:")\n    for method, metrics in analysis.items():\n        print(f"\\n{method.upper()}:")\n        for metric, value in metrics.items():\n            if isinstance(value, (int, float)):\n                print(f"  {metric}: {value:.4f}")\n            else:\n                print(f"  {metric}: {value}")\n\n    # Test hierarchical fusion\n    print("\\nTesting hierarchical fusion...")\n    hier_fusion = HierarchicalFusion(feature_dims=[256, 512, 1024])\n\n    # Create features for different levels\n    level1_vision = torch.randn(batch_size, 256)\n    level2_vision = torch.randn(batch_size, 512)\n    level3_vision = torch.randn(batch_size, 1024)\n    language_features_large = torch.randn(batch_size, 1024)\n\n    hier_output = hier_fusion([level1_vision, level2_vision, level3_vision], language_features_large)\n    print(f"Hierarchical fusion output shape: {hier_output.shape}")\n\n    print("\\nMultimodal fusion techniques example completed successfully")\n'})}),"\n",(0,i.jsx)(e.h2,{id:"exercise-4-action-mapping-and-execution",children:"Exercise 4: Action Mapping and Execution"}),"\n",(0,i.jsx)(e.h3,{id:"step-1-create-action-mapping-system",children:"Step 1: Create action mapping system"}),"\n",(0,i.jsxs)(e.p,{children:["Create ",(0,i.jsx)(e.code,{children:"~/vla_examples/action_mapping.py"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\n# action_mapping.py\n\"\"\"Action mapping and execution system for VLA.\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport re\nfrom typing import Dict, List, Tuple, Any\n\nclass ActionMapper:\n    \"\"\"Map language instructions to robot actions.\"\"\"\n\n    def __init__(self):\n        # Define action vocabulary\n        self.action_types = {\n            'navigation': ['move', 'go', 'navigate', 'walk', 'drive', 'travel', 'proceed'],\n            'manipulation': ['pick', 'grasp', 'hold', 'carry', 'lift', 'raise', 'catch', 'catch', 'take'],\n            'placement': ['place', 'put', 'set', 'drop', 'release', 'position', 'locate', 'deposit'],\n            'rotation': ['turn', 'rotate', 'spin', 'pivot', 'face', 'orient', 'aim'],\n            'interaction': ['push', 'pull', 'press', 'touch', 'activate', 'operate', 'use'],\n            'locomotion': ['step', 'stride', 'crawl', 'jump', 'hop', 'stomp', 'tiptoe']\n        }\n\n        # Define spatial relations\n        self.spatial_relations = [\n            'to', 'toward', 'into', 'onto', 'over', 'under', 'behind', 'in_front_of',\n            'left', 'right', 'above', 'below', 'near', 'far', 'close', 'away',\n            'beside', 'next_to', 'between', 'among', 'through', 'across'\n        ]\n\n        # Define object properties\n        self.object_properties = [\n            'red', 'blue', 'green', 'yellow', 'big', 'small', 'large', 'tiny',\n            'heavy', 'light', 'round', 'square', 'long', 'short', 'tall', 'wide'\n        ]\n\n        # Robot action space\n        self.robot_action_space = {\n            'navigation': {\n                'linear_velocity': (-1.0, 1.0),  # m/s\n                'angular_velocity': (-1.0, 1.0),  # rad/s\n                'duration': (0.1, 5.0)  # seconds\n            },\n            'manipulation': {\n                'joint_positions': (-2.0, 2.0),  # radians\n                'gripper_position': (0.0, 1.0),  # normalized\n                'force_limit': (0.0, 100.0)  # Newtons\n            }\n        }\n\n    def parse_instruction(self, instruction: str) -> Dict[str, Any]:\n        \"\"\"Parse natural language instruction into structured representation.\"\"\"\n        instruction_lower = instruction.lower()\n\n        # Extract action type\n        action_type = self.identify_action_type(instruction_lower)\n\n        # Extract spatial relations\n        spatial_info = self.extract_spatial_relations(instruction_lower)\n\n        # Extract object information\n        objects = self.extract_objects(instruction_lower)\n\n        # Extract numerical information\n        numbers = self.extract_numbers(instruction_lower)\n\n        # Extract directional information\n        directions = self.extract_directions(instruction_lower)\n\n        parsed_instruction = {\n            'original': instruction,\n            'action_type': action_type,\n            'spatial_relations': spatial_info,\n            'objects': objects,\n            'numbers': numbers,\n            'directions': directions,\n            'confidence': 1.0  # This would come from NLP model in practice\n        }\n\n        return parsed_instruction\n\n    def identify_action_type(self, instruction: str) -> str:\n        \"\"\"Identify the primary action type from instruction.\"\"\"\n        for action_type, keywords in self.action_types.items():\n            for keyword in keywords:\n                if keyword in instruction:\n                    return action_type\n        return 'unknown'\n\n    def extract_spatial_relations(self, instruction: str) -> List[str]:\n        \"\"\"Extract spatial relations from instruction.\"\"\"\n        relations = []\n        for relation in self.spatial_relations:\n            if relation in instruction:\n                relations.append(relation)\n        return relations\n\n    def extract_objects(self, instruction: str) -> List[str]:\n        \"\"\"Extract object mentions from instruction.\"\"\"\n        # Simple extraction - in practice, use NER\n        words = instruction.split()\n        objects = []\n        properties = []\n\n        for i, word in enumerate(words):\n            if word in self.object_properties:\n                properties.append(word)\n            elif i > 0 and words[i-1] in self.object_properties:\n                # Property followed by object\n                objects.append(f\"{words[i-1]} {word}\")\n            elif word not in [item for sublist in self.action_types.values() for item in sublist]:\n                # Not an action keyword, might be an object\n                if word not in self.spatial_relations and word not in self.object_properties:\n                    objects.append(word)\n\n        return objects\n\n    def extract_numbers(self, instruction: str) -> List[float]:\n        \"\"\"Extract numerical values from instruction.\"\"\"\n        # Find numbers in the instruction\n        numbers = re.findall(r'\\d+\\.?\\d*', instruction)\n        return [float(num) for num in numbers if num]\n\n    def extract_directions(self, instruction: str) -> List[str]:\n        \"\"\"Extract directional information.\"\"\"\n        directions = []\n        direction_keywords = ['forward', 'backward', 'left', 'right', 'up', 'down', 'north', 'south', 'east', 'west']\n        for direction in direction_keywords:\n            if direction in instruction:\n                directions.append(direction)\n        return directions\n\n    def map_to_action_space(self, parsed_instruction: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Map parsed instruction to robot action space.\"\"\"\n        action_type = parsed_instruction['action_type']\n        numbers = parsed_instruction['numbers']\n        directions = parsed_instruction['directions']\n\n        if action_type == 'navigation':\n            action = self.map_navigation_action(parsed_instruction)\n        elif action_type == 'manipulation':\n            action = self.map_manipulation_action(parsed_instruction)\n        elif action_type == 'placement':\n            action = self.map_placement_action(parsed_instruction)\n        elif action_type == 'rotation':\n            action = self.map_rotation_action(parsed_instruction)\n        else:\n            action = self.map_generic_action(parsed_instruction)\n\n        return action\n\n    def map_navigation_action(self, parsed_instruction: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Map navigation instruction to navigation action.\"\"\"\n        action = {\n            'type': 'navigation',\n            'linear_velocity': 0.0,\n            'angular_velocity': 0.0,\n            'duration': 1.0,\n            'target_position': None\n        }\n\n        # Determine direction and speed based on instruction\n        directions = parsed_instruction['directions']\n        numbers = parsed_instruction['numbers']\n\n        if 'forward' in directions or 'ahead' in directions:\n            action['linear_velocity'] = 0.5\n        elif 'backward' in directions or 'back' in directions:\n            action['linear_velocity'] = -0.5\n        elif 'left' in directions:\n            action['angular_velocity'] = 0.5\n        elif 'right' in directions:\n            action['angular_velocity'] = -0.5\n\n        # Use numbers to determine duration or distance\n        if numbers:\n            # Assume first number is distance or duration\n            distance_or_duration = numbers[0]\n            if distance_or_duration < 5:  # Likely duration in seconds\n                action['duration'] = distance_or_duration\n            else:  # Likely distance in meters\n                action['duration'] = distance_or_duration / 0.5  # distance / speed\n\n        return action\n\n    def map_manipulation_action(self, parsed_instruction: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Map manipulation instruction to manipulation action.\"\"\"\n        action = {\n            'type': 'manipulation',\n            'gripper_position': 0.0,  # Open\n            'joint_positions': [0.0] * 6,  # Default joint positions\n            'force_limit': 50.0\n        }\n\n        instruction = parsed_instruction['original'].lower()\n\n        if 'grasp' in instruction or 'pick' in instruction or 'hold' in instruction:\n            action['gripper_position'] = 1.0  # Close gripper\n        elif 'release' in instruction or 'drop' in instruction:\n            action['gripper_position'] = 0.0  # Open gripper\n\n        # Adjust joint positions based on object properties\n        objects = parsed_instruction['objects']\n        for obj in objects:\n            if 'cup' in obj or 'glass' in obj:\n                # Special grip for cup-like objects\n                action['joint_positions'] = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n\n        return action\n\n    def map_placement_action(self, parsed_instruction: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Map placement instruction to placement action.\"\"\"\n        action = {\n            'type': 'placement',\n            'target_surface': 'table',\n            'position_offset': [0.0, 0.0, 0.0],\n            'orientation': [0.0, 0.0, 0.0, 1.0]  # quaternion\n        }\n\n        spatial_relations = parsed_instruction['spatial_relations']\n        objects = parsed_instruction['objects']\n\n        if 'table' in objects or 'desk' in objects:\n            action['target_surface'] = 'table'\n        elif 'shelf' in objects or 'cabinet' in objects:\n            action['target_surface'] = 'shelf'\n\n        # Spatial relations affect placement\n        if 'on' in spatial_relations or 'onto' in spatial_relations:\n            action['position_offset'][2] = 0.1  # Lift slightly above surface\n        elif 'under' in spatial_relations:\n            action['position_offset'][2] = -0.1  # Below surface\n\n        return action\n\n    def map_rotation_action(self, parsed_instruction: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Map rotation instruction to rotation action.\"\"\"\n        action = {\n            'type': 'rotation',\n            'axis': 'z',  # Default to yaw rotation\n            'angle': 0.0,\n            'angular_velocity': 0.1\n        }\n\n        instruction = parsed_instruction['original'].lower()\n        numbers = parsed_instruction['numbers']\n\n        if 'degrees' in instruction or '\xb0' in instruction:\n            if numbers:\n                action['angle'] = np.radians(numbers[0])  # Convert degrees to radians\n        elif 'radians' in instruction:\n            if numbers:\n                action['angle'] = numbers[0]\n        else:\n            # Default to 90 degrees if not specified\n            action['angle'] = np.radians(90)\n\n        # Determine rotation axis\n        if 'yaw' in instruction or 'turn' in instruction:\n            action['axis'] = 'z'\n        elif 'pitch' in instruction:\n            action['axis'] = 'y'\n        elif 'roll' in instruction:\n            action['axis'] = 'x'\n\n        return action\n\n    def map_generic_action(self, parsed_instruction: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Map generic instruction to default action.\"\"\"\n        return {\n            'type': 'generic',\n            'parameters': {},\n            'confidence': 0.5\n        }\n\n    def execute_action(self, action: Dict[str, Any], robot_interface):\n        \"\"\"Execute action using robot interface.\"\"\"\n        action_type = action['type']\n\n        if action_type == 'navigation':\n            return self.execute_navigation_action(action, robot_interface)\n        elif action_type == 'manipulation':\n            return self.execute_manipulation_action(action, robot_interface)\n        elif action_type == 'placement':\n            return self.execute_placement_action(action, robot_interface)\n        elif action_type == 'rotation':\n            return self.execute_rotation_action(action, robot_interface)\n        else:\n            print(f\"Unknown action type: {action_type}\")\n            return False\n\n    def execute_navigation_action(self, action: Dict[str, Any], robot_interface):\n        \"\"\"Execute navigation action.\"\"\"\n        print(f\"Executing navigation: linear_vel={action['linear_velocity']}, \"\n              f\"angular_vel={action['angular_velocity']}, duration={action['duration']}\")\n        # In practice, send commands to robot\n        return True\n\n    def execute_manipulation_action(self, action: Dict[str, Any], robot_interface):\n        \"\"\"Execute manipulation action.\"\"\"\n        print(f\"Executing manipulation: gripper_pos={action['gripper_position']}\")\n        # In practice, control robot manipulator\n        return True\n\n    def execute_placement_action(self, action: Dict[str, Any], robot_interface):\n        \"\"\"Execute placement action.\"\"\"\n        print(f\"Executing placement: surface={action['target_surface']}\")\n        # In practice, execute placement sequence\n        return True\n\n    def execute_rotation_action(self, action: Dict[str, Any], robot_interface):\n        \"\"\"Execute rotation action.\"\"\"\n        print(f\"Executing rotation: axis={action['axis']}, angle={action['angle']}\")\n        # In practice, rotate robot/base\n        return True\n\n\nclass VLAActionSystem:\n    \"\"\"Complete VLA action system combining parsing and execution.\"\"\"\n\n    def __init__(self):\n        self.action_mapper = ActionMapper()\n        self.execution_history = []\n\n    def process_instruction(self, instruction: str, robot_interface=None):\n        \"\"\"Process natural language instruction and execute action.\"\"\"\n        print(f\"Processing instruction: '{instruction}'\")\n\n        # Parse instruction\n        parsed = self.action_mapper.parse_instruction(instruction)\n        print(f\"Parsed instruction: {parsed}\")\n\n        # Map to action space\n        action = self.action_mapper.map_to_action_space(parsed)\n        print(f\"Mapped action: {action}\")\n\n        # Execute action\n        if robot_interface:\n            success = self.action_mapper.execute_action(action, robot_interface)\n        else:\n            # Simulate execution\n            success = True\n            print(\"(Simulated execution)\")\n\n        # Record in history\n        history_entry = {\n            'instruction': instruction,\n            'parsed': parsed,\n            'action': action,\n            'success': success,\n            'timestamp': np.datetime64('now')\n        }\n        self.execution_history.append(history_entry)\n\n        return {\n            'success': success,\n            'action': action,\n            'parsed_instruction': parsed\n        }\n\n    def get_execution_history(self):\n        \"\"\"Get execution history.\"\"\"\n        return self.execution_history\n\n    def evaluate_action_quality(self, instruction: str, expected_outcome: str) -> float:\n        \"\"\"Evaluate the quality of action execution.\"\"\"\n        # This would involve comparing expected vs actual outcomes\n        # For now, return a dummy score\n        return 0.85  # 85% quality score\n\n\n# Example usage\nif __name__ == \"__main__\":\n    print(\"Action Mapping System Example\")\n\n    # Create action system\n    action_system = VLAActionSystem()\n\n    # Test instructions\n    test_instructions = [\n        \"Move forward 2 meters\",\n        \"Turn left 90 degrees\",\n        \"Pick up the red cup\",\n        \"Place the object on the table\",\n        \"Navigate to the kitchen and bring the cup\"\n    ]\n\n    print(\"\\nTesting action mapping system:\")\n    for instruction in test_instructions:\n        result = action_system.process_instruction(instruction)\n        print(f\"Instruction: '{instruction}' -> Success: {result['success']}\")\n        print(f\"  Action: {result['action']['type']}\")\n        print()\n\n    # Show execution history\n    history = action_system.get_execution_history()\n    print(f\"Execution history contains {len(history)} entries\")\n\n    # Evaluate some actions\n    print(\"\\nEvaluating action quality:\")\n    for instruction in test_instructions[:3]:  # Test first 3\n        quality = action_system.evaluate_action_quality(instruction, \"successful\")\n        print(f\"  '{instruction}': Quality = {quality:.2f}\")\n\n    print(\"\\nAction mapping system example completed successfully\")\n"})}),"\n",(0,i.jsx)(e.h2,{id:"exercise-5-vla-evaluation-and-validation",children:"Exercise 5: VLA Evaluation and Validation"}),"\n",(0,i.jsx)(e.h3,{id:"step-1-create-evaluation-metrics",children:"Step 1: Create evaluation metrics"}),"\n",(0,i.jsxs)(e.p,{children:["Create ",(0,i.jsx)(e.code,{children:"~/vla_examples/vla_evaluation.py"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\n# vla_evaluation.py\n\"\"\"Evaluation and validation system for VLA systems.\"\"\"\n\nimport torch\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import defaultdict\nimport json\n\nclass VLAEvaluator:\n    \"\"\"Evaluation system for Vision-Language-Action models.\"\"\"\n\n    def __init__(self):\n        self.metrics = {\n            'accuracy': [],\n            'precision': [],\n            'recall': [],\n            'f1_score': [],\n            'similarity_score': [],\n            'language_alignment': [],\n            'action_success_rate': [],\n            'response_time': [],\n            'safety_violations': []\n        }\n        self.evaluation_history = []\n\n    def evaluate_action_prediction(self, predicted_actions, ground_truth_actions):\n        \"\"\"Evaluate action prediction accuracy.\"\"\"\n        if len(predicted_actions) != len(ground_truth_actions):\n            raise ValueError(\"Predicted and ground truth actions must have same length\")\n\n        # Calculate accuracy\n        correct = 0\n        for pred, gt in zip(predicted_actions, ground_truth_actions):\n            if self.action_equal(pred, gt):\n                correct += 1\n\n        accuracy = correct / len(predicted_actions) if predicted_actions else 0.0\n\n        # Calculate precision, recall, F1 (for categorical actions)\n        if ground_truth_actions and all(isinstance(gt, (int, str)) for gt in ground_truth_actions):\n            y_true = ground_truth_actions\n            y_pred = predicted_actions[:len(ground_truth_actions)]  # Ensure same length\n\n            try:\n                precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n                recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n                f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n            except:\n                precision = recall = f1 = 0.0\n        else:\n            precision = recall = f1 = 0.0\n\n        return {\n            'accuracy': accuracy,\n            'precision': precision,\n            'recall': recall,\n            'f1_score': f1\n        }\n\n    def action_equal(self, action1, action2):\n        \"\"\"Check if two actions are equal.\"\"\"\n        if isinstance(action1, dict) and isinstance(action2, dict):\n            # Compare dictionary actions\n            return self.dict_equal(action1, action2)\n        elif isinstance(action1, (list, tuple)) and isinstance(action2, (list, tuple)):\n            # Compare sequence actions\n            return np.allclose(action1, action2, rtol=0.1)  # 10% tolerance\n        else:\n            # Compare scalar actions\n            return action1 == action2\n\n    def dict_equal(self, dict1, dict2, tolerance=0.1):\n        \"\"\"Compare dictionaries with numerical tolerance.\"\"\"\n        if set(dict1.keys()) != set(dict2.keys()):\n            return False\n\n        for key in dict1.keys():\n            val1, val2 = dict1[key], dict2[key]\n            if isinstance(val1, (int, float)) and isinstance(val2, (int, float)):\n                if abs(val1 - val2) > tolerance:\n                    return False\n            elif val1 != val2:\n                return False\n\n        return True\n\n    def evaluate_language_alignment(self, instructions, predicted_actions):\n        \"\"\"Evaluate how well actions align with language instructions.\"\"\"\n        alignment_scores = []\n\n        for instruction, action in zip(instructions, predicted_actions):\n            score = self.compute_alignment_score(instruction, action)\n            alignment_scores.append(score)\n\n        avg_alignment = np.mean(alignment_scores) if alignment_scores else 0.0\n\n        return {\n            'alignment_scores': alignment_scores,\n            'average_alignment': avg_alignment,\n            'std_alignment': np.std(alignment_scores) if alignment_scores else 0.0\n        }\n\n    def compute_alignment_score(self, instruction, action):\n        \"\"\"Compute alignment score between instruction and action.\"\"\"\n        # This is a simplified scoring - in practice, use more sophisticated methods\n        instruction_lower = instruction.lower()\n        action_str = str(action).lower()\n\n        # Count relevant keywords\n        keywords = ['move', 'go', 'turn', 'pick', 'place', 'grasp', 'navigate']\n        instruction_keywords = sum(1 for keyword in keywords if keyword in instruction_lower)\n        action_keywords = sum(1 for keyword in keywords if keyword in action_str)\n\n        # Simple overlap score\n        if instruction_keywords == 0:\n            return 0.5  # Neutral if no keywords in instruction\n\n        overlap = min(instruction_keywords, action_keywords)\n        score = overlap / instruction_keywords\n\n        return min(1.0, max(0.0, score))\n\n    def evaluate_action_success_rate(self, execution_results):\n        \"\"\"Evaluate action execution success rate.\"\"\"\n        if not execution_results:\n            return 0.0\n\n        successful_executions = sum(1 for result in execution_results if result.get('success', False))\n        success_rate = successful_executions / len(execution_results)\n\n        return success_rate\n\n    def evaluate_similarity(self, predicted_features, ground_truth_features):\n        \"\"\"Evaluate similarity between predicted and ground truth features.\"\"\"\n        if predicted_features is None or ground_truth_features is None:\n            return 0.0\n\n        # Compute cosine similarity\n        if isinstance(predicted_features, torch.Tensor):\n            predicted_features = predicted_features.detach().cpu().numpy()\n        if isinstance(ground_truth_features, torch.Tensor):\n            ground_truth_features = ground_truth_features.detach().cpu().numpy()\n\n        # Normalize features\n        pred_norm = predicted_features / (np.linalg.norm(predicted_features, axis=-1, keepdims=True) + 1e-8)\n        gt_norm = ground_truth_features / (np.linalg.norm(ground_truth_features, axis=-1, keepdims=True) + 1e-8)\n\n        # Compute similarity\n        similarity = np.sum(pred_norm * gt_norm, axis=-1)\n        avg_similarity = np.mean(similarity) if len(similarity) > 0 else 0.0\n\n        return avg_similarity\n\n    def run_comprehensive_evaluation(self, model, test_dataset):\n        \"\"\"Run comprehensive evaluation on test dataset.\"\"\"\n        print(\"Running comprehensive VLA evaluation...\")\n\n        all_metrics = defaultdict(list)\n        execution_results = []\n\n        for sample in test_dataset:\n            # Process sample\n            image = sample['image']\n            instruction = sample['instruction']\n            ground_truth_action = sample['action']\n\n            # Get model prediction\n            predicted_action = self.get_model_prediction(model, image, instruction)\n\n            # Evaluate action prediction\n            action_metrics = self.evaluate_action_prediction([predicted_action], [ground_truth_action])\n            for metric, value in action_metrics.items():\n                all_metrics[metric].append(value)\n\n            # Evaluate language alignment\n            alignment_metrics = self.evaluate_language_alignment([instruction], [predicted_action])\n            all_metrics['language_alignment'].append(alignment_metrics['average_alignment'])\n\n            # Record execution result\n            execution_result = {\n                'instruction': instruction,\n                'predicted_action': predicted_action,\n                'ground_truth': ground_truth_action,\n                'success': self.action_equal(predicted_action, ground_truth_action)\n            }\n            execution_results.append(execution_result)\n\n        # Calculate final metrics\n        final_metrics = {}\n        for metric, values in all_metrics.items():\n            if values:\n                final_metrics[metric] = np.mean(values)\n                final_metrics[f'{metric}_std'] = np.std(values)\n\n        # Calculate success rate\n        final_metrics['action_success_rate'] = self.evaluate_action_success_rate(execution_results)\n\n        # Add to evaluation history\n        evaluation_result = {\n            'timestamp': np.datetime64('now'),\n            'metrics': final_metrics,\n            'execution_results': execution_results,\n            'dataset_size': len(test_dataset)\n        }\n        self.evaluation_history.append(evaluation_result)\n\n        return final_metrics, evaluation_result\n\n    def get_model_prediction(self, model, image, instruction):\n        \"\"\"Get prediction from model.\"\"\"\n        # This would call the actual model\n        # For simulation, return a dummy prediction\n        return {\n            'type': 'navigation',\n            'linear_velocity': 0.5,\n            'angular_velocity': 0.0,\n            'duration': 2.0\n        }\n\n    def visualize_evaluation_results(self, metrics, save_path='vla_evaluation_results.png'):\n        \"\"\"Visualize evaluation results.\"\"\"\n        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n        axes = axes.flatten()\n\n        # Metric names to plot\n        metric_names = ['accuracy', 'precision', 'recall', 'f1_score', 'language_alignment', 'action_success_rate']\n\n        # Plot metrics\n        for i, metric in enumerate(metric_names):\n            if metric in metrics:\n                value = metrics[metric]\n                std_value = metrics.get(f'{metric}_std', 0)\n\n                axes[i].bar([metric], [value], yerr=[std_value], capsize=5)\n                axes[i].set_title(f'{metric.replace(\"_\", \" \").title()}')\n                axes[i].set_ylim(0, 1)\n                axes[i].set_ylabel('Score')\n\n                # Add value label\n                axes[i].text(0, value + std_value + 0.02, f'{value:.3f}',\n                           ha='center', va='bottom', fontsize=10)\n\n        # Remove unused subplot\n        axes[5].remove()\n\n        plt.tight_layout()\n        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n        plt.show()\n\n        print(f\"Evaluation visualization saved to: {save_path}\")\n\n    def generate_evaluation_report(self, metrics, save_path='vla_evaluation_report.json'):\n        \"\"\"Generate comprehensive evaluation report.\"\"\"\n        report = {\n            'timestamp': str(np.datetime64('now')),\n            'metrics': metrics,\n            'evaluation_summary': self.summarize_metrics(metrics),\n            'recommendations': self.generate_recommendations(metrics)\n        }\n\n        with open(save_path, 'w') as f:\n            json.dump(report, f, indent=2)\n\n        print(f\"Evaluation report saved to: {save_path}\")\n        return report\n\n    def summarize_metrics(self, metrics):\n        \"\"\"Summarize evaluation metrics.\"\"\"\n        summary = {\n            'overall_performance': 'good' if metrics.get('accuracy', 0) > 0.7 else 'needs_improvement',\n            'language_alignment_quality': 'good' if metrics.get('language_alignment', 0) > 0.6 else 'poor',\n            'action_success_rate': f\"{metrics.get('action_success_rate', 0):.2%}\",\n            'technical_indicators': {}\n        }\n\n        for metric, value in metrics.items():\n            if isinstance(value, (int, float)) and not metric.endswith('_std'):\n                summary['technical_indicators'][metric] = f\"{value:.3f}\"\n\n        return summary\n\n    def generate_recommendations(self, metrics):\n        \"\"\"Generate recommendations based on metrics.\"\"\"\n        recommendations = []\n\n        if metrics.get('accuracy', 0) < 0.7:\n            recommendations.append(\"Model accuracy is below threshold. Consider retraining with more diverse data.\")\n        if metrics.get('language_alignment', 0) < 0.6:\n            recommendations.append(\"Language alignment needs improvement. Consider better vision-language fusion.\")\n        if metrics.get('action_success_rate', 0) < 0.8:\n            recommendations.append(\"Action success rate is low. Investigate action space mapping.\")\n\n        return recommendations if recommendations else [\"Model performance is satisfactory.\"]\n\n\nclass SafetyEvaluator:\n    \"\"\"Safety evaluation for VLA systems.\"\"\"\n\n    def __init__(self):\n        self.safety_rules = [\n            self.check_collision_risk,\n            self.check_stability,\n            self.check_joint_limits,\n            self.check_velocity_limits\n        ]\n\n    def check_collision_risk(self, action, environment_state):\n        \"\"\"Check if action poses collision risk.\"\"\"\n        # Simplified collision check\n        if 'linear_velocity' in action and abs(action['linear_velocity']) > 0.8:\n            return False  # High velocity might cause collision\n        return True\n\n    def check_stability(self, action, robot_state):\n        \"\"\"Check if action maintains robot stability.\"\"\"\n        # Simplified stability check\n        if 'angular_velocity' in action and abs(action['angular_velocity']) > 0.5:\n            return False  # High angular velocity might cause instability\n        return True\n\n    def check_joint_limits(self, action, robot_state):\n        \"\"\"Check if action violates joint limits.\"\"\"\n        if 'joint_positions' in action:\n            joint_positions = action['joint_positions']\n            # Check if any joint position is outside safe range\n            for pos in joint_positions:\n                if abs(pos) > 2.0:  # Example limit\n                    return False\n        return True\n\n    def check_velocity_limits(self, action, robot_state):\n        \"\"\"Check if action violates velocity limits.\"\"\"\n        if 'linear_velocity' in action and abs(action['linear_velocity']) > 1.0:\n            return False  # Exceeds velocity limit\n        if 'angular_velocity' in action and abs(action['angular_velocity']) > 0.5:\n            return False  # Exceeds angular velocity limit\n        return True\n\n    def evaluate_safety(self, action, environment_state, robot_state):\n        \"\"\"Evaluate action safety.\"\"\"\n        safety_results = {}\n        is_safe = True\n\n        for rule in self.safety_rules:\n            try:\n                rule_name = rule.__name__\n                is_safe_rule = rule(action, robot_state)\n                safety_results[rule_name] = is_safe_rule\n                is_safe = is_safe and is_safe_rule\n            except Exception as e:\n                print(f\"Safety rule {rule.__name__} failed: {e}\")\n                safety_results[rule.__name__] = False\n                is_safe = False\n\n        return {\n            'is_safe': is_safe,\n            'safety_results': safety_results,\n            'risk_score': 1.0 - float(is_safe)  # Higher risk if not safe\n        }\n\n\n# Example usage\nif __name__ == \"__main__\":\n    print(\"VLA Evaluation System Example\")\n\n    # Create evaluator\n    evaluator = VLAEvaluator()\n\n    # Create safety evaluator\n    safety_evaluator = SafetyEvaluator()\n\n    # Simulate test dataset\n    test_dataset = [\n        {\n            'image': torch.randn(3, 224, 224),\n            'instruction': 'Move forward',\n            'action': {'type': 'navigation', 'linear_velocity': 0.5, 'angular_velocity': 0.0}\n        },\n        {\n            'image': torch.randn(3, 224, 224),\n            'instruction': 'Turn left',\n            'action': {'type': 'navigation', 'linear_velocity': 0.0, 'angular_velocity': 0.5}\n        },\n        {\n            'image': torch.randn(3, 224, 224),\n            'instruction': 'Pick up object',\n            'action': {'type': 'manipulation', 'gripper_position': 1.0}\n        }\n    ]\n\n    print(f\"Testing with {len(test_dataset)} samples...\")\n\n    # Run evaluation (simulated)\n    dummy_metrics = {\n        'accuracy': 0.85,\n        'precision': 0.82,\n        'recall': 0.88,\n        'f1_score': 0.85,\n        'language_alignment': 0.78,\n        'action_success_rate': 0.83,\n        'accuracy_std': 0.05,\n        'precision_std': 0.06,\n        'recall_std': 0.04,\n        'f1_score_std': 0.05,\n        'language_alignment_std': 0.08,\n        'action_success_rate_std': 0.07\n    }\n\n    print(\"\\nEvaluation Results:\")\n    for metric, value in dummy_metrics.items():\n        print(f\"  {metric}: {value:.4f}\")\n\n    # Test safety evaluation\n    print(\"\\nTesting safety evaluation...\")\n    test_action = {\n        'type': 'navigation',\n        'linear_velocity': 0.5,\n        'angular_velocity': 0.2,\n        'joint_positions': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n    }\n\n    safety_result = safety_evaluator.evaluate_safety(test_action, {}, {})\n    print(f\"Safety evaluation result: {safety_result}\")\n\n    # Visualize results (would use dummy metrics in this example)\n    print(\"\\nGenerating evaluation visualization...\")\n    evaluator.visualize_evaluation_results(dummy_metrics)\n\n    # Generate report\n    print(\"\\nGenerating evaluation report...\")\n    report = evaluator.generate_evaluation_report(dummy_metrics)\n\n    print(\"\\nVLA evaluation system example completed successfully\")\n"})}),"\n",(0,i.jsx)(e.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,i.jsx)(e.h3,{id:"common-issues-and-solutions",children:"Common Issues and Solutions"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Poor Language-Action Alignment"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Increase training data diversity"}),"\n",(0,i.jsx)(e.li,{children:"Use better vision-language pre-trained models"}),"\n",(0,i.jsx)(e.li,{children:"Implement fine-grained alignment loss functions"}),"\n",(0,i.jsx)(e.li,{children:"Use reinforcement learning for policy improvement"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Action Space Mismatch"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Ensure consistent action space definitions"}),"\n",(0,i.jsx)(e.li,{children:"Implement proper action space mapping"}),"\n",(0,i.jsx)(e.li,{children:"Use action discretization for complex continuous spaces"}),"\n",(0,i.jsx)(e.li,{children:"Validate action feasibility before execution"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Vision-Language Fusion Issues"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Verify feature dimensions match across modalities"}),"\n",(0,i.jsx)(e.li,{children:"Use proper normalization for features"}),"\n",(0,i.jsx)(e.li,{children:"Implement attention mechanisms for better fusion"}),"\n",(0,i.jsx)(e.li,{children:"Add cross-modal regularization"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Real-World Transfer Problems"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Use domain randomization in training"}),"\n",(0,i.jsx)(e.li,{children:"Implement sim-to-real adaptation techniques"}),"\n",(0,i.jsx)(e.li,{children:"Collect real-world data for fine-tuning"}),"\n",(0,i.jsx)(e.li,{children:"Use robust feature extraction methods"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Performance Bottlenecks"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Optimize model inference with quantization"}),"\n",(0,i.jsx)(e.li,{children:"Use efficient attention mechanisms"}),"\n",(0,i.jsx)(e.li,{children:"Implement caching for repeated computations"}),"\n",(0,i.jsx)(e.li,{children:"Consider model compression techniques"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"assessment-questions",children:"Assessment Questions"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"How do you evaluate the alignment between language instructions and robot actions?"}),"\n",(0,i.jsx)(e.li,{children:"What are the key challenges in fusing vision and language modalities?"}),"\n",(0,i.jsx)(e.li,{children:"How would you handle ambiguous language instructions in VLA systems?"}),"\n",(0,i.jsx)(e.li,{children:"What metrics would you use to evaluate VLA system performance?"}),"\n",(0,i.jsx)(e.li,{children:"How do you ensure safety in VLA system execution?"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"extension-exercises",children:"Extension Exercises"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"Implement a transformer-based VLA architecture"}),"\n",(0,i.jsx)(e.li,{children:"Create a multimodal dataset for VLA training"}),"\n",(0,i.jsx)(e.li,{children:"Implement domain adaptation for sim-to-real transfer"}),"\n",(0,i.jsx)(e.li,{children:"Create a safety-aware VLA system"}),"\n",(0,i.jsx)(e.li,{children:"Implement reinforcement learning for VLA policy improvement"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(e.p,{children:"In this lab, you successfully:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Implemented VLA system architecture with vision, language, and action components"}),"\n",(0,i.jsx)(e.li,{children:"Integrated CLIP for vision-language understanding"}),"\n",(0,i.jsx)(e.li,{children:"Created advanced multimodal fusion techniques"}),"\n",(0,i.jsx)(e.li,{children:"Developed action mapping from language to robot actions"}),"\n",(0,i.jsx)(e.li,{children:"Implemented evaluation and validation systems for VLA performance"}),"\n",(0,i.jsx)(e.li,{children:"Created safety evaluation mechanisms"}),"\n"]}),"\n",(0,i.jsx)(e.p,{children:"These skills are fundamental for developing Vision-Language-Action systems that can understand natural language instructions, perceive visual information, and execute appropriate robotic actions. The combination of multimodal understanding, action planning, and safety considerations forms the foundation for building intelligent robotic systems that can interact naturally with humans in real-world environments."})]})}function d(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(u,{...n})}):u(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>o,x:()=>r});var i=t(6540);const a={},s=i.createContext(a);function o(n){const e=i.useContext(s);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:o(n.components),i.createElement(s.Provider,{value:e},n.children)}}}]);