"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[7240],{548:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>t,default:()=>h,frontMatter:()=>o,metadata:()=>a,toc:()=>c});var r=i(4848),s=i(8453);const o={},t="Vision-Language-Action (VLA) System References",a={id:"modules/vla-system/references",title:"Vision-Language-Action (VLA) System References",description:"Academic Papers and Research",source:"@site/docs/modules/vla-system/references.md",sourceDirName:"modules/vla-system",slug:"/modules/vla-system/references",permalink:"/ai-robotic-book/modules/vla-system/references",draft:!1,unlisted:!1,editUrl:"https://github.com/your-org/physical-ai-book/tree/main/docs/modules/vla-system/references.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Lab 4.3: Action Mapping in Vision-Language-Action Systems",permalink:"/ai-robotic-book/modules/lab-exercises/lab-4-3-action-mapping"},next:{title:"Safety and Evaluation",permalink:"/ai-robotic-book/modules/vla-system/safety-evaluation"}},l={},c=[{value:"Academic Papers and Research",id:"academic-papers-and-research",level:2},{value:"Foundational VLA Research",id:"foundational-vla-research",level:3},{value:"Vision-Language Models for Robotics",id:"vision-language-models-for-robotics",level:3},{value:"Multimodal Learning for Robotics",id:"multimodal-learning-for-robotics",level:3},{value:"Language-Guided Robot Control",id:"language-guided-robot-control",level:3},{value:"Technical Documentation",id:"technical-documentation",level:2},{value:"ROS 2 Integration",id:"ros-2-integration",level:3},{value:"Deep Learning Frameworks",id:"deep-learning-frameworks",level:3},{value:"Vision-Language Models",id:"vision-language-models",level:3},{value:"Simulation and Development Tools",id:"simulation-and-development-tools",level:2},{value:"Isaac Sim and NVIDIA Tools",id:"isaac-sim-and-nvidia-tools",level:3},{value:"Gazebo and Simulation",id:"gazebo-and-simulation",level:3},{value:"Unity and Perception",id:"unity-and-perception",level:3},{value:"Standards and Best Practices",id:"standards-and-best-practices",level:2},{value:"Robotics Standards",id:"robotics-standards",level:3},{value:"AI Safety and Ethics",id:"ai-safety-and-ethics",level:3},{value:"Software Engineering Best Practices",id:"software-engineering-best-practices",level:3},{value:"Open Source Projects and Libraries",id:"open-source-projects-and-libraries",level:2},{value:"Vision-Language-Action Libraries",id:"vision-language-action-libraries",level:3},{value:"Robotics Libraries",id:"robotics-libraries",level:3},{value:"Deep Learning Libraries",id:"deep-learning-libraries",level:3},{value:"Tutorials and Learning Resources",id:"tutorials-and-learning-resources",level:2},{value:"VLA-Specific Tutorials",id:"vla-specific-tutorials",level:3},{value:"Robotics Education",id:"robotics-education",level:3},{value:"Deep Learning for Robotics",id:"deep-learning-for-robotics",level:3},{value:"Industry Applications and Case Studies",id:"industry-applications-and-case-studies",level:2},{value:"Commercial VLA Systems",id:"commercial-vla-systems",level:3},{value:"Research Institutions",id:"research-institutions",level:3},{value:"Evaluation and Benchmarking",id:"evaluation-and-benchmarking",level:2},{value:"Robotics Benchmarks",id:"robotics-benchmarks",level:3},{value:"Vision-Language Benchmarks",id:"vision-language-benchmarks",level:3},{value:"Safety and Security",id:"safety-and-security",level:2},{value:"Safety Standards",id:"safety-standards",level:3},{value:"Security Guidelines",id:"security-guidelines",level:3},{value:"Conferences and Journals",id:"conferences-and-journals",level:2},{value:"Top Robotics Conferences",id:"top-robotics-conferences",level:3},{value:"Top AI/Machine Learning Conferences",id:"top-aimachine-learning-conferences",level:3},{value:"Top Journals",id:"top-journals",level:3},{value:"Additional Resources",id:"additional-resources",level:2},{value:"Online Courses and MOOCs",id:"online-courses-and-moocs",level:3},{value:"Community Resources",id:"community-resources",level:3}];function d(e){const n={a:"a",em:"em",h1:"h1",h2:"h2",h3:"h3",li:"li",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h1,{id:"vision-language-action-vla-system-references",children:"Vision-Language-Action (VLA) System References"}),"\n",(0,r.jsx)(n.h2,{id:"academic-papers-and-research",children:"Academic Papers and Research"}),"\n",(0,r.jsx)(n.h3,{id:"foundational-vla-research",children:"Foundational VLA Research"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Zhu, Y., Zeng, A., Joshi, S., Chen, S., Dogra, K., Welle, J., ... & Tellex, S. (2022). RT-1: Robotics transformer for real-world control at scale. ",(0,r.jsx)(n.em,{children:"arXiv preprint arXiv:2208.01871"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["Brohan, A., Brown, J., Carbajal, J., Chebotar, Y., Dabis, J., Finn, C., ... & Zhu, Y. (2022). RVT: Robotic view transformation learning from unimodal demonstrations. ",(0,r.jsx)(n.em,{children:"arXiv preprint arXiv:2209.11133"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["Chen, M., Mishra, A., Gupta, A., Devin, C., Zhu, Y., Turck, L., ... & Levine, S. (2021). Learning transferable visual models from natural language supervision. ",(0,r.jsx)(n.em,{children:"International Conference on Machine Learning"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., ... & Sutskever, I. (2021). Learning transferable visual models from natural language supervision. ",(0,r.jsx)(n.em,{children:"International Conference on Machine Learning"}),"."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"vision-language-models-for-robotics",children:"Vision-Language Models for Robotics"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Radford, A., et al. (2021). Learning transferable visual models from natural language supervision. ",(0,r.jsx)(n.em,{children:"Proceedings of the International Conference on Machine Learning (ICML)"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["Li, J., Li, D., Savarese, S., & Hoi, S. (2022). BLIP: Bootstrapping language-image pre-training for unified vision-language understanding and generation. ",(0,r.jsx)(n.em,{children:"International Conference on Machine Learning"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["Liu, J., Jia, J., Liu, Y., & Tuzel, O. (2019). Visual semantic search with vision-language models. ",(0,r.jsx)(n.em,{children:"arXiv preprint arXiv:1905.12384"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["Chen, X., et al. (2020). An empirical study of training end-to-end vision-and-language transformers. ",(0,r.jsx)(n.em,{children:"arXiv preprint arXiv:2112.05253"}),"."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"multimodal-learning-for-robotics",children:"Multimodal Learning for Robotics"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Misra, I., Maaten, L. V. D., Efros, A. A., & He, K. (2018). Cross-modal alignment of video and language. ",(0,r.jsx)(n.em,{children:"Proceedings of the European Conference on Computer Vision (ECCV)"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["Alayrac, J. B., et al. (2022). Self-supervised learning of image and video representations by predicting future visual representations. ",(0,r.jsx)(n.em,{children:"arXiv preprint arXiv:2204.07152"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["Kolve, E., et al. (2017). AI2-THOR: An interactive 3D environment for visual AI. ",(0,r.jsx)(n.em,{children:"arXiv preprint arXiv:1712.05474"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["Zhu, Y., et al. (2017). Target-driven visual navigation in indoor scenes using deep reinforcement learning. ",(0,r.jsx)(n.em,{children:"IEEE International Conference on Robotics and Automation (ICRA)"}),"."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"language-guided-robot-control",children:"Language-Guided Robot Control"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Hermann, K., et al. (2017). Grounded language learning in a simulated 3D world. ",(0,r.jsx)(n.em,{children:"arXiv preprint arXiv:1706.06551"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["Misra, D., Lang, A., & Artzi, Y. (2018). Mapping instructions and visual observations to actions with reinforcement learning. ",(0,r.jsx)(n.em,{children:"Conference on Robot Learning (CoRL)"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["Chen, X., et al. (2019). Task-oriented active learning for robot-assisted dressing. ",(0,r.jsx)(n.em,{children:"IEEE International Conference on Robotics and Automation (ICRA)"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["Tellex, S., et al. (2011). Understanding natural language commands for robotic navigation and mobile manipulation. ",(0,r.jsx)(n.em,{children:"AAAI Conference on Artificial Intelligence"}),"."]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"technical-documentation",children:"Technical Documentation"}),"\n",(0,r.jsx)(n.h3,{id:"ros-2-integration",children:"ROS 2 Integration"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["ROS 2 Documentation. (2023). Robot Operating System 2. Retrieved from ",(0,r.jsx)(n.a,{href:"https://docs.ros.org/en/humble/",children:"https://docs.ros.org/en/humble/"})]}),"\n",(0,r.jsxs)(n.li,{children:["ROS 2 Navigation. (2023). Navigation2 System Documentation. Retrieved from ",(0,r.jsx)(n.a,{href:"https://navigation.ros.org/",children:"https://navigation.ros.org/"})]}),"\n",(0,r.jsxs)(n.li,{children:["ROS 2 Control. (2023). ROS 2 Control Framework. Retrieved from ",(0,r.jsx)(n.a,{href:"https://control.ros.org/",children:"https://control.ros.org/"})]}),"\n",(0,r.jsxs)(n.li,{children:["ROS 2 Safety Working Group. (2023). Safety Guidelines for ROS 2. Retrieved from ",(0,r.jsx)(n.a,{href:"https://github.com/ros-safety",children:"https://github.com/ros-safety"})]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"deep-learning-frameworks",children:"Deep Learning Frameworks"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["PyTorch Documentation. (2023). PyTorch: Tensors and Dynamic neural networks in Python with GPU acceleration. Retrieved from ",(0,r.jsx)(n.a,{href:"https://pytorch.org/docs/stable/index.html",children:"https://pytorch.org/docs/stable/index.html"})]}),"\n",(0,r.jsxs)(n.li,{children:["TensorFlow Documentation. (2023). TensorFlow: Large-scale machine learning on heterogeneous systems. Retrieved from ",(0,r.jsx)(n.a,{href:"https://www.tensorflow.org/api_docs",children:"https://www.tensorflow.org/api_docs"})]}),"\n",(0,r.jsxs)(n.li,{children:["Hugging Face Transformers. (2023). State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow. Retrieved from ",(0,r.jsx)(n.a,{href:"https://huggingface.co/docs/transformers/index",children:"https://huggingface.co/docs/transformers/index"})]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"vision-language-models",children:"Vision-Language Models"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["OpenAI CLIP Documentation. (2023). Contrastive Language-Image Pre-training. Retrieved from ",(0,r.jsx)(n.a,{href:"https://github.com/openai/CLIP",children:"https://github.com/openai/CLIP"})]}),"\n",(0,r.jsxs)(n.li,{children:["BLIP Documentation. (2023). Bootstrapping Language-Image Pre-training. Retrieved from ",(0,r.jsx)(n.a,{href:"https://github.com/salesforce/BLIP",children:"https://github.com/salesforce/BLIP"})]}),"\n",(0,r.jsxs)(n.li,{children:["Vision Transformer (ViT) Documentation. (2023). An Image is Worth 16x16 Words. Retrieved from ",(0,r.jsx)(n.a,{href:"https://github.com/google-research/vision_transformer",children:"https://github.com/google-research/vision_transformer"})]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"simulation-and-development-tools",children:"Simulation and Development Tools"}),"\n",(0,r.jsx)(n.h3,{id:"isaac-sim-and-nvidia-tools",children:"Isaac Sim and NVIDIA Tools"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["NVIDIA Isaac Sim Documentation. (2023). NVIDIA Isaac Sim. Retrieved from ",(0,r.jsx)(n.a,{href:"https://docs.omniverse.nvidia.com/isaacsim/latest/",children:"https://docs.omniverse.nvidia.com/isaacsim/latest/"})]}),"\n",(0,r.jsxs)(n.li,{children:["Isaac ROS Documentation. (2023). NVIDIA Isaac ROS. Retrieved from ",(0,r.jsx)(n.a,{href:"https://nvidia-isaac-ros.github.io/",children:"https://nvidia-isaac-ros.github.io/"})]}),"\n",(0,r.jsxs)(n.li,{children:["Isaac Lab Documentation. (2023). NVIDIA Isaac Lab. Retrieved from ",(0,r.jsx)(n.a,{href:"https://isaac-sim.github.io/IsaacLab/",children:"https://isaac-sim.github.io/IsaacLab/"})]}),"\n",(0,r.jsxs)(n.li,{children:["NVIDIA Omniverse Documentation. (2023). NVIDIA Omniverse Platform. Retrieved from ",(0,r.jsx)(n.a,{href:"https://docs.omniverse.nvidia.com/",children:"https://docs.omniverse.nvidia.com/"})]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"gazebo-and-simulation",children:"Gazebo and Simulation"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Gazebo Documentation. (2023). Gazebo Robot Simulator. Retrieved from ",(0,r.jsx)(n.a,{href:"http://gazebosim.org/",children:"http://gazebosim.org/"})]}),"\n",(0,r.jsxs)(n.li,{children:["Gazebo Garden. (2023). Gazebo Garden User Guide. Retrieved from ",(0,r.jsx)(n.a,{href:"https://gazebosim.org/api/garden/",children:"https://gazebosim.org/api/garden/"})]}),"\n",(0,r.jsxs)(n.li,{children:["Robot Web Tools. (2023). Web-based robot interfaces. Retrieved from ",(0,r.jsx)(n.a,{href:"http://robotwebtools.org/",children:"http://robotwebtools.org/"})]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"unity-and-perception",children:"Unity and Perception"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Unity Robotics Hub. (2023). Unity for Robotics. Retrieved from ",(0,r.jsx)(n.a,{href:"https://unity.com/solutions/robotics",children:"https://unity.com/solutions/robotics"})]}),"\n",(0,r.jsxs)(n.li,{children:["Unity Perception Package. (2023). Synthetic Data Generation for Computer Vision. Retrieved from ",(0,r.jsx)(n.a,{href:"https://docs.unity3d.com/Packages/com.unity.perception@latest",children:"https://docs.unity3d.com/Packages/com.unity.perception@latest"})]}),"\n",(0,r.jsxs)(n.li,{children:["Unity ML-Agents Toolkit. (2023). Unity Machine Learning Agents Toolkit. Retrieved from ",(0,r.jsx)(n.a,{href:"https://github.com/Unity-Technologies/ml-agents",children:"https://github.com/Unity-Technologies/ml-agents"})]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"standards-and-best-practices",children:"Standards and Best Practices"}),"\n",(0,r.jsx)(n.h3,{id:"robotics-standards",children:"Robotics Standards"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["IEEE Standards Association. (2023). IEEE Standards for Robotics. Retrieved from ",(0,r.jsx)(n.a,{href:"https://standards.ieee.org/industry-applications/robotics/",children:"https://standards.ieee.org/industry-applications/robotics/"})]}),"\n",(0,r.jsxs)(n.li,{children:["ISO Standards for Robotics. (2023). International Organization for Standardization - Robotics Standards. Retrieved from ",(0,r.jsx)(n.a,{href:"https://www.iso.org/standards.html",children:"https://www.iso.org/standards.html"})]}),"\n",(0,r.jsxs)(n.li,{children:["ROS Enhancement Proposals (REPs). (2023). ROS Standards and Conventions. Retrieved from ",(0,r.jsx)(n.a,{href:"https://ros.org/reps/",children:"https://ros.org/reps/"})]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"ai-safety-and-ethics",children:"AI Safety and Ethics"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Partnership on AI. (2023). AI Safety Guidelines. Retrieved from ",(0,r.jsx)(n.a,{href:"https://www.partnershiponai.org/",children:"https://www.partnershiponai.org/"})]}),"\n",(0,r.jsxs)(n.li,{children:["IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. (2023). Ethically Aligned Design. Retrieved from ",(0,r.jsx)(n.a,{href:"https://ethicsinaction.ieee.org/",children:"https://ethicsinaction.ieee.org/"})]}),"\n",(0,r.jsxs)(n.li,{children:["Future of Life Institute. (2023). AI Safety Research. Retrieved from ",(0,r.jsx)(n.a,{href:"https://futureoflife.org/background/benefits-risks-of-artificial-intelligence/",children:"https://futureoflife.org/background/benefits-risks-of-artificial-intelligence/"})]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"software-engineering-best-practices",children:"Software Engineering Best Practices"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["PEP 8 - Style Guide for Python Code. (2023). Python Software Foundation. Retrieved from ",(0,r.jsx)(n.a,{href:"https://peps.python.org/pep-0008/",children:"https://peps.python.org/pep-0008/"})]}),"\n",(0,r.jsxs)(n.li,{children:["Google Python Style Guide. (2023). Google Engineering Practices. Retrieved from ",(0,r.jsx)(n.a,{href:"https://google.github.io/styleguide/pyguide.html",children:"https://google.github.io/styleguide/pyguide.html"})]}),"\n",(0,r.jsxs)(n.li,{children:["ROS 2 Developer Guide. (2023). Best practices for ROS 2 development. Retrieved from ",(0,r.jsx)(n.a,{href:"https://docs.ros.org/en/humble/The-ROS2-Project/Contributing/Developer-Guide.html",children:"https://docs.ros.org/en/humble/The-ROS2-Project/Contributing/Developer-Guide.html"})]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"open-source-projects-and-libraries",children:"Open Source Projects and Libraries"}),"\n",(0,r.jsx)(n.h3,{id:"vision-language-action-libraries",children:"Vision-Language-Action Libraries"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["OpenVLA: Open Vision-Language-Action Models. (2023). Retrieved from ",(0,r.jsx)(n.a,{href:"https://github.com/openvla/openvla",children:"https://github.com/openvla/openvla"})]}),"\n",(0,r.jsxs)(n.li,{children:["Hugging Face Transformers. (2023). State-of-the-art Natural Language Processing. Retrieved from ",(0,r.jsx)(n.a,{href:"https://github.com/huggingface/transformers",children:"https://github.com/huggingface/transformers"})]}),"\n",(0,r.jsxs)(n.li,{children:["LAVIS: A Library for Language-and-Vision Intelligence. (2023). Retrieved from ",(0,r.jsx)(n.a,{href:"https://github.com/salesforce/LAVIS",children:"https://github.com/salesforce/LAVIS"})]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"robotics-libraries",children:"Robotics Libraries"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["MoveIt! Motion Planning Framework. (2023). Retrieved from ",(0,r.jsx)(n.a,{href:"https://moveit.ros.org/",children:"https://moveit.ros.org/"})]}),"\n",(0,r.jsxs)(n.li,{children:["PyRobot: A Software Framework for Robot Learning Research. (2023). Retrieved from ",(0,r.jsx)(n.a,{href:"https://github.com/facebookresearch/pyrobot",children:"https://github.com/facebookresearch/pyrobot"})]}),"\n",(0,r.jsxs)(n.li,{children:["Habitat-Sim: 3D Simulator for Embodied AI. (2023). Retrieved from ",(0,r.jsx)(n.a,{href:"https://aihabitat.org/",children:"https://aihabitat.org/"})]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"deep-learning-libraries",children:"Deep Learning Libraries"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Detectron2: FAIR's Next-Generation Platform for Object Detection and Segmentation. (2023). Retrieved from ",(0,r.jsx)(n.a,{href:"https://github.com/facebookresearch/detectron2",children:"https://github.com/facebookresearch/detectron2"})]}),"\n",(0,r.jsxs)(n.li,{children:["OpenCV: Open Source Computer Vision Library. (2023). Retrieved from ",(0,r.jsx)(n.a,{href:"https://opencv.org/",children:"https://opencv.org/"})]}),"\n",(0,r.jsxs)(n.li,{children:["PyTorch Geometric: Geometric Deep Learning Extension Library for PyTorch. (2023). Retrieved from ",(0,r.jsx)(n.a,{href:"https://pytorch-geometric.readthedocs.io/",children:"https://pytorch-geometric.readthedocs.io/"})]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"tutorials-and-learning-resources",children:"Tutorials and Learning Resources"}),"\n",(0,r.jsx)(n.h3,{id:"vla-specific-tutorials",children:"VLA-Specific Tutorials"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["NVIDIA Isaac Lab Tutorials. (2023). Retrieved from ",(0,r.jsx)(n.a,{href:"https://isaac-sim.github.io/IsaacLab/",children:"https://isaac-sim.github.io/IsaacLab/"})]}),"\n",(0,r.jsxs)(n.li,{children:["Hugging Face Course on Vision-Language Models. (2023). Retrieved from ",(0,r.jsx)(n.a,{href:"https://huggingface.co/course/chapter7/1",children:"https://huggingface.co/course/chapter7/1"})]}),"\n",(0,r.jsxs)(n.li,{children:["PyTorch Tutorial on Vision-Language Models. (2023). Retrieved from ",(0,r.jsx)(n.a,{href:"https://pytorch.org/tutorials/beginner/transformer_tutorial.html",children:"https://pytorch.org/tutorials/beginner/transformer_tutorial.html"})]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"robotics-education",children:"Robotics Education"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Modern Robotics: Mechanics, Planning, and Control by Lynch and Park. Cambridge University Press, 2017."}),"\n",(0,r.jsx)(n.li,{children:"Probabilistic Robotics by Thrun, Burgard, and Fox. MIT Press, 2005."}),"\n",(0,r.jsx)(n.li,{children:"Robot Learning by Sutton and Barto. MIT Press, 2018."}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"deep-learning-for-robotics",children:"Deep Learning for Robotics"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Deep Learning by Goodfellow, Bengio, and Courville. MIT Press, 2016."}),"\n",(0,r.jsx)(n.li,{children:"Reinforcement Learning: An Introduction by Sutton and Barto. MIT Press, 2018."}),"\n",(0,r.jsx)(n.li,{children:"Computer Vision: Algorithms and Applications by Szeliski. Springer, 2010."}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"industry-applications-and-case-studies",children:"Industry Applications and Case Studies"}),"\n",(0,r.jsx)(n.h3,{id:"commercial-vla-systems",children:"Commercial VLA Systems"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Boston Dynamics. (2023). Spot and Atlas Robots. Retrieved from ",(0,r.jsx)(n.a,{href:"https://www.bostondynamics.com/",children:"https://www.bostondynamics.com/"})]}),"\n",(0,r.jsxs)(n.li,{children:["Amazon Robotics. (2023). Robotic Systems for Warehousing. Retrieved from ",(0,r.jsx)(n.a,{href:"https://www.aboutamazon.com/company/innovation-at-amazon/amazon-robotics",children:"https://www.aboutamazon.com/company/innovation-at-amazon/amazon-robotics"})]}),"\n",(0,r.jsxs)(n.li,{children:["Fetch Robotics. (2023). Autonomous Mobile Robots. Retrieved from ",(0,r.jsx)(n.a,{href:"https://www.fetchrobotics.com/",children:"https://www.fetchrobotics.com/"})]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"research-institutions",children:"Research Institutions"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["MIT Computer Science and Artificial Intelligence Laboratory (CSAIL). (2023). Robotics Research. Retrieved from ",(0,r.jsx)(n.a,{href:"https://www.csail.mit.edu/",children:"https://www.csail.mit.edu/"})]}),"\n",(0,r.jsxs)(n.li,{children:["Stanford AI Lab. (2023). Vision and Learning Research. Retrieved from ",(0,r.jsx)(n.a,{href:"https://ai.stanford.edu/",children:"https://ai.stanford.edu/"})]}),"\n",(0,r.jsxs)(n.li,{children:["Google AI Robotics. (2023). Learning for Robotics. Retrieved from ",(0,r.jsx)(n.a,{href:"https://ai.google/research/teams/brain/robotics/",children:"https://ai.google/research/teams/brain/robotics/"})]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"evaluation-and-benchmarking",children:"Evaluation and Benchmarking"}),"\n",(0,r.jsx)(n.h3,{id:"robotics-benchmarks",children:"Robotics Benchmarks"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["RoboCup@Home. (2023). Benchmarking Service and Domestic Robots. Retrieved from ",(0,r.jsx)(n.a,{href:"https://athome.robocup.org/",children:"https://athome.robocup.org/"})]}),"\n",(0,r.jsxs)(n.li,{children:["Amazon Picking Challenge. (2023). Robotic Manipulation Benchmark. Retrieved from ",(0,r.jsx)(n.a,{href:"https://amazonpickingchallenge.org/",children:"https://amazonpickingchallenge.org/"})]}),"\n",(0,r.jsxs)(n.li,{children:["ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks. (2023). Retrieved from ",(0,r.jsx)(n.a,{href:"https://askforalfred.com/",children:"https://askforalfred.com/"})]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"vision-language-benchmarks",children:"Vision-Language Benchmarks"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["COCO Dataset. (2023). Common Objects in Context. Retrieved from ",(0,r.jsx)(n.a,{href:"https://cocodataset.org/",children:"https://cocodataset.org/"})]}),"\n",(0,r.jsxs)(n.li,{children:["VQA Dataset. (2023). Visual Question Answering. Retrieved from ",(0,r.jsx)(n.a,{href:"https://visualqa.org/",children:"https://visualqa.org/"})]}),"\n",(0,r.jsxs)(n.li,{children:["NLVR2 Dataset. (2023). Natural Language for Visual Reasoning. Retrieved from ",(0,r.jsx)(n.a,{href:"https://lil.nlp.cornell.edu/nlvr/",children:"https://lil.nlp.cornell.edu/nlvr/"})]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"safety-and-security",children:"Safety and Security"}),"\n",(0,r.jsx)(n.h3,{id:"safety-standards",children:"Safety Standards"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"ISO 10218-1:2011 - Robots and robotic devices \u2014 Safety requirements for industrial robots. International Organization for Standardization."}),"\n",(0,r.jsx)(n.li,{children:"ISO 13482:2014 - Robots and robotic devices \u2014 Safety requirements for personal care robots. International Organization for Standardization."}),"\n",(0,r.jsx)(n.li,{children:"IEC 61508: Functional safety of electrical/electronic/programmable electronic safety-related systems. International Electrotechnical Commission."}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"security-guidelines",children:"Security Guidelines"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["NIST Cybersecurity Framework. (2023). National Institute of Standards and Technology. Retrieved from ",(0,r.jsx)(n.a,{href:"https://www.nist.gov/cyberframework",children:"https://www.nist.gov/cyberframework"})]}),"\n",(0,r.jsxs)(n.li,{children:["OWASP Top 10 for IoT. (2023). Open Web Application Security Project. Retrieved from ",(0,r.jsx)(n.a,{href:"https://owasp.org/www-project-internet-of-things/",children:"https://owasp.org/www-project-internet-of-things/"})]}),"\n",(0,r.jsxs)(n.li,{children:["ROS 2 Security Working Group. (2023). Security Best Practices. Retrieved from ",(0,r.jsx)(n.a,{href:"https://github.com/ros-security",children:"https://github.com/ros-security"})]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"conferences-and-journals",children:"Conferences and Journals"}),"\n",(0,r.jsx)(n.h3,{id:"top-robotics-conferences",children:"Top Robotics Conferences"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"IEEE International Conference on Robotics and Automation (ICRA)"}),"\n",(0,r.jsx)(n.li,{children:"IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"}),"\n",(0,r.jsx)(n.li,{children:"Robotics: Science and Systems (RSS)"}),"\n",(0,r.jsx)(n.li,{children:"Conference on Robot Learning (CoRL)"}),"\n",(0,r.jsx)(n.li,{children:"International Symposium of Robotics Research (ISRR)"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"top-aimachine-learning-conferences",children:"Top AI/Machine Learning Conferences"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Conference on Neural Information Processing Systems (NeurIPS)"}),"\n",(0,r.jsx)(n.li,{children:"International Conference on Machine Learning (ICML)"}),"\n",(0,r.jsx)(n.li,{children:"International Conference on Learning Representations (ICLR)"}),"\n",(0,r.jsx)(n.li,{children:"AAAI Conference on Artificial Intelligence"}),"\n",(0,r.jsx)(n.li,{children:"International Joint Conference on Artificial Intelligence (IJCAI)"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"top-journals",children:"Top Journals"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"IEEE Transactions on Robotics"}),"\n",(0,r.jsx)(n.li,{children:"The International Journal of Robotics Research"}),"\n",(0,r.jsx)(n.li,{children:"Autonomous Robots"}),"\n",(0,r.jsx)(n.li,{children:"Journal of Field Robotics"}),"\n",(0,r.jsx)(n.li,{children:"IEEE Robotics and Automation Letters"}),"\n",(0,r.jsx)(n.li,{children:"Nature Machine Intelligence"}),"\n",(0,r.jsx)(n.li,{children:"Science Robotics"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"additional-resources",children:"Additional Resources"}),"\n",(0,r.jsx)(n.h3,{id:"online-courses-and-moocs",children:"Online Courses and MOOCs"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"CS229: Machine Learning - Stanford University"}),"\n",(0,r.jsx)(n.li,{children:"CS231n: Convolutional Neural Networks for Visual Recognition - Stanford University"}),"\n",(0,r.jsx)(n.li,{children:"MIT 6.034 Artificial Intelligence - MIT OpenCourseWare"}),"\n",(0,r.jsx)(n.li,{children:"Robotics Specialization - University of Pennsylvania (Coursera)"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"community-resources",children:"Community Resources"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["ROS Discourse. (2023). Community forum for ROS users. Retrieved from ",(0,r.jsx)(n.a,{href:"https://discourse.ros.org/",children:"https://discourse.ros.org/"})]}),"\n",(0,r.jsxs)(n.li,{children:["PyTorch Discuss. (2023). Community forum for PyTorch users. Retrieved from ",(0,r.jsx)(n.a,{href:"https://discuss.pytorch.org/",children:"https://discuss.pytorch.org/"})]}),"\n",(0,r.jsxs)(n.li,{children:["Reddit r/MachineLearning. (2023). Community for machine learning discussions. Retrieved from ",(0,r.jsx)(n.a,{href:"https://www.reddit.com/r/MachineLearning/",children:"https://www.reddit.com/r/MachineLearning/"})]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>a});var r=i(6540);const s={},o=r.createContext(s);function t(e){const n=r.useContext(o);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),r.createElement(o.Provider,{value:n},e.children)}}}]);