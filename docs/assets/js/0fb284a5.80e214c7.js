"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[5903],{2367:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>c,frontMatter:()=>t,metadata:()=>o,toc:()=>p});var r=i(4848),a=i(8453);const t={},s="Lab 3.2: Isaac Sim Perception Systems",o={id:"modules/lab-exercises/lab-3-2-perception-systems",title:"Lab 3.2: Isaac Sim Perception Systems",description:"Overview",source:"@site/docs/modules/lab-exercises/lab-3-2-perception-systems.md",sourceDirName:"modules/lab-exercises",slug:"/modules/lab-exercises/lab-3-2-perception-systems",permalink:"/hackathon-ai-book/modules/lab-exercises/lab-3-2-perception-systems",draft:!1,unlisted:!1,editUrl:"https://github.com/sanilahmed/hackathon-ai-book/tree/main/docs/modules/lab-exercises/lab-3-2-perception-systems.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Lab 3.1: Isaac Sim Setup and Environment",permalink:"/hackathon-ai-book/modules/lab-exercises/lab-3-1-isaac-sim-setup"},next:{title:"Lab 3.3: Isaac Sim Planning and Control Systems",permalink:"/hackathon-ai-book/modules/lab-exercises/lab-3-3-planning-control"}},l={},p=[{value:"Overview",id:"overview",level:2},{value:"Objectives",id:"objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Duration",id:"duration",level:2},{value:"Exercise 1: Camera Sensor Setup and Configuration",id:"exercise-1-camera-sensor-setup-and-configuration",level:2},{value:"Step 1: Create a basic camera setup",id:"step-1-create-a-basic-camera-setup",level:3},{value:"Step 2: Advanced camera configuration",id:"step-2-advanced-camera-configuration",level:3},{value:"Exercise 2: LiDAR Sensor Setup and Processing",id:"exercise-2-lidar-sensor-setup-and-processing",level:2},{value:"Step 1: Create LiDAR sensor configuration",id:"step-1-create-lidar-sensor-configuration",level:3},{value:"Step 2: Multi-sensor fusion system",id:"step-2-multi-sensor-fusion-system",level:3},{value:"Exercise 3: ROS Integration for Perception",id:"exercise-3-ros-integration-for-perception",level:2},{value:"Step 1: Create ROS bridge configuration for sensors",id:"step-1-create-ros-bridge-configuration-for-sensors",level:3},{value:"Exercise 4: Computer Vision Algorithms Integration",id:"exercise-4-computer-vision-algorithms-integration",level:2},{value:"Step 1: Create a perception pipeline with computer vision",id:"step-1-create-a-perception-pipeline-with-computer-vision",level:3},{value:"Exercise 5: Perception Data Validation and Quality Assurance",id:"exercise-5-perception-data-validation-and-quality-assurance",level:2},{value:"Step 1: Create perception quality metrics",id:"step-1-create-perception-quality-metrics",level:3},{value:"Exercise 6: Troubleshooting and Optimization",id:"exercise-6-troubleshooting-and-optimization",level:2},{value:"Step 1: Create perception optimization tools",id:"step-1-create-perception-optimization-tools",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues and Solutions",id:"common-issues-and-solutions",level:3},{value:"Assessment Questions",id:"assessment-questions",level:2},{value:"Extension Exercises",id:"extension-exercises",level:2},{value:"Summary",id:"summary",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h1,{id:"lab-32-isaac-sim-perception-systems",children:"Lab 3.2: Isaac Sim Perception Systems"}),"\n",(0,r.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,r.jsx)(n.p,{children:"In this lab, you will learn how to implement perception systems in Isaac Sim for robotics applications. You'll work with various sensors including cameras, LiDAR, IMU, and other perception sensors, and learn how to process and integrate sensor data for robotics applications. This includes understanding sensor models, configuring sensor parameters, and integrating with ROS."}),"\n",(0,r.jsx)(n.h2,{id:"objectives",children:"Objectives"}),"\n",(0,r.jsx)(n.p,{children:"By the end of this lab, you will be able to:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Configure and use different types of sensors in Isaac Sim"}),"\n",(0,r.jsx)(n.li,{children:"Process camera images and point cloud data"}),"\n",(0,r.jsx)(n.li,{children:"Integrate perception data with ROS topics"}),"\n",(0,r.jsx)(n.li,{children:"Implement basic computer vision algorithms"}),"\n",(0,r.jsx)(n.li,{children:"Create perception pipelines for robotics"}),"\n",(0,r.jsx)(n.li,{children:"Validate sensor data quality and accuracy"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Completion of Lab 3.1: Isaac Sim Setup and Environment"}),"\n",(0,r.jsx)(n.li,{children:"Understanding of ROS 2 topics and messages"}),"\n",(0,r.jsx)(n.li,{children:"Basic knowledge of computer vision concepts"}),"\n",(0,r.jsx)(n.li,{children:"Experience with Isaac Sim basics"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"duration",children:"Duration"}),"\n",(0,r.jsx)(n.p,{children:"4-5 hours"}),"\n",(0,r.jsx)(n.h2,{id:"exercise-1-camera-sensor-setup-and-configuration",children:"Exercise 1: Camera Sensor Setup and Configuration"}),"\n",(0,r.jsx)(n.h3,{id:"step-1-create-a-basic-camera-setup",children:"Step 1: Create a basic camera setup"}),"\n",(0,r.jsxs)(n.p,{children:["Create ",(0,r.jsx)(n.code,{children:"~/isaac_sim_examples/camera_sensor.py"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n# camera_sensor.py\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.utils.prims import create_primitive\nfrom omni.isaac.core.utils.viewports import set_camera_view\nfrom omni.isaac.sensor import Camera\nfrom omni.isaac.core.utils.semantics import add_semantic_data_to_stage\nimport numpy as np\nimport cv2\nimport carb\n\n# Initialize the world\nworld = World(stage_units_in_meters=1.0)\n\n# Create a simple environment\nground_plane = create_primitive(\n    prim_path="/World/GroundPlane",\n    primitive_type="Plane",\n    scale=[10, 10, 1],\n    position=[0, 0, 0]\n)\n\n# Create objects with different materials\nred_cube = create_primitive(\n    prim_path="/World/RedCube",\n    primitive_type="Cube",\n    scale=[0.5, 0.5, 0.5],\n    position=[1, 0, 0.25],\n    color=[1, 0, 0]\n)\n\nblue_cube = create_primitive(\n    prim_path="/World/BlueCube",\n    primitive_type="Cube",\n    scale=[0.5, 0.5, 0.5],\n    position=[0, 1, 0.25],\n    color=[0, 0, 1]\n)\n\ngreen_cube = create_primitive(\n    prim_path="/World/GreenCube",\n    primitive_type="Cube",\n    scale=[0.5, 0.5, 0.5],\n    position=[1, 1, 0.25],\n    color=[0, 1, 0]\n)\n\n# Create a robot with camera\nrobot = create_primitive(\n    prim_path="/World/Robot",\n    primitive_type="Cylinder",\n    scale=[0.3, 0.3, 0.5],\n    position=[0, 0, 0.25]\n)\n\n# Create camera sensor\ncamera = Camera(\n    prim_path="/World/Robot/Camera",\n    frequency=30,\n    resolution=(640, 480)\n)\n\n# Set camera position relative to robot\ncamera.set_local_pose(translation=np.array([0.3, 0, 0.2]))\n\n# Set camera view for visualization\nset_camera_view(eye=[3, 3, 3], target=[0, 0, 0])\n\n# Play the simulation\nworld.reset()\n\n# Initialize image data collection\nimage_count = 0\n\nfor i in range(500):\n    world.step(render=True)\n\n    # Capture and process images every 30 steps (1Hz)\n    if i % 30 == 0:\n        # Get RGB image\n        rgb_image = camera.get_rgb()\n\n        if rgb_image is not None:\n            print(f"Captured image {image_count}: shape={rgb_image.shape}, dtype={rgb_image.dtype}")\n\n            # Process the image (example: convert to OpenCV format)\n            # Note: Isaac Sim uses different coordinate systems\n            processed_image = np.flip(rgb_image, axis=0)  # Flip vertically\n\n            # Save image for inspection\n            import os\n            os.makedirs("~/isaac_sim_outputs", exist_ok=True)\n            output_path = f"~/isaac_sim_outputs/camera_image_{image_count:03d}.png"\n            # Note: In practice, you\'d use cv2.imwrite or similar\n            print(f"Image would be saved to: {output_path}")\n\n            image_count += 1\n\n# Stop the simulation\nworld.stop()\nprint("Camera sensor simulation completed")\n'})}),"\n",(0,r.jsx)(n.h3,{id:"step-2-advanced-camera-configuration",children:"Step 2: Advanced camera configuration"}),"\n",(0,r.jsxs)(n.p,{children:["Create ",(0,r.jsx)(n.code,{children:"~/isaac_sim_examples/advanced_camera.py"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n# advanced_camera.py\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.prims import create_primitive\nfrom omni.isaac.core.utils.viewports import set_camera_view\nfrom omni.isaac.sensor import Camera\nfrom omni.isaac.range_sensor import create_lidar\nimport numpy as np\nimport carb\n\nclass AdvancedCameraSystem:\n    def __init__(self):\n        self.world = World(stage_units_in_meters=1.0)\n        self.cameras = []\n        self.setup_environment()\n\n    def setup_environment(self):\n        """Set up the environment with multiple objects."""\n        # Create ground plane\n        create_primitive(\n            prim_path="/World/GroundPlane",\n            primitive_type="Plane",\n            scale=[10, 10, 1],\n            position=[0, 0, 0]\n        )\n\n        # Create various objects for perception testing\n        for i in range(10):\n            position = [np.random.uniform(-3, 3), np.random.uniform(-3, 3), 0.5]\n            create_primitive(\n                prim_path=f"/World/Object_{i}",\n                primitive_type="Cylinder",\n                scale=[0.2, 0.2, 1.0],\n                position=position,\n                color=[np.random.rand(), np.random.rand(), np.random.rand()]\n            )\n\n        # Create robot\n        self.robot = create_primitive(\n            prim_path="/World/Robot",\n            primitive_type="Cylinder",\n            scale=[0.4, 0.4, 0.8],\n            position=[0, 0, 0.4]\n        )\n\n    def add_camera_system(self):\n        """Add multiple cameras with different configurations."""\n        # Front-facing RGB camera\n        front_camera = Camera(\n            prim_path="/World/Robot/FrontCamera",\n            frequency=30,\n            resolution=(1280, 720),\n            position=[0.3, 0, 0.2]\n        )\n        front_camera.set_local_pose(translation=np.array([0.3, 0, 0.2]))\n        self.cameras.append(front_camera)\n\n        # Depth camera\n        depth_camera = Camera(\n            prim_path="/World/Robot/DepthCamera",\n            frequency=30,\n            resolution=(640, 480),\n            position=[0.3, 0, 0.2]\n        )\n        depth_camera.set_local_pose(translation=np.array([0.3, 0, 0.2]))\n\n        # Enable depth data\n        depth_camera.add_depth_to_frame()\n        depth_camera.add_instance_segmentation_to_frame()\n        self.cameras.append(depth_camera)\n\n        # Segmentation camera\n        seg_camera = Camera(\n            prim_path="/World/Robot/SegCamera",\n            frequency=15,\n            resolution=(640, 480),\n            position=[0.3, 0, 0.2]\n        )\n        seg_camera.set_local_pose(translation=np.array([0.3, 0, 0.2]))\n        seg_camera.add_instance_segmentation_to_frame()\n        self.cameras.append(seg_camera)\n\n    def process_camera_data(self):\n        """Process data from all cameras."""\n        for i, camera in enumerate(self.cameras):\n            # Get different types of data based on camera configuration\n            if i == 0:  # RGB camera\n                rgb_image = camera.get_rgb()\n                if rgb_image is not None:\n                    print(f"RGB Camera {i}: Captured image with shape {rgb_image.shape}")\n\n            elif i == 1:  # Depth + RGB camera\n                rgb_image = camera.get_rgb()\n                depth_image = camera.get_depth()\n\n                if rgb_image is not None:\n                    print(f"Depth Camera {i}: RGB shape {rgb_image.shape}")\n                if depth_image is not None:\n                    print(f"Depth Camera {i}: Depth shape {depth_image.shape}")\n\n            elif i == 2:  # Segmentation camera\n                seg_image = camera.get_semantic_segmentation()\n                if seg_image is not None:\n                    print(f"Segmentation Camera {i}: Segmentation shape {seg_image.shape}")\n\n    def run_simulation(self, steps=1000):\n        """Run the simulation with camera data processing."""\n        self.world.reset()\n        self.add_camera_system()\n\n        # Set camera view\n        set_camera_view(eye=[5, 5, 5], target=[0, 0, 0])\n\n        for i in range(steps):\n            self.world.step(render=True)\n\n            # Process camera data every 60 steps (0.5Hz)\n            if i % 60 == 0:\n                self.process_camera_data()\n\n        print(f"Advanced camera system simulation completed with {steps} steps")\n\n# Create and run the advanced camera system\nif __name__ == "__main__":\n    camera_system = AdvancedCameraSystem()\n    camera_system.run_simulation(steps=600)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"exercise-2-lidar-sensor-setup-and-processing",children:"Exercise 2: LiDAR Sensor Setup and Processing"}),"\n",(0,r.jsx)(n.h3,{id:"step-1-create-lidar-sensor-configuration",children:"Step 1: Create LiDAR sensor configuration"}),"\n",(0,r.jsxs)(n.p,{children:["Create ",(0,r.jsx)(n.code,{children:"~/isaac_sim_examples/lidar_sensor.py"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n# lidar_sensor.py\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.prims import create_primitive\nfrom omni.isaac.core.utils.viewports import set_camera_view\nfrom omni.isaac.range_sensor import LidarRtx\nimport numpy as np\nimport carb\n\nclass LidarSensorSystem:\n    def __init__(self):\n        self.world = World(stage_units_in_meters=1.0)\n        self.lidar = None\n        self.setup_environment()\n\n    def setup_environment(self):\n        """Set up environment with obstacles for LiDAR testing."""\n        # Create ground plane\n        create_primitive(\n            prim_path="/World/GroundPlane",\n            primitive_type="Plane",\n            scale=[20, 20, 1],\n            position=[0, 0, 0]\n        )\n\n        # Create walls\n        wall_thickness = 0.1\n        wall_height = 2.0\n\n        # North wall\n        create_primitive(\n            prim_path="/World/Wall_North",\n            primitive_type="Cuboid",\n            scale=[20, wall_thickness, wall_height],\n            position=[0, 10, wall_height/2]\n        )\n\n        # South wall\n        create_primitive(\n            prim_path="/World/Wall_South",\n            primitive_type="Cuboid",\n            scale=[20, wall_thickness, wall_height],\n            position=[0, -10, wall_height/2]\n        )\n\n        # East wall\n        create_primitive(\n            prim_path="/World/Wall_East",\n            primitive_type="Cuboid",\n            scale=[wall_thickness, 20, wall_height],\n            position=[10, 0, wall_height/2]\n        )\n\n        # West wall\n        create_primitive(\n            prim_path="/World/Wall_West",\n            primitive_type="Cuboid",\n            scale=[wall_thickness, 20, wall_height],\n            position=[-10, 0, wall_height/2]\n        )\n\n        # Create obstacles inside the area\n        for i in range(15):\n            position = [np.random.uniform(-8, 8), np.random.uniform(-8, 8), 0.5]\n            if np.linalg.norm(position[:2]) > 1.0:  # Keep center clear\n                create_primitive(\n                    prim_path=f"/World/Obstacle_{i}",\n                    primitive_type="Cylinder",\n                    scale=[0.3, 0.3, 1.0],\n                    position=position\n                )\n\n        # Create robot with LiDAR\n        self.robot = create_primitive(\n            prim_path="/World/Robot",\n            primitive_type="Cylinder",\n            scale=[0.5, 0.5, 1.0],\n            position=[0, 0, 0.5]\n        )\n\n    def add_lidar(self):\n        """Add LiDAR sensor to the robot."""\n        self.lidar = LidarRtx(\n            prim_path="/World/Robot/Lidar",\n            translation=(0.0, 0.0, 0.8),  # Mount on top of robot\n            config="Example_Rotary",\n            range_resolution=0.005,  # 5mm resolution\n            rotation_frequency=10,   # 10 Hz rotation\n            horizontal_resolution=0.25,  # 0.25 degree horizontal resolution\n            vertical_resolution=0.4,     # 0.4 degree vertical resolution\n            horizontal_samples=1080,     # Samples per revolution\n            vertical_samples=64,         # Vertical beams\n            max_range=25.0,              # Maximum range 25m\n            min_range=0.1                # Minimum range 10cm\n        )\n\n    def process_lidar_data(self):\n        """Process LiDAR scan data."""\n        try:\n            # Get LiDAR data\n            lidar_data = self.lidar.get_linear_depth_data()\n\n            if lidar_data is not None:\n                # Calculate statistics\n                valid_points = lidar_data[lidar_data > 0]  # Filter out invalid ranges\n\n                if len(valid_points) > 0:\n                    avg_distance = np.mean(valid_points)\n                    min_distance = np.min(valid_points)\n                    max_distance = np.max(valid_points)\n\n                    print(f"LiDAR Scan: Points={len(valid_points)}, "\n                          f"Avg={avg_distance:.2f}m, Min={min_distance:.2f}m, Max={max_distance:.2f}m")\n\n                    # Calculate hit rate (percentage of valid measurements)\n                    total_points = lidar_data.size\n                    hit_rate = len(valid_points) / total_points if total_points > 0 else 0\n                    print(f"Hit Rate: {hit_rate:.2%}")\n                else:\n                    print("LiDAR Scan: No valid points detected")\n            else:\n                print("LiDAR Scan: No data available")\n\n        except Exception as e:\n            print(f"Error processing LiDAR data: {e}")\n\n    def run_simulation(self, steps=1000):\n        """Run simulation with LiDAR data processing."""\n        self.world.reset()\n        self.add_lidar()\n\n        # Set camera view\n        set_camera_view(eye=[15, 15, 15], target=[0, 0, 0])\n\n        for i in range(steps):\n            self.world.step(render=True)\n\n            # Process LiDAR data every 30 steps (1Hz)\n            if i % 30 == 0:\n                self.process_lidar_data()\n\n        print(f"LiDAR sensor simulation completed with {steps} steps")\n\n# Create and run the LiDAR system\nif __name__ == "__main__":\n    lidar_system = LidarSensorSystem()\n    lidar_system.run_simulation(steps=600)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"step-2-multi-sensor-fusion-system",children:"Step 2: Multi-sensor fusion system"}),"\n",(0,r.jsxs)(n.p,{children:["Create ",(0,r.jsx)(n.code,{children:"~/isaac_sim_examples/multi_sensor_fusion.py"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n# multi_sensor_fusion.py\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.prims import create_primitive\nfrom omni.isaac.core.utils.viewports import set_camera_view\nfrom omni.isaac.sensor import Camera, ImuSensor\nfrom omni.isaac.range_sensor import LidarRtx\nimport numpy as np\nimport carb\n\nclass MultiSensorFusion:\n    def __init__(self):\n        self.world = World(stage_units_in_meters=1.0)\n        self.sensors = {}\n        self.setup_environment()\n\n    def setup_environment(self):\n        """Set up environment with multiple sensor targets."""\n        # Create ground plane\n        create_primitive(\n            prim_path="/World/GroundPlane",\n            primitive_type="Plane",\n            scale=[20, 20, 1],\n            position=[0, 0, 0]\n        )\n\n        # Create landmarks for localization\n        landmark_positions = [\n            [5, 5, 0.5], [-5, 5, 0.5], [-5, -5, 0.5], [5, -5, 0.5],  # Corners\n            [0, 0, 0.5], [0, 5, 0.5], [5, 0, 0.5], [-5, 0, 0.5], [0, -5, 0.5]  # Center and sides\n        ]\n\n        for i, pos in enumerate(landmark_positions):\n            create_primitive(\n                prim_path=f"/World/Landmark_{i}",\n                primitive_type="Cylinder",\n                scale=[0.2, 0.2, 1.0],\n                position=pos,\n                color=[1, 0, 0]  # Red landmarks\n            )\n\n        # Create robot\n        self.robot = create_primitive(\n            prim_path="/World/Robot",\n            primitive_type="Cylinder",\n            scale=[0.5, 0.5, 1.0],\n            position=[0, 0, 0.5]\n        )\n\n    def add_sensors(self):\n        """Add multiple types of sensors to the robot."""\n        # Camera sensor\n        camera = Camera(\n            prim_path="/World/Robot/Camera",\n            frequency=30,\n            resolution=(640, 480),\n            position=[0.3, 0, 0.5]\n        )\n        camera.set_local_pose(translation=np.array([0.3, 0, 0.5]))\n        camera.add_depth_to_frame()\n        self.sensors[\'camera\'] = camera\n\n        # LiDAR sensor\n        lidar = LidarRtx(\n            prim_path="/World/Robot/Lidar",\n            translation=(0.0, 0.0, 0.8),\n            config="Example_Rotary",\n            range_resolution=0.005,\n            rotation_frequency=10,\n            horizontal_resolution=0.25,\n            vertical_resolution=0.4,\n            horizontal_samples=1080,\n            vertical_samples=64,\n            max_range=25.0,\n            min_range=0.1\n        )\n        self.sensors[\'lidar\'] = lidar\n\n        # IMU sensor\n        imu = ImuSensor(\n            prim_path="/World/Robot/Imu",\n            name="robot_imu",\n            translation=np.array([0.0, 0.0, 0.5]),\n            orientation=np.array([0.0, 0.0, 0.0, 1.0])\n        )\n        self.sensors[\'imu\'] = imu\n\n    def process_sensor_fusion(self):\n        """Process and fuse data from multiple sensors."""\n        # Get camera data\n        if \'camera\' in self.sensors:\n            rgb_image = self.sensors[\'camera\'].get_rgb()\n            depth_image = self.sensors[\'camera\'].get_depth()\n\n            if rgb_image is not None:\n                print(f"Camera: RGB image shape {rgb_image.shape}")\n            if depth_image is not None:\n                print(f"Camera: Depth image shape {depth_image.shape}")\n\n        # Get LiDAR data\n        if \'lidar\' in self.sensors:\n            lidar_data = self.sensors[\'lidar\'].get_linear_depth_data()\n            if lidar_data is not None:\n                valid_points = lidar_data[lidar_data > 0]\n                if len(valid_points) > 0:\n                    print(f"LiDAR: {len(valid_points)} valid points, avg range {np.mean(valid_points):.2f}m")\n\n        # Get IMU data\n        if \'imu\' in self.sensors:\n            # IMU data processing would typically happen here\n            print("IMU: Data available (processing simulated)")\n\n        # Sensor fusion logic would go here\n        # For example: combine camera landmarks with LiDAR points for better localization\n        print("Multi-sensor fusion: Processing complete")\n\n    def run_simulation(self, steps=1000):\n        """Run simulation with multi-sensor fusion."""\n        self.world.reset()\n        self.add_sensors()\n\n        # Set camera view\n        set_camera_view(eye=[10, 10, 10], target=[0, 0, 0])\n\n        for i in range(steps):\n            self.world.step(render=True)\n\n            # Process sensor fusion every 60 steps (0.5Hz)\n            if i % 60 == 0:\n                self.process_sensor_fusion()\n\n        print(f"Multi-sensor fusion simulation completed with {steps} steps")\n\n# Create and run the multi-sensor system\nif __name__ == "__main__":\n    fusion_system = MultiSensorFusion()\n    fusion_system.run_simulation(steps=600)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"exercise-3-ros-integration-for-perception",children:"Exercise 3: ROS Integration for Perception"}),"\n",(0,r.jsx)(n.h3,{id:"step-1-create-ros-bridge-configuration-for-sensors",children:"Step 1: Create ROS bridge configuration for sensors"}),"\n",(0,r.jsxs)(n.p,{children:["Create ",(0,r.jsx)(n.code,{children:"~/isaac_sim_examples/ros_perception_bridge.py"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n# ros_perception_bridge.py\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.prims import create_primitive\nfrom omni.isaac.core.utils.viewports import set_camera_view\nfrom omni.isaac.sensor import Camera\nfrom omni.isaac.range_sensor import LidarRtx\nimport omni.isaac.ros2_bridge._ros2_bridge as ros2_bridge\nimport numpy as np\nimport carb\n\nclass ROSPerceptionBridge:\n    def __init__(self):\n        self.world = World(stage_units_in_meters=1.0)\n        self.ros2_bridge = ros2_bridge.acquire_ros2_bridge_interface()\n        self.sensors = {}\n        self.setup_environment()\n\n    def setup_environment(self):\n        """Set up environment for ROS perception testing."""\n        # Create ground plane\n        create_primitive(\n            prim_path="/World/GroundPlane",\n            primitive_type="Plane",\n            scale=[20, 20, 1],\n            position=[0, 0, 0]\n        )\n\n        # Create test objects\n        for i in range(5):\n            position = [np.random.uniform(-8, 8), np.random.uniform(-8, 8), 0.5]\n            create_primitive(\n                prim_path=f"/World/TestObject_{i}",\n                primitive_type="Cylinder",\n                scale=[0.3, 0.3, 1.0],\n                position=position\n            )\n\n        # Create robot\n        self.robot = create_primitive(\n            prim_path="/World/Robot",\n            primitive_type="Cylinder",\n            scale=[0.5, 0.5, 1.0],\n            position=[0, 0, 0.5]\n        )\n\n    def add_ros_sensors(self):\n        """Add sensors with ROS bridge integration."""\n        # Camera with ROS bridge\n        camera = Camera(\n            prim_path="/World/Robot/Camera",\n            frequency=30,\n            resolution=(640, 480),\n            position=[0.3, 0, 0.5]\n        )\n        camera.set_local_pose(translation=np.array([0.3, 0, 0.5]))\n        camera.add_depth_to_frame()\n        self.sensors[\'camera\'] = camera\n\n        # LiDAR with ROS bridge\n        lidar = LidarRtx(\n            prim_path="/World/Robot/Lidar",\n            translation=(0.0, 0.0, 0.8),\n            config="Example_Rotary",\n            range_resolution=0.005,\n            rotation_frequency=10,\n            horizontal_resolution=0.25,\n            vertical_resolution=0.4,\n            horizontal_samples=1080,\n            vertical_samples=64,\n            max_range=25.0,\n            min_range=0.1\n        )\n        self.sensors[\'lidar\'] = lidar\n\n        # Enable ROS bridge for sensors\n        self.enable_ros_bridge()\n\n    def enable_ros_bridge(self):\n        """Enable ROS bridge for all sensors."""\n        # Publish camera data to ROS\n        if \'camera\' in self.sensors:\n            self.ros2_bridge.publish_camera(\n                self.sensors[\'camera\'],\n                topic_name="/humanoid/camera/image_raw",\n                sensor_name="camera"\n            )\n\n            # Also publish depth\n            self.ros2_bridge.publish_depth(\n                self.sensors[\'camera\'],\n                topic_name="/humanoid/camera/depth",\n                sensor_name="depth_camera"\n            )\n\n        # Publish LiDAR data to ROS\n        if \'lidar\' in self.sensors:\n            self.ros2_bridge.publish_lidar(\n                self.sensors[\'lidar\'],\n                topic_name="/humanoid/scan",\n                sensor_name="lidar"\n            )\n\n    def run_simulation(self, steps=1000):\n        """Run simulation with ROS perception bridge."""\n        self.world.reset()\n        self.add_ros_sensors()\n\n        # Set camera view\n        set_camera_view(eye=[10, 10, 10], target=[0, 0, 0])\n\n        for i in range(steps):\n            self.world.step(render=True)\n\n            # Periodic status update\n            if i % 100 == 0:\n                print(f"ROS Perception Bridge: Simulation step {i}/{steps}")\n\n        print(f"ROS perception bridge simulation completed with {steps} steps")\n\n# Create and run the ROS perception bridge\nif __name__ == "__main__":\n    ros_bridge = ROSPerceptionBridge()\n    ros_bridge.run_simulation(steps=600)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"exercise-4-computer-vision-algorithms-integration",children:"Exercise 4: Computer Vision Algorithms Integration"}),"\n",(0,r.jsx)(n.h3,{id:"step-1-create-a-perception-pipeline-with-computer-vision",children:"Step 1: Create a perception pipeline with computer vision"}),"\n",(0,r.jsxs)(n.p,{children:["Create ",(0,r.jsx)(n.code,{children:"~/isaac_sim_examples/computer_vision_pipeline.py"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n# computer_vision_pipeline.py\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.prims import create_primitive\nfrom omni.isaac.core.utils.viewports import set_camera_view\nfrom omni.isaac.sensor import Camera\nimport numpy as np\nimport cv2\nimport carb\n\nclass ComputerVisionPipeline:\n    def __init__(self):\n        self.world = World(stage_units_in_meters=1.0)\n        self.camera = None\n        self.setup_environment()\n\n    def setup_environment(self):\n        """Set up environment with objects for computer vision."""\n        # Create ground plane\n        create_primitive(\n            prim_path="/World/GroundPlane",\n            primitive_type="Plane",\n            scale=[15, 15, 1],\n            position=[0, 0, 0]\n        )\n\n        # Create objects with different shapes and colors\n        # Red circles\n        for i in range(3):\n            create_primitive(\n                prim_path=f"/World/RedCircle_{i}",\n                primitive_type="Cylinder",\n                scale=[0.3, 0.3, 0.5],\n                position=[np.random.uniform(-5, 5), np.random.uniform(-5, 5), 0.25],\n                color=[1, 0, 0]\n            )\n\n        # Blue squares\n        for i in range(3):\n            create_primitive(\n                prim_path=f"/World/BlueSquare_{i}",\n                primitive_type="Cube",\n                scale=[0.3, 0.3, 0.5],\n                position=[np.random.uniform(-5, 5), np.random.uniform(-5, 5), 0.25],\n                color=[0, 0, 1]\n            )\n\n        # Green triangles (approximated with cones)\n        for i in range(3):\n            create_primitive(\n                prim_path=f"/World/GreenTriangle_{i}",\n                primitive_type="Cone",\n                scale=[0.3, 0.3, 0.5],\n                position=[np.random.uniform(-5, 5), np.random.uniform(-5, 5), 0.25],\n                color=[0, 1, 0]\n            )\n\n        # Create robot with camera\n        self.robot = create_primitive(\n            prim_path="/World/Robot",\n            primitive_type="Cylinder",\n            scale=[0.5, 0.5, 1.0],\n            position=[0, 0, 0.5]\n        )\n\n    def add_camera(self):\n        """Add camera to robot."""\n        self.camera = Camera(\n            prim_path="/World/Robot/Camera",\n            frequency=30,\n            resolution=(640, 480),\n            position=[0.3, 0, 0.5]\n        )\n        self.camera.set_local_pose(translation=np.array([0.3, 0, 0.5]))\n\n    def object_detection_pipeline(self, image):\n        """Process image for object detection."""\n        # Convert Isaac Sim image format to OpenCV format\n        # Isaac Sim images are typically in RGB format\n        rgb_image = np.flip(image, axis=0)  # Flip vertically\n        bgr_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)\n\n        # Convert to HSV for color-based segmentation\n        hsv_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2HSV)\n\n        # Define color ranges for different objects\n        color_ranges = {\n            \'red\': ([0, 50, 50], [10, 255, 255]),\n            \'red2\': ([170, 50, 50], [180, 255, 255]),  # Red wraps around in HSV\n            \'blue\': ([100, 50, 50], [130, 255, 255]),\n            \'green\': ([40, 50, 50], [80, 255, 255])\n        }\n\n        detected_objects = []\n\n        for color_name, (lower, upper) in color_ranges.items():\n            # Create mask for the color\n            lower = np.array(lower)\n            upper = np.array(upper)\n            mask = cv2.inRange(hsv_image, lower, upper)\n\n            # If it\'s red, combine both red ranges\n            if color_name == \'red\':\n                mask2 = cv2.inRange(hsv_image, np.array([170, 50, 50]), np.array([180, 255, 255]))\n                mask = cv2.bitwise_or(mask, mask2)\n\n            # Find contours\n            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n            for contour in contours:\n                # Filter by area to remove noise\n                area = cv2.contourArea(contour)\n                if area > 100:  # Minimum area threshold\n                    # Get bounding box\n                    x, y, w, h = cv2.boundingRect(contour)\n\n                    # Calculate center\n                    center_x = x + w // 2\n                    center_y = y + h // 2\n\n                    # Determine object type based on aspect ratio\n                    aspect_ratio = float(w) / h\n                    if 0.8 <= aspect_ratio <= 1.2:\n                        obj_type = "circle/square"\n                    elif aspect_ratio > 1.2:\n                        obj_type = "horizontal"\n                    else:\n                        obj_type = "vertical"\n\n                    detected_objects.append({\n                        \'type\': obj_type,\n                        \'color\': color_name,\n                        \'center\': (center_x, center_y),\n                        \'bbox\': (x, y, w, h),\n                        \'area\': area\n                    })\n\n        return detected_objects\n\n    def process_perception_data(self):\n        """Process perception data from camera."""\n        if self.camera:\n            rgb_image = self.camera.get_rgb()\n\n            if rgb_image is not None:\n                # Run object detection\n                detected_objects = self.object_detection_pipeline(rgb_image)\n\n                print(f"Detected {len(detected_objects)} objects:")\n                for i, obj in enumerate(detected_objects):\n                    print(f"  {i+1}. {obj[\'color\']} {obj[\'type\']} at {obj[\'center\']}, area={obj[\'area\']:.0f}")\n\n                return detected_objects\n            else:\n                print("No camera image available")\n                return []\n        else:\n            print("No camera available")\n            return []\n\n    def run_simulation(self, steps=1000):\n        """Run simulation with computer vision processing."""\n        self.world.reset()\n        self.add_camera()\n\n        # Set camera view\n        set_camera_view(eye=[10, 10, 10], target=[0, 0, 0])\n\n        for i in range(steps):\n            self.world.step(render=True)\n\n            # Process perception data every 30 steps (1Hz)\n            if i % 30 == 0:\n                detected_objects = self.process_perception_data()\n\n        print(f"Computer vision pipeline simulation completed with {steps} steps")\n\n# Create and run the computer vision pipeline\nif __name__ == "__main__":\n    cv_pipeline = ComputerVisionPipeline()\n    cv_pipeline.run_simulation(steps=600)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"exercise-5-perception-data-validation-and-quality-assurance",children:"Exercise 5: Perception Data Validation and Quality Assurance"}),"\n",(0,r.jsx)(n.h3,{id:"step-1-create-perception-quality-metrics",children:"Step 1: Create perception quality metrics"}),"\n",(0,r.jsxs)(n.p,{children:["Create ",(0,r.jsx)(n.code,{children:"~/isaac_sim_examples/perception_quality.py"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n# perception_quality.py\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.prims import create_primitive\nfrom omni.isaac.core.utils.viewports import set_camera_view\nfrom omni.isaac.sensor import Camera\nfrom omni.isaac.range_sensor import LidarRtx\nimport numpy as np\nimport carb\n\nclass PerceptionQualityAssessment:\n    def __init__(self):\n        self.world = World(stage_units_in_meters=1.0)\n        self.sensors = {}\n        self.quality_metrics = {}\n        self.setup_environment()\n\n    def setup_environment(self):\n        \"\"\"Set up controlled environment for quality assessment.\"\"\"\n        # Create ground plane\n        create_primitive(\n            prim_path=\"/World/GroundPlane\",\n            primitive_type=\"Plane\",\n            scale=[20, 20, 1],\n            position=[0, 0, 0]\n        )\n\n        # Create standardized test objects at known positions\n        self.test_objects = [\n            {'name': 'Target_1', 'position': [2, 0, 0.5], 'size': [0.5, 0.5, 1.0], 'type': 'cube'},\n            {'name': 'Target_2', 'position': [0, 2, 0.5], 'size': [0.3, 0.3, 0.8], 'type': 'cylinder'},\n            {'name': 'Target_3', 'position': [-2, 0, 0.5], 'size': [0.4, 0.4, 0.6], 'type': 'sphere'},\n            {'name': 'Target_4', 'position': [0, -2, 0.5], 'size': [0.6, 0.2, 0.4], 'type': 'box'},\n        ]\n\n        for obj in self.test_objects:\n            create_primitive(\n                prim_path=f\"/World/{obj['name']}\",\n                primitive_type=obj['type'].capitalize() if obj['type'] != 'box' else 'Cuboid',\n                scale=obj['size'],\n                position=obj['position']\n            )\n\n        # Create robot with sensors\n        self.robot = create_primitive(\n            prim_path=\"/World/Robot\",\n            primitive_type=\"Cylinder\",\n            scale=[0.5, 0.5, 1.0],\n            position=[0, 0, 0.5]\n        )\n\n    def add_quality_sensors(self):\n        \"\"\"Add sensors configured for quality assessment.\"\"\"\n        # High-resolution camera for detailed perception\n        camera = Camera(\n            prim_path=\"/World/Robot/QualityCamera\",\n            frequency=60,\n            resolution=(1280, 720),  # High resolution\n            position=[0.3, 0, 0.5]\n        )\n        camera.set_local_pose(translation=np.array([0.3, 0, 0.5]))\n        camera.add_depth_to_frame()\n        camera.add_instance_segmentation_to_frame()\n        self.sensors['quality_camera'] = camera\n\n        # Precise LiDAR for ground truth comparison\n        lidar = LidarRtx(\n            prim_path=\"/World/Robot/PrecisionLidar\",\n            translation=(0.0, 0.0, 0.8),\n            config=\"Example_Rotary\",\n            range_resolution=0.001,  # 1mm resolution\n            rotation_frequency=20,\n            horizontal_resolution=0.1,  # 0.1 degree\n            vertical_resolution=0.2,   # 0.2 degree\n            horizontal_samples=2160,   # Higher resolution\n            vertical_samples=128,\n            max_range=20.0,\n            min_range=0.05\n        )\n        self.sensors['precision_lidar'] = lidar\n\n    def assess_camera_quality(self):\n        \"\"\"Assess camera perception quality.\"\"\"\n        camera = self.sensors['quality_camera']\n\n        # Get various data types\n        rgb_image = camera.get_rgb()\n        depth_image = camera.get_depth()\n        segmentation = camera.get_semantic_segmentation()\n\n        metrics = {}\n\n        if rgb_image is not None:\n            # Image quality metrics\n            height, width, channels = rgb_image.shape\n            metrics['image_resolution'] = f\"{width}x{height}\"\n            metrics['image_channels'] = channels\n\n            # Brightness assessment\n            brightness = np.mean(rgb_image)\n            metrics['brightness'] = brightness\n\n            # Contrast assessment (simplified)\n            contrast = np.std(rgb_image)\n            metrics['contrast'] = contrast\n\n        if depth_image is not None:\n            # Depth quality metrics\n            valid_depths = depth_image[depth_image > 0]\n            if len(valid_depths) > 0:\n                metrics['depth_coverage'] = len(valid_depths) / depth_image.size\n                metrics['avg_depth'] = np.mean(valid_depths)\n                metrics['depth_range'] = [np.min(valid_depths), np.max(valid_depths)]\n\n        if segmentation is not None:\n            # Segmentation quality\n            unique_labels = np.unique(segmentation)\n            metrics['segmentation_classes'] = len(unique_labels)\n\n        return metrics\n\n    def assess_lidar_quality(self):\n        \"\"\"Assess LiDAR perception quality.\"\"\"\n        lidar = self.sensors['precision_lidar']\n\n        try:\n            lidar_data = lidar.get_linear_depth_data()\n\n            if lidar_data is not None:\n                # LiDAR quality metrics\n                valid_points = lidar_data[lidar_data > 0]\n\n                metrics = {\n                    'total_points': lidar_data.size,\n                    'valid_points': len(valid_points),\n                    'hit_rate': len(valid_points) / lidar_data.size if lidar_data.size > 0 else 0,\n                    'avg_range': np.mean(valid_points) if len(valid_points) > 0 else 0,\n                    'range_std': np.std(valid_points) if len(valid_points) > 0 else 0\n                }\n\n                # Range accuracy assessment\n                if len(valid_points) > 0:\n                    # In a real system, you'd compare with ground truth\n                    # Here we just assess the data quality\n                    metrics['range_accuracy'] = 'high' if metrics['hit_rate'] > 0.8 else 'low'\n\n                return metrics\n            else:\n                return {'error': 'No LiDAR data available'}\n\n        except Exception as e:\n            return {'error': f'LiDAR processing error: {str(e)}'}\n\n    def run_quality_assessment(self, steps=500):\n        \"\"\"Run perception quality assessment.\"\"\"\n        self.world.reset()\n        self.add_quality_sensors()\n\n        # Set camera view\n        set_camera_view(eye=[10, 10, 10], target=[0, 0, 0])\n\n        for i in range(steps):\n            self.world.step(render=True)\n\n            # Assess quality every 100 steps\n            if i % 100 == 0:\n                print(f\"\\n--- Quality Assessment at Step {i} ---\")\n\n                # Camera quality assessment\n                camera_metrics = self.assess_camera_quality()\n                print(\"Camera Quality Metrics:\")\n                for key, value in camera_metrics.items():\n                    print(f\"  {key}: {value}\")\n\n                # LiDAR quality assessment\n                lidar_metrics = self.assess_lidar_quality()\n                print(\"LiDAR Quality Metrics:\")\n                for key, value in lidar_metrics.items():\n                    print(f\"  {key}: {value}\")\n\n        print(f\"Perception quality assessment completed with {steps} steps\")\n\n# Create and run the quality assessment\nif __name__ == \"__main__\":\n    quality_assessment = PerceptionQualityAssessment()\n    quality_assessment.run_quality_assessment(steps=500)\n"})}),"\n",(0,r.jsx)(n.h2,{id:"exercise-6-troubleshooting-and-optimization",children:"Exercise 6: Troubleshooting and Optimization"}),"\n",(0,r.jsx)(n.h3,{id:"step-1-create-perception-optimization-tools",children:"Step 1: Create perception optimization tools"}),"\n",(0,r.jsxs)(n.p,{children:["Create ",(0,r.jsx)(n.code,{children:"~/isaac_sim_examples/perception_optimization.py"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n# perception_optimization.py\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.prims import create_primitive\nfrom omni.isaac.core.utils.viewports import set_camera_view\nfrom omni.isaac.sensor import Camera\nfrom omni.isaac.range_sensor import LidarRtx\nimport numpy as np\nimport time\nimport carb\n\nclass PerceptionOptimization:\n    def __init__(self):\n        self.world = World(stage_units_in_meters=1.0)\n        self.sensors = {}\n        self.performance_stats = {}\n        self.setup_environment()\n\n    def setup_environment(self):\n        """Set up environment for performance testing."""\n        # Create ground plane\n        create_primitive(\n            prim_path="/World/GroundPlane",\n            primitive_type="Plane",\n            scale=[30, 30, 1],\n            position=[0, 0, 0]\n        )\n\n        # Create many objects for performance testing\n        for i in range(50):\n            position = [np.random.uniform(-10, 10), np.random.uniform(-10, 10), 0.5]\n            create_primitive(\n                prim_path=f"/World/PerformanceObj_{i}",\n                primitive_type="Cylinder",\n                scale=[0.2, 0.2, 0.8],\n                position=position\n            )\n\n        # Create robot with multiple sensors\n        self.robot = create_primitive(\n            prim_path="/World/Robot",\n            primitive_type="Cylinder",\n            scale=[0.5, 0.5, 1.0],\n            position=[0, 0, 0.5]\n        )\n\n    def add_optimized_sensors(self):\n        """Add sensors with optimized configurations."""\n        # Optimized camera (balanced performance and quality)\n        optimized_camera = Camera(\n            prim_path="/World/Robot/OptimizedCamera",\n            frequency=30,  # Balanced frequency\n            resolution=(640, 480),  # Balanced resolution\n            position=[0.3, 0, 0.5]\n        )\n        optimized_camera.set_local_pose(translation=np.array([0.3, 0, 0.5]))\n        self.sensors[\'optimized_camera\'] = optimized_camera\n\n        # Optimized LiDAR (balanced performance and quality)\n        optimized_lidar = LidarRtx(\n            prim_path="/World/Robot/OptimizedLidar",\n            translation=(0.0, 0.0, 0.8),\n            config="Example_Rotary",\n            range_resolution=0.005,  # Good resolution\n            rotation_frequency=10,   # Balanced frequency\n            horizontal_resolution=0.25,  # Good resolution\n            vertical_resolution=0.4,\n            horizontal_samples=1080,  # Balanced samples\n            vertical_samples=64,\n            max_range=25.0,\n            min_range=0.1\n        )\n        self.sensors[\'optimized_lidar\'] = optimized_lidar\n\n    def measure_performance(self):\n        """Measure performance of perception system."""\n        start_time = time.time()\n\n        # Process camera data\n        if \'optimized_camera\' in self.sensors:\n            start_camera = time.time()\n            rgb_image = self.sensors[\'optimized_camera\'].get_rgb()\n            camera_time = time.time() - start_camera\n\n            if rgb_image is not None:\n                camera_fps = 1.0 / camera_time if camera_time > 0 else float(\'inf\')\n            else:\n                camera_fps = 0\n\n        # Process LiDAR data\n        if \'optimized_lidar\' in self.sensors:\n            start_lidar = time.time()\n            lidar_data = self.sensors[\'optimized_lidar\'].get_linear_depth_data()\n            lidar_time = time.time() - start_lidar\n\n            if lidar_data is not None:\n                lidar_fps = 1.0 / lidar_time if lidar_time > 0 else float(\'inf\')\n            else:\n                lidar_fps = 0\n\n        total_time = time.time() - start_time\n\n        return {\n            \'camera_processing_time\': camera_time,\n            \'lidar_processing_time\': lidar_time,\n            \'total_processing_time\': total_time,\n            \'camera_fps\': camera_fps,\n            \'lidar_fps\': lidar_fps\n        }\n\n    def run_optimization_test(self, steps=300):\n        """Run optimization test."""\n        self.world.reset()\n        self.add_optimized_sensors()\n\n        # Set camera view\n        set_camera_view(eye=[15, 15, 15], target=[0, 0, 0])\n\n        total_camera_time = 0\n        total_lidar_time = 0\n        total_steps = 0\n\n        for i in range(steps):\n            step_start = time.time()\n            self.world.step(render=True)\n\n            # Measure performance every 50 steps\n            if i % 50 == 0 and i > 0:\n                perf_metrics = self.measure_performance()\n\n                total_camera_time += perf_metrics[\'camera_processing_time\']\n                total_lidar_time += perf_metrics[\'lidar_processing_time\']\n                total_steps += 1\n\n                print(f"\\n--- Performance Metrics (Step {i}) ---")\n                print(f"Camera processing: {perf_metrics[\'camera_processing_time\']:.4f}s ({perf_metrics[\'camera_fps\']:.1f} FPS)")\n                print(f"LiDAR processing: {perf_metrics[\'lidar_processing_time\']:.4f}s ({perf_metrics[\'lidar_fps\']:.1f} FPS)")\n                print(f"Total processing: {perf_metrics[\'total_processing_time\']:.4f}s")\n\n        # Calculate averages\n        if total_steps > 0:\n            avg_camera_time = total_camera_time / total_steps\n            avg_lidar_time = total_lidar_time / total_steps\n\n            print(f"\\n--- Average Performance ---")\n            print(f"Average camera processing: {avg_camera_time:.4f}s")\n            print(f"Average LiDAR processing: {avg_lidar_time:.4f}s")\n\n        print(f"Perception optimization test completed with {steps} steps")\n\n# Create and run the optimization test\nif __name__ == "__main__":\n    optimization_test = PerceptionOptimization()\n    optimization_test.run_optimization_test(steps=300)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,r.jsx)(n.h3,{id:"common-issues-and-solutions",children:"Common Issues and Solutions"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Camera images appear distorted or incorrect"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Check camera intrinsic parameters"}),"\n",(0,r.jsx)(n.li,{children:"Verify coordinate system transformations"}),"\n",(0,r.jsx)(n.li,{children:"Ensure proper lens distortion settings"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"LiDAR data has artifacts or incorrect ranges"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Verify LiDAR configuration parameters"}),"\n",(0,r.jsx)(n.li,{children:"Check for occlusions or reflections"}),"\n",(0,r.jsx)(n.li,{children:"Adjust range resolution and sample rates"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Performance issues with multiple sensors"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Reduce sensor frequencies"}),"\n",(0,r.jsx)(n.li,{children:"Lower resolution settings"}),"\n",(0,r.jsx)(n.li,{children:"Optimize rendering quality"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"ROS bridge connection problems"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Verify ROS 2 network configuration"}),"\n",(0,r.jsx)(n.li,{children:"Check topic names and message types"}),"\n",(0,r.jsx)(n.li,{children:"Ensure Isaac Sim and ROS are on same network"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Segmentation data not available"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Enable segmentation in camera configuration"}),"\n",(0,r.jsx)(n.li,{children:"Check semantic annotation on objects"}),"\n",(0,r.jsx)(n.li,{children:"Verify Isaac Sim extensions are loaded"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"assessment-questions",children:"Assessment Questions"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"How do you configure camera intrinsic parameters in Isaac Sim?"}),"\n",(0,r.jsx)(n.li,{children:"What factors affect LiDAR performance in simulation?"}),"\n",(0,r.jsx)(n.li,{children:"How would you implement sensor fusion in Isaac Sim?"}),"\n",(0,r.jsx)(n.li,{children:"What are the key metrics for evaluating perception system quality?"}),"\n",(0,r.jsx)(n.li,{children:"How do you optimize perception performance in Isaac Sim?"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"extension-exercises",children:"Extension Exercises"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Implement a SLAM system using Isaac Sim sensors"}),"\n",(0,r.jsx)(n.li,{children:"Create a 3D object detection pipeline"}),"\n",(0,r.jsx)(n.li,{children:"Implement semantic segmentation with neural networks"}),"\n",(0,r.jsx)(n.li,{children:"Create a multi-camera stereo vision system"}),"\n",(0,r.jsx)(n.li,{children:"Implement sensor calibration procedures"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsx)(n.p,{children:"In this lab, you successfully:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Configured and used various perception sensors in Isaac Sim"}),"\n",(0,r.jsx)(n.li,{children:"Processed camera images, LiDAR point clouds, and other sensor data"}),"\n",(0,r.jsx)(n.li,{children:"Integrated perception systems with ROS"}),"\n",(0,r.jsx)(n.li,{children:"Implemented computer vision algorithms for object detection"}),"\n",(0,r.jsx)(n.li,{children:"Assessed perception quality and optimized performance"}),"\n",(0,r.jsx)(n.li,{children:"Validated sensor data accuracy and reliability"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"These skills are essential for developing robust perception systems in robotics applications. The ability to configure, calibrate, and optimize perception sensors is crucial for building reliable autonomous robots that can accurately perceive and understand their environment."})]})}function c(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(m,{...e})}):m(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>o});var r=i(6540);const a={},t=r.createContext(a);function s(e){const n=r.useContext(t);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),r.createElement(t.Provider,{value:n},e.children)}}}]);