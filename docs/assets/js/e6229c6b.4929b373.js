"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[7238],{7041:(n,e,a)=>{a.r(e),a.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>o,metadata:()=>s,toc:()=>c});var i=a(4848),t=a(8453);const o={},r="Reinforcement Learning",s={id:"modules/ai-robot-brain/reinforcement-learning",title:"Reinforcement Learning",description:"Overview",source:"@site/docs/modules/ai-robot-brain/reinforcement-learning.md",sourceDirName:"modules/ai-robot-brain",slug:"/modules/ai-robot-brain/reinforcement-learning",permalink:"/ai-robotic-book/modules/ai-robot-brain/reinforcement-learning",draft:!1,unlisted:!1,editUrl:"https://github.com/your-org/physical-ai-book/tree/main/docs/modules/ai-robot-brain/reinforcement-learning.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Planning and Control",permalink:"/ai-robotic-book/modules/ai-robot-brain/planning-control"},next:{title:"Lab 3.1: NVIDIA Isaac Navigation System",permalink:"/ai-robotic-book/modules/lab-exercises/lab-3-1-isaac-navigation"}},l={},c=[{value:"Overview",id:"overview",level:2},{value:"Isaac RL Framework",id:"isaac-rl-framework",level:2},{value:"Isaac Lab Setup",id:"isaac-lab-setup",level:3},{value:"RL Environment Configuration",id:"rl-environment-configuration",level:3},{value:"Deep Reinforcement Learning Algorithms",id:"deep-reinforcement-learning-algorithms",level:2},{value:"Proximal Policy Optimization (PPO)",id:"proximal-policy-optimization-ppo",level:3},{value:"Soft Actor-Critic (SAC) Implementation",id:"soft-actor-critic-sac-implementation",level:3},{value:"Isaac-Specific RL Training",id:"isaac-specific-rl-training",level:2},{value:"Parallel Environment Training",id:"parallel-environment-training",level:3},{value:"Domain Randomization",id:"domain-randomization",level:3},{value:"Locomotion Learning",id:"locomotion-learning",level:2},{value:"Walking Gait Learning",id:"walking-gait-learning",level:3},{value:"Balance Recovery Learning",id:"balance-recovery-learning",level:3},{value:"Manipulation Learning",id:"manipulation-learning",level:2},{value:"Object Manipulation with RL",id:"object-manipulation-with-rl",level:3},{value:"Multi-Task Learning",id:"multi-task-learning",level:2},{value:"Hierarchical RL for Complex Tasks",id:"hierarchical-rl-for-complex-tasks",level:3},{value:"Isaac RL Training Pipeline",id:"isaac-rl-training-pipeline",level:2},{value:"Training Script",id:"training-script",level:3},{value:"Configuration File",id:"configuration-file",level:3},{value:"Transfer Learning and Deployment",id:"transfer-learning-and-deployment",level:2},{value:"Sim-to-Real Transfer",id:"sim-to-real-transfer",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Isaac-Specific Optimizations",id:"isaac-specific-optimizations",level:3},{value:"Evaluation and Testing",id:"evaluation-and-testing",level:2},{value:"RL Performance Metrics",id:"rl-performance-metrics",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common RL Issues and Solutions",id:"common-rl-issues-and-solutions",level:3}];function d(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.h1,{id:"reinforcement-learning",children:"Reinforcement Learning"}),"\n",(0,i.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,i.jsx)(e.p,{children:"This section covers the implementation of reinforcement learning (RL) algorithms for humanoid robot behavior using NVIDIA Isaac. Reinforcement learning enables robots to learn complex behaviors through interaction with the environment, making it essential for autonomous humanoid capabilities."}),"\n",(0,i.jsx)(e.h2,{id:"isaac-rl-framework",children:"Isaac RL Framework"}),"\n",(0,i.jsx)(e.p,{children:"NVIDIA Isaac provides an optimized RL framework that includes:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Isaac Lab"}),": Framework for robot learning and deployment"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"GPU-accelerated simulation"}),": Parallel environments for faster training"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Pre-built RL algorithms"}),": PPO, SAC, TD3, and more"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Domain randomization"}),": Transfer learning from simulation to reality"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"isaac-lab-setup",children:"Isaac Lab Setup"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'# Setting up Isaac Lab for RL training\nimport omni\nfrom omni.isaac.kit import SimulationApp\n\n# Launch Isaac Sim with RL configuration\nconfig = {\n    "headless": False,\n    "enable_cameras": True,\n    "physics_dt": 1.0/60.0,\n    "rendering_dt": 1.0/60.0,\n    "stage_units_in_meters": 1.0\n}\n\nsimulation_app = SimulationApp(config)\n\n# Import Isaac Lab components\nfrom omni.isaac.orbit_tasks.utils import parse_env_cfg\nfrom omni.isaac.orbit_tasks.locomotion.velocity.velocity_env_cfg import AnymalDFlatEnvCfg\n'})}),"\n",(0,i.jsx)(e.h3,{id:"rl-environment-configuration",children:"RL Environment Configuration"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'# Example RL environment configuration for humanoid\nfrom omni.isaac.orbit_assets import HUMANOID_ASSETS\nfrom omni.isaac.orbit.envs import RLTaskEnv\nfrom omni.isaac.orbit.assets import AssetBaseCfg\nfrom omni.isaac.orbit.managers import SceneEntityCfg\nfrom omni.isaac.orbit.sensors import ContactSensorCfg\nfrom omni.isaac.orbit actuators import DCMotorCfg\n\nclass HumanoidEnvCfg:\n    # Scene\n    scene = SceneEntityCfg(\n        num_envs=4096,  # Number of parallel environments\n        env_spacing=2.5,\n    )\n\n    # Robot\n    robot = AssetBaseCfg(\n        prim_path="{ENV_REGEX_NS}/Robot",\n        spawn=HUMANOID_ASSETS.HUMANOID_PX,\n        init_state={\n            "joint_pos": {\n                ".*": 0.0,\n            },\n            "joint_vel": {\n                ".*": 0.0,\n            },\n        },\n    )\n\n    # Actuators\n    actuators = {\n        "legs": DCMotorCfg(\n            joint_names=["hip_.*", "knee_.*", "ankle_.*"],\n            effort_limit=80.0,\n            velocity_limit=100.0,\n            stiffness=10.0,\n            damping=1.0,\n        ),\n        "arms": DCMotorCfg(\n            joint_names=["shoulder_.*", "elbow_.*"],\n            effort_limit=40.0,\n            velocity_limit=100.0,\n            stiffness=5.0,\n            damping=0.5,\n        ),\n    }\n\n    # Sensors\n    contact_sensor = ContactSensorCfg(\n        prim_path="{ENV_REGEX_NS}/Robot/torso",\n        update_period=0.005,\n        filter_prim_paths_expr=["{ENV_REGEX_NS}/.*"],\n    )\n\n    # Rewards\n    class rewards:\n        termination_penalty = -200.0\n        tracking_lin_vel = 1.0\n        tracking_ang_vel = 0.5\n        lin_vel_z = -2.0\n        ang_vel_xy = -0.05\n        dof_vel = -0.01\n        action_rate = -0.01\n        stand_still = -0.5\n'})}),"\n",(0,i.jsx)(e.h2,{id:"deep-reinforcement-learning-algorithms",children:"Deep Reinforcement Learning Algorithms"}),"\n",(0,i.jsx)(e.h3,{id:"proximal-policy-optimization-ppo",children:"Proximal Policy Optimization (PPO)"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'# PPO implementation for humanoid locomotion\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.distributions import Normal\n\nclass ActorCritic(nn.Module):\n    def __init__(self, state_dim, action_dim, hidden_dim=256):\n        super(ActorCritic, self).__init__()\n\n        # Shared feature extractor\n        self.feature_extractor = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.Tanh(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.Tanh()\n        )\n\n        # Actor (policy network)\n        self.actor_mean = nn.Linear(hidden_dim, action_dim)\n        self.actor_logstd = nn.Parameter(torch.zeros(action_dim))\n\n        # Critic (value network)\n        self.critic = nn.Linear(hidden_dim, 1)\n\n    def forward(self, state):\n        features = self.feature_extractor(state)\n\n        # Actor\n        action_mean = torch.tanh(self.actor_mean(features))\n        action_std = torch.exp(self.actor_logstd)\n\n        # Critic\n        value = self.critic(features)\n\n        return action_mean, action_std, value\n\nclass PPOAgent:\n    def __init__(self, state_dim, action_dim, lr=3e-4, gamma=0.99, eps_clip=0.2):\n        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\n\n        self.actor_critic = ActorCritic(state_dim, action_dim).to(self.device)\n        self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=lr)\n\n        self.gamma = gamma\n        self.eps_clip = eps_clip\n        self.MseLoss = nn.MSELoss()\n\n    def select_action(self, state):\n        state = torch.FloatTensor(state).to(self.device).unsqueeze(0)\n        action_mean, action_std, _ = self.actor_critic(state)\n\n        # Sample action from normal distribution\n        dist = Normal(action_mean, action_std)\n        action = dist.sample()\n        action_logprob = dist.log_prob(action).sum(dim=1)\n\n        return action.cpu().data.numpy().flatten(), action_logprob.cpu().data.numpy()\n\n    def update(self, states, actions, rewards, logprobs, next_states, dones):\n        # Convert to tensors\n        states = torch.FloatTensor(states).to(self.device)\n        actions = torch.FloatTensor(actions).to(self.device)\n        rewards = torch.FloatTensor(rewards).to(self.device)\n        old_logprobs = torch.FloatTensor(logprobs).to(self.device)\n\n        # Get current policy values\n        action_means, action_stds, values = self.actor_critic(states)\n        dist = Normal(action_means, action_stds)\n        logprobs = dist.log_prob(actions).sum(dim=1)\n        entropy = dist.entropy().sum(dim=1)\n        new_values = self.actor_critic(states)[2]\n\n        # Calculate advantages\n        advantages = rewards + self.gamma * new_values.detach() * (1 - dones) - values.detach()\n\n        # Calculate ratios\n        ratios = torch.exp(logprobs - old_logprobs)\n\n        # PPO loss\n        surr1 = ratios * advantages\n        surr2 = torch.clamp(ratings, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n        actor_loss = -torch.min(surr1, surr2).mean()\n        critic_loss = advantages.pow(2).mean()\n        entropy_loss = entropy.mean()\n\n        # Total loss\n        loss = actor_loss + 0.5 * critic_loss - 0.01 * entropy_loss\n\n        # Update\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n'})}),"\n",(0,i.jsx)(e.h3,{id:"soft-actor-critic-sac-implementation",children:"Soft Actor-Critic (SAC) Implementation"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'# SAC implementation for continuous humanoid control\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam\n\nclass SACAgent:\n    def __init__(self, state_dim, action_dim, hidden_dim=256, alpha=0.2):\n        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\n\n        # Networks\n        self.actor = GaussianPolicy(state_dim, action_dim, hidden_dim).to(self.device)\n        self.critic = QNetwork(state_dim, action_dim, hidden_dim).to(self.device)\n        self.target_critic = QNetwork(state_dim, action_dim, hidden_dim).to(self.device)\n\n        # Copy weights to target\n        for target_param, param in zip(self.target_critic.parameters(), self.critic.parameters()):\n            target_param.data.copy_(param.data)\n\n        # Optimizers\n        self.actor_optimizer = Adam(self.actor.parameters(), lr=3e-4)\n        self.critic_optimizer = Adam(self.critic.parameters(), lr=3e-4)\n\n        self.alpha = alpha\n        self.target_entropy = -torch.prod(torch.Tensor(action_dim).to(self.device)).item()\n        self.log_alpha = torch.zeros(1, requires_grad=True, device=self.device)\n        self.alpha_optimizer = Adam([self.log_alpha], lr=3e-4)\n\n    def select_action(self, state, evaluate=False):\n        state = torch.FloatTensor(state).to(self.device).unsqueeze(0)\n        if evaluate:\n            _, _, action = self.actor.sample(state)\n        else:\n            action, _, _ = self.actor.sample(state)\n        return action.cpu().data.numpy().flatten()\n\nclass GaussianPolicy(nn.Module):\n    def __init__(self, num_inputs, num_actions, hidden_dim=256, action_space=None):\n        super(GaussianPolicy, self).__init__()\n\n        self.linear1 = nn.Linear(num_inputs, hidden_dim)\n        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n\n        self.mean_linear = nn.Linear(hidden_dim, num_actions)\n        self.log_std_linear = nn.Linear(hidden_dim, num_actions)\n\n        self.apply(weights_init_)\n\n        # Action rescaling\n        if action_space is None:\n            self.action_scale = torch.tensor(1.)\n            self.action_bias = torch.tensor(0.)\n        else:\n            self.action_scale = torch.FloatTensor((action_space.high - action_space.low) / 2.)\n            self.action_bias = torch.FloatTensor((action_space.high + action_space.low) / 2.)\n\n    def forward(self, state):\n        x = F.relu(self.linear1(state))\n        x = F.relu(self.linear2(x))\n        mean = self.mean_linear(x)\n        log_std = self.log_std_linear(x)\n        log_std = torch.clamp(log_std, min=-20, max=2)\n        return mean, log_std\n\n    def sample(self, state):\n        mean, log_std = self.forward(state)\n        std = log_std.exp()\n        normal = torch.distributions.Normal(mean, std)\n        x_t = normal.rsample()  # Reparameterization trick\n        y_t = torch.tanh(x_t)\n        action = y_t * self.action_scale + self.action_bias\n        log_prob = normal.log_prob(x_t)\n        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)\n        log_prob = log_prob.sum(1, keepdim=True)\n        mean = torch.tanh(mean) * self.action_scale + self.action_bias\n        return action, log_prob, mean\n'})}),"\n",(0,i.jsx)(e.h2,{id:"isaac-specific-rl-training",children:"Isaac-Specific RL Training"}),"\n",(0,i.jsx)(e.h3,{id:"parallel-environment-training",children:"Parallel Environment Training"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'# Parallel environment setup for faster RL training\nimport gymnasium as gym\nfrom omni.isaac.orbit.envs import RLTaskEnv\nfrom omni.isaac.orbit_tasks.utils import parse_env_cfg\nimport torch\n\ndef train_humanoid_rl():\n    # Parse environment configuration\n    env_cfg = parse_env_cfg("Isaac-Velocity-Flat-Humanoid-v0", use_gpu=True)\n\n    # Create environment\n    env = gym.make("Isaac-Velocity-Flat-Humanoid-v0", cfg=env_cfg)\n\n    # Get dimensions\n    obs_dim = env.observation_space.shape[0]\n    action_dim = env.action_space.shape[0]\n\n    # Initialize agent\n    agent = PPOAgent(obs_dim, action_dim)\n\n    # Training loop\n    for episode in range(10000):\n        obs, _ = env.reset()\n        episode_reward = 0\n        episode_steps = 0\n\n        while True:\n            # Select action\n            action, logprob = agent.select_action(obs)\n\n            # Step environment\n            next_obs, reward, terminated, truncated, info = env.step(action)\n\n            # Store transition\n            agent.store_transition(obs, action, reward, logprob, next_obs, terminated)\n\n            obs = next_obs\n            episode_reward += reward\n            episode_steps += 1\n\n            if terminated or truncated:\n                break\n\n        # Update agent\n        if episode % 10 == 0:\n            agent.update_networks()\n\n        print(f"Episode {episode}: Reward = {episode_reward:.2f}")\n\n    env.close()\n'})}),"\n",(0,i.jsx)(e.h3,{id:"domain-randomization",children:"Domain Randomization"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"# Domain randomization for sim-to-real transfer\nfrom omni.isaac.orbit.assets import AssetBaseCfg\nimport numpy as np\n\nclass DomainRandomization:\n    def __init__(self):\n        self.randomization_params = {\n            'mass_ratio_range': [0.8, 1.2],\n            'friction_range': [0.5, 1.5],\n            'restitution_range': [0.0, 0.2],\n            'dof_damping_range': [0.5, 1.5],\n            'actuator_strength_range': [0.8, 1.2],\n        }\n\n    def randomize_mass(self, asset_cfg):\n        # Randomize mass properties\n        mass_ratio = np.random.uniform(\n            self.randomization_params['mass_ratio_range'][0],\n            self.randomization_params['mass_ratio_range'][1]\n        )\n        # Apply mass scaling to asset\n        return asset_cfg\n\n    def randomize_dynamics(self, asset_cfg):\n        # Randomize dynamic properties\n        friction = np.random.uniform(\n            self.randomization_params['friction_range'][0],\n            self.randomization_params['friction_range'][1]\n        )\n        # Apply friction scaling\n        return asset_cfg\n\n    def apply_randomization(self, env_cfg):\n        # Apply domain randomization to environment\n        env_cfg.scene = self.randomize_mass(env_cfg.scene)\n        env_cfg.scene = self.randomize_dynamics(env_cfg.scene)\n        return env_cfg\n"})}),"\n",(0,i.jsx)(e.h2,{id:"locomotion-learning",children:"Locomotion Learning"}),"\n",(0,i.jsx)(e.h3,{id:"walking-gait-learning",children:"Walking Gait Learning"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"# Learning humanoid walking gaits with RL\nimport torch\nimport numpy as np\n\nclass GaitLearning:\n    def __init__(self, robot_model):\n        self.robot = robot_model\n        self.gait_patterns = {\n            'walk': self.walk_gait,\n            'trot': self.trot_gait,\n            'pace': self.pace_gait,\n        }\n\n    def walk_gait(self, phase, speed=1.0):\n        # Generate walking gait pattern\n        # phase: 0 to 2*pi, represents gait cycle\n        left_leg_phase = phase\n        right_leg_phase = (phase + np.pi) % (2 * np.pi)\n\n        # Calculate joint angles for walking\n        left_hip = 0.2 * np.sin(left_leg_phase)\n        left_knee = 0.3 * np.sin(2 * left_leg_phase)\n        left_ankle = -0.1 * np.sin(left_leg_phase)\n\n        right_hip = 0.2 * np.sin(right_leg_phase)\n        right_knee = 0.3 * np.sin(2 * right_leg_phase)\n        right_ankle = -0.1 * np.sin(right_leg_phase)\n\n        return np.array([left_hip, left_knee, left_ankle,\n                        right_hip, right_knee, right_ankle])\n\n    def train_gait_network(self):\n        # Train neural network to generate gait patterns\n        gait_network = nn.Sequential(\n            nn.Linear(3, 64),  # phase, speed, terrain\n            nn.ReLU(),\n            nn.Linear(64, 128),\n            nn.ReLU(),\n            nn.Linear(128, 64),  # joint commands\n            nn.Tanh()\n        )\n        return gait_network\n"})}),"\n",(0,i.jsx)(e.h3,{id:"balance-recovery-learning",children:"Balance Recovery Learning"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"# Learning balance recovery behaviors\nclass BalanceRecovery:\n    def __init__(self, robot_model):\n        self.robot = robot_model\n        self.com_estimator = CenterOfMassEstimator()\n        self.balance_policy = BalancePolicyNetwork()\n\n    def estimate_balance_state(self):\n        # Estimate current balance state\n        com_pos = self.com_estimator.get_com_position()\n        com_vel = self.com_estimator.get_com_velocity()\n        robot_orientation = self.robot.get_orientation()\n        robot_angular_vel = self.robot.get_angular_velocity()\n\n        # Balance state vector\n        balance_state = np.concatenate([\n            com_pos, com_vel,\n            robot_orientation,\n            robot_angular_vel\n        ])\n        return balance_state\n\n    def compute_balance_action(self, balance_state):\n        # Use trained policy to compute balance recovery action\n        with torch.no_grad():\n            state_tensor = torch.FloatTensor(balance_state).unsqueeze(0)\n            action = self.balance_policy(state_tensor)\n        return action.numpy().flatten()\n\n    def train_balance_policy(self):\n        # Train balance recovery policy using RL\n        # Reward function encourages staying upright\n        def balance_reward(state, action, next_state):\n            com_height = next_state[2]  # z-component of COM\n            orientation = next_state[6:10]  # quaternion\n            angular_vel = next_state[10:13]\n\n            # Reward for maintaining upright position\n            upright_reward = 10.0 * (1 - abs(orientation[2]))  # z component of quaternion\n            height_reward = 5.0 * max(0, com_height - 0.5)  # minimum height\n            stability_reward = -2.0 * np.linalg.norm(angular_vel)  # penalize angular velocity\n\n            return upright_reward + height_reward + stability_reward\n"})}),"\n",(0,i.jsx)(e.h2,{id:"manipulation-learning",children:"Manipulation Learning"}),"\n",(0,i.jsx)(e.h3,{id:"object-manipulation-with-rl",children:"Object Manipulation with RL"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"# Learning manipulation tasks with RL\nclass ManipulationLearning:\n    def __init__(self, robot_model):\n        self.robot = robot_model\n        self.ik_solver = InverseKinematicsSolver()\n        self.manipulation_agent = ManipulationRLAgent()\n\n    def grasp_object(self, object_pose):\n        # Learn to grasp objects using RL\n        grasp_policy = self.manipulation_agent.get_grasp_policy()\n\n        # Calculate grasp approach\n        approach_poses = self.calculate_grasp_approach(object_pose)\n\n        for approach_pose in approach_poses:\n            # Use RL policy to determine grasp parameters\n            grasp_params = grasp_policy(approach_pose)\n\n            # Execute grasp\n            success = self.execute_grasp(approach_pose, grasp_params)\n\n            if success:\n                return True\n\n        return False\n\n    def calculate_grasp_approach(self, object_pose):\n        # Calculate multiple grasp approach poses\n        approaches = []\n        for angle in np.linspace(0, 2*np.pi, 8):\n            approach_pose = object_pose.copy()\n            approach_pose[0] += 0.1 * np.cos(angle)  # offset from object\n            approach_pose[1] += 0.1 * np.sin(angle)\n            approaches.append(approach_pose)\n        return approaches\n"})}),"\n",(0,i.jsx)(e.h2,{id:"multi-task-learning",children:"Multi-Task Learning"}),"\n",(0,i.jsx)(e.h3,{id:"hierarchical-rl-for-complex-tasks",children:"Hierarchical RL for Complex Tasks"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"# Hierarchical reinforcement learning for complex humanoid tasks\nclass HierarchicalRL:\n    def __init__(self):\n        self.high_level_policy = HighLevelPolicy()\n        self.low_level_skills = {\n            'walk': SkillPolicy('walk'),\n            'grasp': SkillPolicy('grasp'),\n            'balance': SkillPolicy('balance'),\n            'manipulate': SkillPolicy('manipulate')\n        }\n\n    def execute_task(self, high_level_goal):\n        # Use high-level policy to determine sequence of skills\n        skill_sequence = self.high_level_policy.plan(high_level_goal)\n\n        for skill in skill_sequence:\n            if skill in self.low_level_skills:\n                success = self.low_level_skills[skill].execute()\n                if not success:\n                    return False\n\n        return True\n\nclass SkillPolicy:\n    def __init__(self, skill_name):\n        self.skill_name = skill_name\n        self.skill_network = self.build_skill_network()\n\n    def build_skill_network(self):\n        # Build skill-specific neural network\n        if self.skill_name == 'walk':\n            return nn.Sequential(\n                nn.Linear(24, 128),  # state: 24-dim\n                nn.ReLU(),\n                nn.Linear(128, 64),\n                nn.ReLU(),\n                nn.Linear(64, 12)  # 6 joints per leg\n            )\n        # Add other skill networks...\n        return nn.Sequential()\n\n    def execute(self):\n        # Execute the skill using trained policy\n        pass\n"})}),"\n",(0,i.jsx)(e.h2,{id:"isaac-rl-training-pipeline",children:"Isaac RL Training Pipeline"}),"\n",(0,i.jsx)(e.h3,{id:"training-script",children:"Training Script"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n# Isaac RL training script for humanoid\nimport hydra\nfrom omegaconf import DictConfig\nfrom omni.isaac.orbit_tasks.utils import parse_env_cfg\nfrom omni.isaac.orbit_tasks.utils.wrappers.sb3 import Sb3VecEnvWrapper\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.callbacks import EvalCallback\nimport torch\n\n@hydra.main(config_path="cfg", config_name="train_cfg", version_base="1.2")\ndef main(cfg: DictConfig):\n    """Main function for training humanoid RL agent."""\n    # Parse environment configuration\n    env_cfg = parse_env_cfg(cfg.env_id, use_gpu=True)\n\n    # Create environment\n    env = hydra.utils.instantiate(cfg.task, cfg=env_cfg)\n    env = Sb3VecEnvWrapper(env)\n\n    # Create agent\n    agent = PPO(\n        policy=cfg.policy,\n        env=env,\n        learning_rate=cfg.learning_rate,\n        n_steps=cfg.n_steps,\n        batch_size=cfg.batch_size,\n        n_epochs=cfg.n_epochs,\n        gamma=cfg.gamma,\n        gae_lambda=cfg.gae_lambda,\n        clip_range=cfg.clip_range,\n        verbose=1,\n        device="cuda" if torch.cuda.is_available() else "cpu"\n    )\n\n    # Training\n    agent.learn(total_timesteps=cfg.total_timesteps)\n\n    # Save model\n    agent.save(cfg.output_dir + "/humanoid_ppo_model")\n\n    # Close environment\n    env.close()\n\nif __name__ == "__main__":\n    main()\n'})}),"\n",(0,i.jsx)(e.h3,{id:"configuration-file",children:"Configuration File"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-yaml",children:'# train_cfg.yaml\ndefaults:\n  - task: isaac-humanoid-env\n  - _self_\n\nenv_id: "Isaac-Velocity-Flat-Humanoid-v0"\n\ntask:\n  _target_: omni.isaac.orbit.envs.RLTaskEnv\n  cfg: ${...}\n\npolicy:\n  _target_: stable_baselines3.common.policies.ActorCriticPolicy\n\nlearning_rate: 1e-3\nn_steps: 2048\nbatch_size: 64\nn_epochs: 10\ngamma: 0.99\ngae_lambda: 0.95\nclip_range: 0.2\n\ntotal_timesteps: 10000000\noutput_dir: "outputs/humanoid_ppo"\n\n# Training parameters\nseed: 42\ncapture_video: True\ncapture_video_freq: 2000\ncapture_video_length: 100\n'})}),"\n",(0,i.jsx)(e.h2,{id:"transfer-learning-and-deployment",children:"Transfer Learning and Deployment"}),"\n",(0,i.jsx)(e.h3,{id:"sim-to-real-transfer",children:"Sim-to-Real Transfer"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"# Sim-to-real transfer techniques\nclass SimToRealTransfer:\n    def __init__(self):\n        self.domain_randomization = DomainRandomization()\n        self.system_identification = SystemIdentification()\n        self.adaptation_network = AdaptationNetwork()\n\n    def prepare_for_real_world(self, sim_policy):\n        # Adapt simulation policy for real world\n        real_params = self.system_identification.identify_real_robot()\n        adapted_policy = self.adaptation_network.adapt(\n            sim_policy,\n            real_params\n        )\n        return adapted_policy\n\n    def online_adaptation(self, policy, real_data):\n        # Online adaptation during real-world deployment\n        adaptation_loss = self.compute_adaptation_loss(policy, real_data)\n        policy.update(adaptation_loss)\n        return policy\n\nclass SystemIdentification:\n    def identify_real_robot(self):\n        # Identify real robot parameters\n        # Mass, friction, actuator dynamics, etc.\n        return {\n            'mass': self.measure_mass(),\n            'friction': self.measure_friction(),\n            'actuator_dynamics': self.measure_actuator_dynamics()\n        }\n"})}),"\n",(0,i.jsx)(e.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,i.jsx)(e.h3,{id:"isaac-specific-optimizations",children:"Isaac-Specific Optimizations"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'# Optimized RL training for Isaac\nclass OptimizedRLTraining:\n    def __init__(self):\n        # Enable GPU acceleration\n        torch.backends.cudnn.benchmark = True\n        self.use_mixed_precision = True\n        self.use_tensor_cores = True\n\n    def optimize_training_loop(self):\n        # Optimize training with gradient accumulation\n        scaler = torch.cuda.amp.GradScaler(enabled=self.use_mixed_precision)\n\n        for batch in training_batches:\n            with torch.cuda.amp.autocast(enabled=self.use_mixed_precision):\n                loss = self.compute_loss(batch)\n\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n\n            # Zero gradients\n            optimizer.zero_grad()\n\n    def parallel_environment_optimization(self):\n        # Optimize parallel environments\n        env_cfg = parse_env_cfg("Isaac-Humanoid-v0")\n        env_cfg.scene.num_envs = 4096  # Max parallel environments\n\n        # Memory optimization\n        env_cfg.simulation.device = "cuda:0"\n        env_cfg.simulation.use_gpu_pipeline = True\n\n        return env_cfg\n'})}),"\n",(0,i.jsx)(e.h2,{id:"evaluation-and-testing",children:"Evaluation and Testing"}),"\n",(0,i.jsx)(e.h3,{id:"rl-performance-metrics",children:"RL Performance Metrics"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"# RL evaluation metrics\nclass RLEvaluation:\n    def __init__(self):\n        self.metrics = {\n            'episode_reward': [],\n            'episode_length': [],\n            'success_rate': [],\n            'stability_score': [],\n            'energy_efficiency': []\n        }\n\n    def evaluate_policy(self, policy, num_episodes=100):\n        for episode in range(num_episodes):\n            obs, _ = env.reset()\n            episode_reward = 0\n            episode_length = 0\n            success = False\n\n            while True:\n                action = policy.select_action(obs)\n                obs, reward, terminated, truncated, info = env.step(action)\n\n                episode_reward += reward\n                episode_length += 1\n\n                if terminated or truncated:\n                    success = info.get('success', False)\n                    break\n\n            # Record metrics\n            self.metrics['episode_reward'].append(episode_reward)\n            self.metrics['episode_length'].append(episode_length)\n            self.metrics['success_rate'].append(success)\n            self.metrics['stability_score'].append(self.calculate_stability(obs))\n\n    def calculate_stability(self, state):\n        # Calculate stability based on COM position and orientation\n        com_pos = state[:3]\n        orientation = state[3:7]  # quaternion\n\n        # Stability is higher when COM is within support polygon\n        # and robot is upright\n        stability = 1.0 - abs(orientation[2])  # upright measure\n        return stability\n"})}),"\n",(0,i.jsx)(e.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,i.jsx)(e.h3,{id:"common-rl-issues-and-solutions",children:"Common RL Issues and Solutions"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Training Instability"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Reduce learning rate"}),"\n",(0,i.jsx)(e.li,{children:"Increase batch size"}),"\n",(0,i.jsx)(e.li,{children:"Use gradient clipping"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Poor Convergence"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Check reward function design"}),"\n",(0,i.jsx)(e.li,{children:"Verify action space bounds"}),"\n",(0,i.jsx)(e.li,{children:"Adjust network architecture"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Sim-to-Real Gap"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Increase domain randomization"}),"\n",(0,i.jsx)(e.li,{children:"Add noise to sensors"}),"\n",(0,i.jsx)(e.li,{children:"Use system identification"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.a,{href:"/ai-robotic-book/modules/ai-robot-brain/sim2real",children:"Next: Simulation-to-Reality"})," | ",(0,i.jsx)(e.a,{href:"/ai-robotic-book/modules/ai-robot-brain/planning-control",children:"Previous: Planning and Control"})]})]})}function p(n={}){const{wrapper:e}={...(0,t.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(d,{...n})}):d(n)}},8453:(n,e,a)=>{a.d(e,{R:()=>r,x:()=>s});var i=a(6540);const t={},o=i.createContext(t);function r(n){const e=i.useContext(o);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:r(n.components),i.createElement(o.Provider,{value:e},n.children)}}}]);