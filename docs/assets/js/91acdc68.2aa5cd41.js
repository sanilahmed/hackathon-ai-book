"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[3388],{8453:(e,n,a)=>{a.d(n,{R:()=>o,x:()=>l});var t=a(6540);const i={},s=t.createContext(i);function o(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),t.createElement(s.Provider,{value:n},e.children)}},9636:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>r,contentTitle:()=>o,default:()=>c,frontMatter:()=>s,metadata:()=>l,toc:()=>d});var t=a(4848),i=a(8453);const s={},o="Lab 4.2: Multimodal Perception in Vision-Language-Action Systems",l={id:"modules/lab-exercises/lab-4-2-multimodal-perception",title:"Lab 4.2: Multimodal Perception in Vision-Language-Action Systems",description:"Overview",source:"@site/docs/modules/lab-exercises/lab-4-2-multimodal-perception.md",sourceDirName:"modules/lab-exercises",slug:"/modules/lab-exercises/lab-4-2-multimodal-perception",permalink:"/hackathon-ai-book/modules/lab-exercises/lab-4-2-multimodal-perception",draft:!1,unlisted:!1,editUrl:"https://github.com/sanilahmed/hackathon-ai-book/tree/main/docs/modules/lab-exercises/lab-4-2-multimodal-perception.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Lab 4.1: Vision-Language-Action (VLA) Fundamentals",permalink:"/hackathon-ai-book/modules/lab-exercises/lab-4-1-vla-fundamentals"},next:{title:"Lab 4.3: Action Mapping in Vision-Language-Action Systems",permalink:"/hackathon-ai-book/modules/lab-exercises/lab-4-3-action-mapping"}},r={},d=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Theory Background",id:"theory-background",level:2},{value:"Lab Exercise",id:"lab-exercise",level:2},{value:"Part 1: Cross-Modal Attention Implementation",id:"part-1-cross-modal-attention-implementation",level:3},{value:"Part 2: Multimodal Fusion Network",id:"part-2-multimodal-fusion-network",level:3},{value:"Part 3: Vision-Language-Action Integration",id:"part-3-vision-language-action-integration",level:3},{value:"Part 4: Evaluation Metrics for Multimodal Perception",id:"part-4-evaluation-metrics-for-multimodal-perception",level:3},{value:"Part 5: Integration with ROS 2",id:"part-5-integration-with-ros-2",level:3},{value:"Implementation Steps",id:"implementation-steps",level:2},{value:"Expected Outcomes",id:"expected-outcomes",level:2},{value:"Troubleshooting Tips",id:"troubleshooting-tips",level:2},{value:"Further Exploration",id:"further-exploration",level:2}];function u(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"lab-42-multimodal-perception-in-vision-language-action-systems",children:"Lab 4.2: Multimodal Perception in Vision-Language-Action Systems"}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(n.p,{children:"This lab exercise focuses on implementing multimodal perception systems that combine visual, linguistic, and action data for humanoid robots. Students will learn to integrate multiple sensory inputs and create unified representations for decision-making."}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"By the end of this lab, students will be able to:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Implement multimodal fusion architectures combining vision and language"}),"\n",(0,t.jsx)(n.li,{children:"Design attention mechanisms for cross-modal information processing"}),"\n",(0,t.jsx)(n.li,{children:"Create unified feature spaces for vision-language-action systems"}),"\n",(0,t.jsx)(n.li,{children:"Evaluate multimodal perception performance using relevant metrics"}),"\n",(0,t.jsx)(n.li,{children:"Integrate multimodal perception with action planning systems"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Completion of Lab 4.1: VLA Fundamentals"}),"\n",(0,t.jsx)(n.li,{children:"Understanding of deep learning frameworks (PyTorch/TensorFlow)"}),"\n",(0,t.jsx)(n.li,{children:"Basic knowledge of computer vision and natural language processing"}),"\n",(0,t.jsx)(n.li,{children:"Familiarity with ROS 2 for robot control"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"theory-background",children:"Theory Background"}),"\n",(0,t.jsx)(n.p,{children:"Multimodal perception combines information from multiple sensory modalities to create a more robust and comprehensive understanding of the environment. In VLA systems, this typically involves:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Visual Processing"}),": Extracting spatial and object information from camera feeds"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Language Processing"}),": Understanding commands, descriptions, and contextual information"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cross-Modal Attention"}),": Mechanisms that allow different modalities to attend to relevant information"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Unified Representations"}),": Embedding space where different modalities can interact meaningfully"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"lab-exercise",children:"Lab Exercise"}),"\n",(0,t.jsx)(n.h3,{id:"part-1-cross-modal-attention-implementation",children:"Part 1: Cross-Modal Attention Implementation"}),"\n",(0,t.jsx)(n.p,{children:"First, let's implement a cross-modal attention mechanism that can attend to relevant visual regions based on language descriptions:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass CrossModalAttention(nn.Module):\n    def __init__(self, d_model, num_heads=8):\n        super().__init__()\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.head_dim = d_model // num_heads\n\n        # Linear projections for queries, keys, values\n        self.q_proj = nn.Linear(d_model, d_model)\n        self.k_proj = nn.Linear(d_model, d_model)\n        self.v_proj = nn.Linear(d_model, d_model)\n\n        # Output projection\n        self.out_proj = nn.Linear(d_model, d_model)\n\n    def forward(self, visual_features, language_features):\n        """\n        Args:\n            visual_features: [batch_size, seq_len_v, d_model] - Visual features from CNN\n            language_features: [batch_size, seq_len_l, d_model] - Language features from encoder\n        Returns:\n            attended_features: [batch_size, seq_len_v, d_model] - Visual features attended by language\n        """\n        batch_size = visual_features.size(0)\n\n        # Project features\n        Q = self.q_proj(language_features)  # Language as queries\n        K = self.k_proj(visual_features)    # Visual as keys\n        V = self.v_proj(visual_features)    # Visual as values\n\n        # Reshape for multi-head attention\n        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        K = K.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        V = V.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n\n        # Scaled dot-product attention\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n        attention_weights = F.softmax(scores, dim=-1)\n\n        # Apply attention to values\n        attended_values = torch.matmul(attention_weights, V)\n\n        # Reshape back to original dimensions\n        attended_values = attended_values.transpose(1, 2).contiguous().view(\n            batch_size, -1, self.d_model\n        )\n\n        # Output projection\n        output = self.out_proj(attended_values)\n\n        return output\n\n# Test the cross-modal attention\ndef test_cross_modal_attention():\n    batch_size, seq_len_v, seq_len_l, d_model = 2, 49, 10, 256  # 7x7 visual grid, 10-word sentence\n\n    visual_features = torch.randn(batch_size, seq_len_v, d_model)\n    language_features = torch.randn(batch_size, seq_len_l, d_model)\n\n    attention_module = CrossModalAttention(d_model)\n    attended_output = attention_module(visual_features, language_features)\n\n    print(f"Input visual shape: {visual_features.shape}")\n    print(f"Input language shape: {language_features.shape}")\n    print(f"Output attended shape: {attended_output.shape}")\n\n    return attended_output\n\nif __name__ == "__main__":\n    test_cross_modal_attention()\n'})}),"\n",(0,t.jsx)(n.h3,{id:"part-2-multimodal-fusion-network",children:"Part 2: Multimodal Fusion Network"}),"\n",(0,t.jsx)(n.p,{children:"Now let's create a multimodal fusion network that combines visual and language features:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class MultimodalFusion(nn.Module):\n    def __init__(self, visual_dim, language_dim, fused_dim):\n        super().__init__()\n        self.visual_dim = visual_dim\n        self.language_dim = language_dim\n        self.fused_dim = fused_dim\n\n        # Individual modality encoders\n        self.visual_encoder = nn.Sequential(\n            nn.Linear(visual_dim, fused_dim),\n            nn.ReLU(),\n            nn.Dropout(0.1)\n        )\n\n        self.language_encoder = nn.Sequential(\n            nn.Linear(language_dim, fused_dim),\n            nn.ReLU(),\n            nn.Dropout(0.1)\n        )\n\n        # Cross-attention modules\n        self.visual_to_language = CrossModalAttention(fused_dim)\n        self.language_to_visual = CrossModalAttention(fused_dim)\n\n        # Fusion layers\n        self.fusion_layer = nn.Sequential(\n            nn.Linear(fused_dim * 2, fused_dim),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(fused_dim, fused_dim)\n        )\n\n        # Residual connection\n        self.residual_proj = nn.Linear(fused_dim * 2, fused_dim)\n\n    def forward(self, visual_input, language_input):\n        """\n        Args:\n            visual_input: [batch_size, visual_dim] or [batch_size, seq_len_v, visual_dim]\n            language_input: [batch_size, language_dim] or [batch_size, seq_len_l, language_dim]\n        Returns:\n            fused_output: [batch_size, fused_dim] - Fused multimodal representation\n        """\n        # Encode individual modalities\n        encoded_visual = self.visual_encoder(visual_input)\n        encoded_language = self.language_encoder(language_input)\n\n        # Cross-attention: language attends to visual, visual attends to language\n        lang_attended_vis = self.visual_to_language(encoded_visual, encoded_language)\n        vis_attended_lang = self.language_to_visual(encoded_language, encoded_visual)\n\n        # Aggregate attended features (mean pooling for sequences)\n        if len(lang_attended_vis.shape) > 2:\n            lang_attended_vis = lang_attended_vis.mean(dim=1)  # Average over sequence dimension\n        if len(vis_attended_lang.shape) > 2:\n            vis_attended_lang = vis_attended_lang.mean(dim=1)  # Average over sequence dimension\n\n        # Concatenate and fuse\n        concat_features = torch.cat([lang_attended_vis, vis_attended_lang], dim=-1)\n\n        # Apply fusion with residual connection\n        fused_output = self.fusion_layer(concat_features)\n        residual = self.residual_proj(concat_features)\n\n        final_output = fused_output + residual\n\n        return final_output\n\n# Test the multimodal fusion\ndef test_multimodal_fusion():\n    batch_size, visual_dim, language_dim, fused_dim = 4, 512, 512, 256\n\n    visual_input = torch.randn(batch_size, visual_dim)\n    language_input = torch.randn(batch_size, language_dim)\n\n    fusion_module = MultimodalFusion(visual_dim, language_dim, fused_dim)\n    fused_output = fusion_module(visual_input, language_input)\n\n    print(f"Visual input shape: {visual_input.shape}")\n    print(f"Language input shape: {language_input.shape}")\n    print(f"Fused output shape: {fused_output.shape}")\n\n    return fused_output\n\nif __name__ == "__main__":\n    test_multimodal_fusion()\n'})}),"\n",(0,t.jsx)(n.h3,{id:"part-3-vision-language-action-integration",children:"Part 3: Vision-Language-Action Integration"}),"\n",(0,t.jsx)(n.p,{children:"Now let's create an integrated system that connects perception to action:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class VisionLanguageActionSystem(nn.Module):\n    def __init__(self, visual_backbone, language_backbone, action_space_dim):\n        super().__init__()\n        self.visual_backbone = visual_backbone\n        self.language_backbone = language_backbone\n\n        # Feature dimensions after backbones\n        self.visual_dim = 512  # From visual backbone\n        self.language_dim = 512  # From language backbone\n        self.fused_dim = 256\n\n        # Multimodal fusion\n        self.multimodal_fusion = MultimodalFusion(\n            self.visual_dim, self.language_dim, self.fused_dim\n        )\n\n        # Action prediction head\n        self.action_head = nn.Sequential(\n            nn.Linear(self.fused_dim, 512),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(512, action_space_dim)\n        )\n\n        # Action decoder\n        self.action_decoder = nn.GRU(\n            input_size=action_space_dim,\n            hidden_size=256,\n            num_layers=2,\n            batch_first=True\n        )\n\n    def forward(self, visual_obs, language_command):\n        """\n        Args:\n            visual_obs: [batch_size, channels, height, width] - Visual observation\n            language_command: [batch_size, seq_len, embed_dim] - Language command\n        Returns:\n            action_pred: [batch_size, action_space_dim] - Predicted action\n        """\n        # Extract features from backbones\n        visual_features = self.visual_backbone(visual_obs)  # Shape: [batch, visual_dim]\n        language_features = self.language_backbone(language_command)  # Shape: [batch, seq_len, language_dim]\n\n        # Fuse multimodal information\n        fused_representation = self.multimodal_fusion(visual_features, language_features)\n\n        # Predict initial action\n        action_pred = self.action_head(fused_representation)\n\n        return action_pred\n\n# Dummy backbone models for testing\nclass DummyVisualBackbone(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_layers = nn.Sequential(\n            nn.Conv2d(3, 64, 3, padding=1),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool2d((1, 1)),\n            nn.Flatten(),\n            nn.Linear(64, 512)\n        )\n\n    def forward(self, x):\n        return self.conv_layers(x)\n\nclass DummyLanguageBackbone(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.embedding = nn.Embedding(1000, 512)  # 1000 vocab size\n        self.lstm = nn.LSTM(512, 512, batch_first=True)\n\n    def forward(self, x):\n        embedded = self.embedding(x)\n        lstm_out, _ = self.lstm(embedded)\n        return lstm_out  # Return sequence for attention\n\n# Test the integrated system\ndef test_vla_system():\n    batch_size, channels, height, width = 2, 3, 224, 224\n    seq_len, vocab_size = 10, 1000\n    action_space_dim = 14  # 14 DOF humanoid robot\n\n    visual_obs = torch.randn(batch_size, channels, height, width)\n    language_command = torch.randint(0, vocab_size, (batch_size, seq_len))\n\n    vla_system = VisionLanguageActionSystem(\n        DummyVisualBackbone(),\n        DummyLanguageBackbone(),\n        action_space_dim\n    )\n\n    action_pred = vla_system(visual_obs, language_command)\n\n    print(f"Visual observation shape: {visual_obs.shape}")\n    print(f"Language command shape: {language_command.shape}")\n    print(f"Action prediction shape: {action_pred.shape}")\n    print(f"Action prediction: {action_pred[0]}")\n\n    return action_pred\n\nif __name__ == "__main__":\n    test_vla_system()\n'})}),"\n",(0,t.jsx)(n.h3,{id:"part-4-evaluation-metrics-for-multimodal-perception",children:"Part 4: Evaluation Metrics for Multimodal Perception"}),"\n",(0,t.jsx)(n.p,{children:"Let's implement evaluation metrics for multimodal perception:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import numpy as np\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\nclass MultimodalEvaluator:\n    def __init__(self):\n        pass\n\n    def compute_modality_alignment(self, visual_features, language_features):\n        """\n        Compute alignment between visual and language modalities using cosine similarity\n        """\n        # Normalize features\n        visual_norm = F.normalize(visual_features, p=2, dim=-1)\n        language_norm = F.normalize(language_features, p=2, dim=-1)\n\n        # Compute cosine similarity\n        alignment_scores = torch.sum(visual_norm * language_norm, dim=-1)\n\n        return alignment_scores.mean().item()\n\n    def compute_attention_heatmap_similarity(self, attention_maps_1, attention_maps_2):\n        """\n        Compute similarity between attention heatmaps\n        """\n        # Flatten attention maps and compute correlation\n        flat_map_1 = attention_maps_1.view(attention_maps_1.size(0), -1)\n        flat_map_2 = attention_maps_2.view(attention_maps_2.size(0), -1)\n\n        # Compute cosine similarity between flattened maps\n        similarity = F.cosine_similarity(flat_map_1, flat_map_2, dim=1)\n\n        return similarity.mean().item()\n\n    def evaluate_perception_accuracy(self, predicted_actions, ground_truth_actions, threshold=0.1):\n        """\n        Evaluate perception accuracy based on action predictions\n        """\n        # Compute mean squared error\n        mse = F.mse_loss(predicted_actions, ground_truth_actions)\n\n        # Compute accuracy based on threshold\n        diff = torch.abs(predicted_actions - ground_truth_actions)\n        accuracy = (diff < threshold).float().mean()\n\n        return {\n            \'mse\': mse.item(),\n            \'accuracy\': accuracy.item(),\n            \'rmse\': torch.sqrt(mse).item()\n        }\n\n# Example usage of evaluator\ndef test_evaluation():\n    # Simulate some predictions and ground truth\n    batch_size, action_dim = 8, 14\n    predicted_actions = torch.randn(batch_size, action_dim)\n    ground_truth_actions = torch.randn(batch_size, action_dim)\n\n    evaluator = MultimodalEvaluator()\n    eval_results = evaluator.evaluate_perception_accuracy(\n        predicted_actions,\n        ground_truth_actions\n    )\n\n    print("Evaluation Results:")\n    for metric, value in eval_results.items():\n        print(f"{metric}: {value:.4f}")\n\nif __name__ == "__main__":\n    test_evaluation()\n'})}),"\n",(0,t.jsx)(n.h3,{id:"part-5-integration-with-ros-2",children:"Part 5: Integration with ROS 2"}),"\n",(0,t.jsx)(n.p,{children:"Finally, let's create a ROS 2 node that integrates the multimodal perception system:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CompressedImage\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nimport cv2\nfrom cv_bridge import CvBridge\nimport torch\nimport numpy as np\n\nclass MultimodalPerceptionNode(Node):\n    def __init__(self):\n        super().__init__(\'multimodal_perception_node\')\n\n        # Initialize CV bridge\n        self.bridge = CvBridge()\n\n        # Load the trained VLA model\n        self.load_model()\n\n        # Publishers and subscribers\n        self.image_sub = self.create_subscription(\n            Image,\n            \'/camera/image_raw\',\n            self.image_callback,\n            10\n        )\n\n        self.command_sub = self.create_subscription(\n            String,\n            \'/robot/command\',\n            self.command_callback,\n            10\n        )\n\n        self.action_pub = self.create_publisher(\n            Twist,\n            \'/cmd_vel\',\n            10\n        )\n\n        # Timer for processing loop\n        self.timer = self.create_timer(0.1, self.process_loop)\n\n        # Internal state\n        self.current_image = None\n        self.current_command = None\n        self.last_action = None\n\n    def load_model(self):\n        """Load the trained multimodal perception model"""\n        # Initialize the model (using dummy backbones for this example)\n        self.visual_backbone = DummyVisualBackbone()\n        self.language_backbone = DummyLanguageBackbone()\n\n        self.vla_model = VisionLanguageActionSystem(\n            self.visual_backbone,\n            self.language_backbone,\n            action_space_dim=2  # For Twist message (linear.x, angular.z)\n        )\n\n        # Set model to evaluation mode\n        self.vla_model.eval()\n        self.get_logger().info(\'Multimodal perception model loaded\')\n\n    def image_callback(self, msg):\n        """Process incoming image messages"""\n        try:\n            # Convert ROS image to OpenCV image\n            cv_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")\n\n            # Preprocess image for model\n            self.current_image = self.preprocess_image(cv_image)\n\n        except Exception as e:\n            self.get_logger().error(f\'Error processing image: {str(e)}\')\n\n    def command_callback(self, msg):\n        """Process incoming command messages"""\n        try:\n            # Process natural language command\n            self.current_command = self.process_language_command(msg.data)\n        except Exception as e:\n            self.get_logger().error(f\'Error processing command: {str(e)}\')\n\n    def preprocess_image(self, cv_image):\n        """Preprocess image for the model"""\n        # Resize image to expected input size\n        resized = cv2.resize(cv_image, (224, 224))\n\n        # Convert BGR to RGB and normalize\n        rgb_image = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n        normalized = rgb_image.astype(np.float32) / 255.0\n\n        # Transpose to CHW format and add batch dimension\n        tensor_image = torch.from_numpy(normalized).permute(2, 0, 1).unsqueeze(0)\n\n        return tensor_image\n\n    def process_language_command(self, command_str):\n        """Convert natural language command to tensor representation"""\n        # Simple tokenization for demonstration\n        # In practice, you\'d use a proper tokenizer\n        tokens = command_str.lower().split()[:10]  # Limit to 10 tokens\n\n        # Convert to indices (dummy conversion for demo)\n        vocab = {"move": 1, "forward": 2, "backward": 3, "left": 4, "right": 5, "stop": 6}\n        token_indices = []\n\n        for token in tokens:\n            if token in vocab:\n                token_indices.append(vocab[token])\n            else:\n                token_indices.append(0)  # Unknown token\n\n        # Pad or truncate to fixed length\n        while len(token_indices) < 10:\n            token_indices.append(0)  # Padding token\n\n        return torch.tensor([token_indices])  # Add batch dimension\n\n    def process_loop(self):\n        """Main processing loop"""\n        if self.current_image is not None and self.current_command is not None:\n            try:\n                # Generate action prediction\n                with torch.no_grad():\n                    action_pred = self.vla_model(self.current_image, self.current_command)\n\n                # Convert prediction to Twist message\n                twist_msg = Twist()\n                twist_msg.linear.x = float(action_pred[0, 0])  # Forward/backward\n                twist_msg.angular.z = float(action_pred[0, 1])  # Turn left/right\n\n                # Publish action\n                self.action_pub.publish(twist_msg)\n                self.last_action = action_pred\n\n                self.get_logger().info(f\'Published action: linear.x={twist_msg.linear.x:.3f}, \'\n                                     f\'angular.z={twist_msg.angular.z:.3f}\')\n\n            except Exception as e:\n                self.get_logger().error(f\'Error in processing loop: {str(e)}\')\n\n    def destroy_node(self):\n        """Cleanup when node is destroyed"""\n        super().destroy_node()\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    perception_node = MultimodalPerceptionNode()\n\n    try:\n        rclpy.spin(perception_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        perception_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"implementation-steps",children:"Implementation Steps"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Create the multimodal perception files in your workspace"}),"\n",(0,t.jsx)(n.li,{children:"Implement the cross-modal attention mechanism"}),"\n",(0,t.jsx)(n.li,{children:"Build the fusion network"}),"\n",(0,t.jsx)(n.li,{children:"Integrate with action prediction"}),"\n",(0,t.jsx)(n.li,{children:"Test the evaluation metrics"}),"\n",(0,t.jsx)(n.li,{children:"Deploy the ROS 2 node"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"expected-outcomes",children:"Expected Outcomes"}),"\n",(0,t.jsx)(n.p,{children:"After completing this lab, you should have:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"A working cross-modal attention system that can attend to relevant visual regions based on language"}),"\n",(0,t.jsx)(n.li,{children:"A multimodal fusion network that combines visual and language features effectively"}),"\n",(0,t.jsx)(n.li,{children:"An integrated VLA system that connects perception to action"}),"\n",(0,t.jsx)(n.li,{children:"Evaluation metrics to assess multimodal perception performance"}),"\n",(0,t.jsx)(n.li,{children:"A ROS 2 node that implements multimodal perception for robot control"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"troubleshooting-tips",children:"Troubleshooting Tips"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"If attention weights are NaN, check for proper normalization and gradient clipping"}),"\n",(0,t.jsx)(n.li,{children:"If fusion performance is poor, try adjusting the fusion architecture or training procedure"}),"\n",(0,t.jsx)(n.li,{children:"If ROS 2 integration fails, verify message types and topic names"}),"\n",(0,t.jsx)(n.li,{children:"Monitor GPU memory usage during training and inference"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"further-exploration",children:"Further Exploration"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Experiment with different attention mechanisms (e.g., self-attention, co-attention)"}),"\n",(0,t.jsx)(n.li,{children:"Implement temporal attention for video sequences"}),"\n",(0,t.jsx)(n.li,{children:"Add tactile sensing modalities to the fusion system"}),"\n",(0,t.jsx)(n.li,{children:"Create hierarchical fusion architectures for multi-level perception"}),"\n"]})]})}function c(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(u,{...e})}):u(e)}}}]);