"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[5849],{5607:(n,e,s)=>{s.r(e),s.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>a,metadata:()=>r,toc:()=>c});var i=s(4848),t=s(8453);const a={},o="Lab 3.4: Isaac Sim Reinforcement Learning Systems",r={id:"modules/lab-exercises/lab-3-4-reinforcement-learning",title:"Lab 3.4: Isaac Sim Reinforcement Learning Systems",description:"Overview",source:"@site/docs/modules/lab-exercises/lab-3-4-reinforcement-learning.md",sourceDirName:"modules/lab-exercises",slug:"/modules/lab-exercises/lab-3-4-reinforcement-learning",permalink:"/ai-robotic-book/modules/lab-exercises/lab-3-4-reinforcement-learning",draft:!1,unlisted:!1,editUrl:"https://github.com/your-org/physical-ai-book/tree/main/docs/modules/lab-exercises/lab-3-4-reinforcement-learning.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Lab 3.3: Isaac Sim Planning and Control Systems",permalink:"/ai-robotic-book/modules/lab-exercises/lab-3-3-planning-control"},next:{title:"Lab 3.5: Sim-to-Real Transfer",permalink:"/ai-robotic-book/modules/lab-exercises/lab-3-5-sim2real-transfer"}},l={},c=[{value:"Overview",id:"overview",level:2},{value:"Objectives",id:"objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Duration",id:"duration",level:2},{value:"Exercise 1: Isaac Lab Setup and Environment Creation",id:"exercise-1-isaac-lab-setup-and-environment-creation",level:2},{value:"Step 1: Install Isaac Lab",id:"step-1-install-isaac-lab",level:3},{value:"Step 2: Create a basic RL environment",id:"step-2-create-a-basic-rl-environment",level:3},{value:"Step 3: Create a more complex environment for navigation",id:"step-3-create-a-more-complex-environment-for-navigation",level:3},{value:"Exercise 2: Implement PPO Algorithm",id:"exercise-2-implement-ppo-algorithm",level:2},{value:"Step 1: Create a PPO implementation",id:"step-1-create-a-ppo-implementation",level:3},{value:"Step 2: Create a complete training script",id:"step-2-create-a-complete-training-script",level:3},{value:"Exercise 3: Domain Randomization for Sim-to-Real Transfer",id:"exercise-3-domain-randomization-for-sim-to-real-transfer",level:2},{value:"Step 1: Implement domain randomization",id:"step-1-implement-domain-randomization",level:3},{value:"Exercise 4: ROS Integration for RL Policies",id:"exercise-4-ros-integration-for-rl-policies",level:2},{value:"Step 1: Create ROS bridge for RL policies",id:"step-1-create-ros-bridge-for-rl-policies",level:3},{value:"Exercise 5: Policy Evaluation and Validation",id:"exercise-5-policy-evaluation-and-validation",level:2},{value:"Step 1: Create policy evaluation tools",id:"step-1-create-policy-evaluation-tools",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues and Solutions",id:"common-issues-and-solutions",level:3},{value:"Assessment Questions",id:"assessment-questions",level:2},{value:"Extension Exercises",id:"extension-exercises",level:2},{value:"Summary",id:"summary",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.h1,{id:"lab-34-isaac-sim-reinforcement-learning-systems",children:"Lab 3.4: Isaac Sim Reinforcement Learning Systems"}),"\n",(0,i.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,i.jsx)(e.p,{children:"In this lab, you will learn how to implement reinforcement learning systems in Isaac Sim for robotics applications. You'll work with Isaac Lab's reinforcement learning framework, create environments for training, implement various RL algorithms, and integrate with ROS for real-world transfer. This includes understanding the RL environment architecture, reward shaping, and training policies for complex robotic behaviors."}),"\n",(0,i.jsx)(e.h2,{id:"objectives",children:"Objectives"}),"\n",(0,i.jsx)(e.p,{children:"By the end of this lab, you will be able to:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Set up Isaac Lab for reinforcement learning"}),"\n",(0,i.jsx)(e.li,{children:"Create custom RL environments in Isaac Sim"}),"\n",(0,i.jsx)(e.li,{children:"Implement and train various RL algorithms (PPO, SAC, etc.)"}),"\n",(0,i.jsx)(e.li,{children:"Design effective reward functions for robotic tasks"}),"\n",(0,i.jsx)(e.li,{children:"Integrate RL policies with ROS control systems"}),"\n",(0,i.jsx)(e.li,{children:"Implement domain randomization for sim-to-real transfer"}),"\n",(0,i.jsx)(e.li,{children:"Validate and evaluate trained RL policies"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Completion of Lab 3.1-3.3: Isaac Sim Setup, Perception, and Planning/Control"}),"\n",(0,i.jsx)(e.li,{children:"Understanding of reinforcement learning concepts"}),"\n",(0,i.jsx)(e.li,{children:"Experience with Isaac Sim and ROS integration"}),"\n",(0,i.jsx)(e.li,{children:"Basic knowledge of neural networks and deep learning"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"duration",children:"Duration"}),"\n",(0,i.jsx)(e.p,{children:"6-8 hours"}),"\n",(0,i.jsx)(e.h2,{id:"exercise-1-isaac-lab-setup-and-environment-creation",children:"Exercise 1: Isaac Lab Setup and Environment Creation"}),"\n",(0,i.jsx)(e.h3,{id:"step-1-install-isaac-lab",children:"Step 1: Install Isaac Lab"}),"\n",(0,i.jsx)(e.p,{children:"First, let's set up Isaac Lab:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-bash",children:"# Create Isaac Lab directory\nmkdir -p ~/isaac_lab\ncd ~/isaac_lab\n\n# Clone Isaac Lab repository\ngit clone https://github.com/isaac-sim/IsaacLab.git\ncd IsaacLab\n\n# Install Isaac Lab\n./isaaclab.sh -i\n\n# Install additional dependencies\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n"})}),"\n",(0,i.jsx)(e.h3,{id:"step-2-create-a-basic-rl-environment",children:"Step 2: Create a basic RL environment"}),"\n",(0,i.jsxs)(e.p,{children:["Create ",(0,i.jsx)(e.code,{children:"~/isaac_lab_examples/basic_rl_env.py"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n# basic_rl_env.py\n"""Basic reinforcement learning environment in Isaac Lab."""\n\nimport gymnasium as gym\nfrom gymnasium import spaces\nimport numpy as np\nimport torch\nimport carb\n\nfrom omni.isaac.orbit.assets import AssetBase\nfrom omni.isaac.orbit.envs import RLTaskEnv\nfrom omni.isaac.orbit.assets import RigidObjectCfg, AssetBaseCfg\nfrom omni.isaac.orbit.managers import SceneEntityCfg\nfrom omni.isaac.orbit.utils import configclass\n\n@configclass\nclass BasicRLEnvCfg:\n    # Environment settings\n    episode_length = 1000\n    decimation = 4  # Control decimation\n    action_scale = 1.0\n    obs_type = "proprioceptive"  # "proprioceptive" or "cameras"\n\n    # Scene settings\n    scene = SceneEntityCfg(\n        num_envs=1024,\n        env_spacing=2.5,\n    )\n\n    # Robot settings\n    robot = AssetBaseCfg(\n        prim_path="{ENV_REGEX_NS}/Robot",\n        spawn=RigidObjectCfg(\n            prim_path="{ENV_REGEX_NS}/Robot",\n            init_state=RigidObjectCfg.InitialStateCfg(\n                pos=(0.0, 0.0, 0.5),\n                rot=(1.0, 0.0, 0.0, 0.0),\n            ),\n            spawn_config=RigidObjectCfg.SpawnConfiguration(\n                mass=1.0,\n                rigid_body_enabled=True,\n                collision_mesh_visible=True,\n            ),\n        ),\n        init_state=AssetBaseCfg.InitialStateCfg(\n            pos=(0.0, 0.0, 0.5),\n            rot=(1.0, 0.0, 0.0, 0.0),\n        ),\n    )\n\nclass BasicRLEnv(RLTaskEnv):\n    """Basic reinforcement learning environment."""\n\n    def __init__(self, cfg: BasicRLEnvCfg):\n        super().__init__(cfg)\n\n        # Action space: [x_force, y_force, z_force]\n        self.action_space = spaces.Box(\n            low=-1.0,\n            high=1.0,\n            shape=(3,),\n            dtype=np.float32\n        )\n\n        # Observation space: [position, velocity, goal_position]\n        self.observation_space = spaces.Box(\n            low=-np.inf,\n            high=np.inf,\n            shape=(9,),  # 3 position + 3 velocity + 3 goal\n            dtype=np.float32\n        )\n\n        # Goal position\n        self.goal_position = np.array([5.0, 5.0, 0.5])\n\n    def _reset_idx(self, env_ids):\n        """Reset environment with given IDs."""\n        # Reset robot position to origin\n        self.robot.data.default_root_state[env_ids] = torch.zeros_like(\n            self.robot.data.default_root_state[env_ids]\n        )\n        self.robot.data.default_root_state[env_ids, :3] = torch.tensor(\n            [0.0, 0.0, 0.5], device=self.device\n        )\n\n        # Reset robot velocities\n        self.robot.data.vel_b[:, :] = 0.0\n        self.robot.data.ang_vel_b[:, :] = 0.0\n\n    def _compute_observations(self, env_ids=None):\n        """Compute observations."""\n        if env_ids is None:\n            env_ids = slice(None)\n\n        # Get current state\n        positions = self.robot.data.root_pos_w[env_ids].cpu().numpy()\n        velocities = self.robot.data.root_vel_w[env_ids].cpu().numpy()\n\n        # Create observation vector\n        obs = np.zeros((len(positions), 9))\n        obs[:, :3] = positions  # Robot position\n        obs[:, 3:6] = velocities  # Robot velocity\n        obs[:, 6:9] = self.goal_position  # Goal position\n\n        return obs\n\n    def _compute_rewards(self, env_ids=None):\n        """Compute rewards."""\n        if env_ids is None:\n            env_ids = slice(None)\n\n        # Get current positions\n        current_pos = self.robot.data.root_pos_w[env_ids, :2]\n        goal_pos = torch.tensor(self.goal_position[:2], device=self.device).unsqueeze(0).expand_as(current_pos)\n\n        # Calculate distance to goal\n        distance = torch.norm(current_pos - goal_pos, dim=1)\n\n        # Reward based on distance to goal (closer = higher reward)\n        rewards = -distance\n\n        # Bonus for reaching goal\n        goal_reached = distance < 0.5\n        rewards[goal_reached] += 100.0\n\n        return rewards\n\n    def _compute_terminals(self, env_ids=None):\n        """Compute terminals (done flags)."""\n        if env_ids is None:\n            env_ids = slice(None)\n\n        # Get current positions\n        current_pos = self.robot.data.root_pos_w[env_ids, :2]\n        goal_pos = torch.tensor(self.goal_position[:2], device=self.device).unsqueeze(0).expand_as(current_pos)\n\n        # Calculate distance to goal\n        distance = torch.norm(current_pos - goal_pos, dim=1)\n\n        # Terminate if goal is reached or episode length exceeded\n        goal_reached = distance < 0.5\n        max_episode_length = self.episode_length\n        time_out = self.episode_length_buf[env_ids] >= max_episode_length\n\n        dones = goal_reached | time_out\n        resets = goal_reached | time_out\n\n        return dones, resets\n\n    def step(self, action):\n        """Execute one step in the environment."""\n        # Apply action (forces to the robot)\n        if self.cfg.action_scale != 1.0:\n            action = action * self.cfg.action_scale\n\n        # Convert action to forces\n        forces = torch.tensor(action, device=self.device, dtype=torch.float32).unsqueeze(1).repeat(1, 3)\n\n        # Apply forces to the robot\n        self.robot.set_external_force(forces, indices=slice(None))\n\n        # Step physics\n        self.world.step(render=True)\n\n        # Compute observations, rewards, and terminals\n        obs = self._compute_observations()\n        rewards = self._compute_rewards()\n        dones, resets = self._compute_terminals()\n\n        # Update episode lengths\n        self.episode_length_buf += 1\n\n        # Reset environments that are done\n        reset_env_ids = resets.nonzero(as_tuple=False).squeeze(-1)\n        if len(reset_env_ids) > 0:\n            self._reset_idx(reset_env_ids)\n            self.episode_length_buf[reset_env_ids] = 0\n\n        return obs, rewards.cpu().numpy(), dones.cpu().numpy(), {}\n\n    def reset(self):\n        """Reset the environment."""\n        self._reset_idx(slice(None))\n        self.episode_length_buf[:] = 0\n        return self._compute_observations()\n\n# Create and test the environment\nif __name__ == "__main__":\n    import argparse\n\n    parser = argparse.ArgumentParser("Basic RL Environment")\n    parser.add_argument("--num-envs", type=int, default=1024, help="Number of environments")\n    args = parser.parse_args()\n\n    # Create environment configuration\n    cfg = BasicRLEnvCfg()\n    cfg.scene.num_envs = args.num_envs\n\n    # Create environment\n    env = BasicRLEnv(cfg)\n\n    print(f"Environment created with {args.num_envs} environments")\n    print(f"Action space: {env.action_space}")\n    print(f"Observation space: {env.observation_space}")\n\n    # Test environment\n    obs = env.reset()\n    print(f"Initial observation shape: {obs.shape}")\n\n    # Take random steps\n    for i in range(100):\n        # Random action\n        action = np.random.uniform(-1, 1, size=(args.num_envs, 3))\n\n        # Step environment\n        obs, rewards, dones, info = env.step(action)\n\n        if i % 20 == 0:\n            print(f"Step {i}: Average reward = {np.mean(rewards):.3f}")\n\n    print("Environment test completed successfully")\n'})}),"\n",(0,i.jsx)(e.h3,{id:"step-3-create-a-more-complex-environment-for-navigation",children:"Step 3: Create a more complex environment for navigation"}),"\n",(0,i.jsxs)(e.p,{children:["Create ",(0,i.jsx)(e.code,{children:"~/isaac_lab_examples/navigation_env.py"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n# navigation_env.py\n"""Navigation reinforcement learning environment."""\n\nimport gymnasium as gym\nfrom gymnasium import spaces\nimport numpy as np\nimport torch\nimport carb\n\nfrom omni.isaac.orbit.assets import RigidObjectCfg, AssetBaseCfg\nfrom omni.isaac.orbit.envs import RLTaskEnv\nfrom omni.isaac.orbit.managers import SceneEntityCfg\nfrom omni.isaac.orbit.utils import configclass\nfrom omni.isaac.orbit.assets import ArticulationCfg\n\n@configclass\nclass NavigationEnvCfg:\n    # Environment settings\n    episode_length = 500\n    decimation = 4\n    action_scale = 1.0\n    obs_type = "proprioceptive"\n\n    # Scene settings\n    scene = SceneEntityCfg(\n        num_envs=2048,\n        env_spacing=3.0,\n    )\n\n    # Robot settings (simple wheeled robot)\n    robot = ArticulationCfg(\n        prim_path="{ENV_REGEX_NS}/Robot",\n        spawn=ArticulationCfg.SPAWN_CFG(\n            asset_path="{ISAAC_ORBIT_NUCLEUS_DIR}/Robots/Ant/ant.usd",\n            activate_contact_sensors=True,\n            rigid_contact_offset=0.03,\n        ),\n        init_state=ArticulationCfg.InitialStateCfg(\n            pos=(0.0, 0.0, 0.5),\n            joint_pos={\n                ".*": 0.0,\n            },\n        ),\n    )\n\nclass NavigationEnv(RLTaskEnv):\n    """Navigation environment for reinforcement learning."""\n\n    def __init__(self, cfg: NavigationEnvCfg):\n        super().__init__(cfg)\n\n        # Action space: [left_wheel_vel, right_wheel_vel]\n        self.action_space = spaces.Box(\n            low=-1.0,\n            high=1.0,\n            shape=(2,),\n            dtype=np.float32\n        )\n\n        # Observation space: [robot_pos, robot_vel, goal_pos, obstacle_distances]\n        self.observation_space = spaces.Box(\n            low=-np.inf,\n            high=np.inf,\n            shape=(15,),  # 3 pos + 3 vel + 3 goal + 6 obstacle distances\n            dtype=np.float32\n        )\n\n        # Generate random goals for each environment\n        self.goals = torch.zeros((self.num_envs, 3), device=self.device)\n        self.generate_goals()\n\n        # Generate obstacles\n        self.obstacles = torch.zeros((10, 3), device=self.device)\n        self.generate_obstacles()\n\n    def generate_goals(self):\n        """Generate random goals."""\n        # Goals within a certain area\n        x_range = [-8.0, 8.0]\n        y_range = [-8.0, 8.0]\n\n        self.goals[:, 0] = torch.FloatTensor(self.num_envs).uniform_(x_range[0], x_range[1]).to(self.device)\n        self.goals[:, 1] = torch.FloatTensor(self.num_envs).uniform_(y_range[0], y_range[1]).to(self.device)\n        self.goals[:, 2] = 0.5  # Fixed height\n\n    def generate_obstacles(self):\n        """Generate obstacles."""\n        # Static obstacles in the environment\n        obstacle_positions = [\n            [-6, 0, 0.5], [6, 0, 0.5], [0, 6, 0.5], [0, -6, 0.5],\n            [-4, 4, 0.5], [4, 4, 0.5], [-4, -4, 0.5], [4, -4, 0.5]\n        ]\n\n        for i, pos in enumerate(obstacle_positions):\n            if i < len(self.obstacles):\n                self.obstacles[i] = torch.tensor(pos, device=self.device)\n\n    def _reset_idx(self, env_ids):\n        """Reset environment with given IDs."""\n        # Reset robot to origin\n        self.robot.data.default_root_state[env_ids, :3] = torch.tensor(\n            [0.0, 0.0, 0.5], device=self.device\n        ).unsqueeze(0).expand(len(env_ids), -1)\n\n        # Reset robot velocities\n        self.robot.data.vel_b[env_ids, :] = 0.0\n        self.robot.data.ang_vel_b[env_ids, :] = 0.0\n\n        # Reset robot joint positions and velocities\n        self.robot.data.default_joint_pos[env_ids] = self.robot.data.default_joint_pos[env_ids]\n        self.robot.data.joint_vel[env_ids] = 0.0\n\n        # Generate new goals for reset environments\n        x_range = [-8.0, 8.0]\n        y_range = [-8.0, 8.0]\n\n        self.goals[env_ids, 0] = torch.FloatTensor(len(env_ids)).uniform_(x_range[0], x_range[1]).to(self.device)\n        self.goals[env_ids, 1] = torch.FloatTensor(len(env_ids)).uniform_(y_range[0], y_range[1]).to(self.device)\n\n    def _compute_observations(self, env_ids=None):\n        """Compute observations."""\n        if env_ids is None:\n            env_ids = slice(None)\n\n        # Get current state\n        positions = self.robot.data.root_pos_w[env_ids].clone()\n        velocities = self.robot.data.root_vel_w[env_ids].clone()\n        goals = self.goals[env_ids].clone()\n\n        # Calculate obstacle distances\n        robot_pos_2d = positions[:, :2]  # Only x, y\n        obstacle_pos_2d = self.obstacles[:, :2].unsqueeze(0).expand(len(env_ids), -1, -1)\n\n        # Calculate distances to obstacles\n        dist_to_obstacles = torch.norm(\n            robot_pos_2d.unsqueeze(1) - obstacle_pos_2d,\n            dim=2\n        )  # [num_envs, num_obstacles]\n\n        # Get 6 closest obstacles (pad if needed)\n        if dist_to_obstacles.shape[1] >= 6:\n            closest_obstacles, _ = torch.topk(dist_to_obstacles, 6, dim=1, largest=False)\n        else:\n            closest_obstacles = torch.cat([\n                torch.topk(dist_to_obstacles, dist_to_obstacles.shape[1], dim=1, largest=False)[0],\n                torch.full((dist_to_obstacles.shape[0], 6 - dist_to_obstacles.shape[1]),\n                          float(\'inf\'), device=self.device)\n            ], dim=1)\n\n        # Create observation vector\n        obs = torch.cat([\n            positions,  # 3: position\n            velocities,  # 3: velocity\n            goals,  # 3: goal position\n            closest_obstacles  # 6: obstacle distances\n        ], dim=1)  # Total: 15\n\n        return obs.cpu().numpy()\n\n    def _compute_rewards(self, env_ids=None):\n        """Compute rewards."""\n        if env_ids is None:\n            env_ids = slice(None)\n\n        # Get current positions\n        current_pos = self.robot.data.root_pos_w[env_ids, :2]\n        goals_2d = self.goals[env_ids, :2]\n\n        # Calculate distance to goal\n        distance_to_goal = torch.norm(current_pos - goals_2d, dim=1)\n\n        # Calculate progress (decrease in distance compared to previous step)\n        prev_distance = getattr(self, \'_prev_distance\', distance_to_goal.clone())\n        progress = prev_distance - distance_to_goal\n        self._prev_distance = distance_to_goal.clone()\n\n        # Calculate distance to obstacles\n        robot_pos_2d = current_pos\n        obstacle_pos_2d = self.obstacles[:, :2].unsqueeze(0).expand(len(env_ids), -1, -1)\n        dist_to_obstacles = torch.norm(\n            robot_pos_2d.unsqueeze(1) - obstacle_pos_2d,\n            dim=2\n        )\n        min_obstacle_dist = torch.min(dist_to_obstacles, dim=1)[0]\n\n        # Reward components\n        goal_reward = -distance_to_goal * 0.1  # Negative distance penalty\n        progress_reward = progress * 10.0  # Positive progress reward\n        obstacle_penalty = torch.where(\n            min_obstacle_dist < 1.0,\n            -10.0 * (1.0 - min_obstacle_dist),\n            torch.zeros_like(min_obstacle_dist)\n        )  # Penalty for being too close to obstacles\n\n        # Bonus for reaching goal\n        goal_reached = distance_to_goal < 0.5\n        goal_bonus = torch.where(goal_reached, torch.ones_like(goal_reached) * 100.0, torch.zeros_like(goal_reached))\n\n        # Total reward\n        rewards = goal_reward + progress_reward + obstacle_penalty + goal_bonus\n\n        return rewards\n\n    def _compute_terminals(self, env_ids=None):\n        """Compute terminals."""\n        if env_ids is None:\n            env_ids = slice(None)\n\n        # Get current positions\n        current_pos = self.robot.data.root_pos_w[env_ids, :2]\n        goals_2d = self.goals[env_ids, :2]\n\n        # Calculate distance to goal\n        distance_to_goal = torch.norm(current_pos - goals_2d, dim=1)\n\n        # Terminate if goal is reached\n        goal_reached = distance_to_goal < 0.5\n\n        # Terminate if max episode length reached\n        max_episode_length = self.episode_length\n        time_out = self.episode_length_buf[env_ids] >= max_episode_length\n\n        # Terminate if robot goes out of bounds\n        out_of_bounds = (torch.abs(current_pos[:, 0]) > 10.0) | (torch.abs(current_pos[:, 1]) > 10.0)\n\n        dones = goal_reached | time_out | out_of_bounds\n        resets = goal_reached | time_out | out_of_bounds\n\n        return dones, resets\n\n    def step(self, action):\n        """Execute one step in the environment."""\n        # Apply action (velocity commands to joints)\n        if self.cfg.action_scale != 1.0:\n            action = action * self.cfg.action_scale\n\n        # Convert action to joint velocities\n        joint_velocities = torch.tensor(action, device=self.device, dtype=torch.float32)\n\n        # Apply actions to the robot (simplified - in reality, you\'d need to map to specific joints)\n        # For Ant robot, the action would control the legs\n        self.robot.set_joint_velocity_target(joint_velocities, joint_ids=slice(None))\n\n        # Step physics\n        self.world.step(render=True)\n\n        # Compute observations, rewards, and terminals\n        obs = self._compute_observations()\n        rewards = self._compute_rewards()\n        dones, resets = self._compute_terminals()\n\n        # Update episode lengths\n        self.episode_length_buf += 1\n\n        # Reset environments that are done\n        reset_env_ids = resets.nonzero(as_tuple=False).squeeze(-1)\n        if len(reset_env_ids) > 0:\n            self._reset_idx(reset_env_ids)\n            self.episode_length_buf[reset_env_ids] = 0\n\n        return obs, rewards.cpu().numpy(), dones.cpu().numpy(), {}\n\n# Create and test the navigation environment\nif __name__ == "__main__":\n    import argparse\n\n    parser = argparse.ArgumentParser("Navigation RL Environment")\n    parser.add_argument("--num-envs", type=int, default=2048, help="Number of environments")\n    args = parser.parse_args()\n\n    # Create environment configuration\n    cfg = NavigationEnvCfg()\n    cfg.scene.num_envs = args.num_envs\n\n    # Create environment\n    env = NavigationEnv(cfg)\n\n    print(f"Navigation environment created with {args.num_envs} environments")\n    print(f"Action space: {env.action_space}")\n    print(f"Observation space: {env.observation_space}")\n\n    # Test environment\n    obs = env.reset()\n    print(f"Initial observation shape: {obs.shape}")\n\n    # Take random steps\n    for i in range(100):\n        # Random action\n        action = np.random.uniform(-1, 1, size=(args.num_envs, 2))\n\n        # Step environment\n        obs, rewards, dones, info = env.step(action)\n\n        if i % 20 == 0:\n            print(f"Step {i}: Average reward = {np.mean(rewards):.3f}, "\n                  f"Success rate = {np.mean(dones):.3f}")\n\n    print("Navigation environment test completed successfully")\n'})}),"\n",(0,i.jsx)(e.h2,{id:"exercise-2-implement-ppo-algorithm",children:"Exercise 2: Implement PPO Algorithm"}),"\n",(0,i.jsx)(e.h3,{id:"step-1-create-a-ppo-implementation",children:"Step 1: Create a PPO implementation"}),"\n",(0,i.jsxs)(e.p,{children:["Create ",(0,i.jsx)(e.code,{children:"~/isaac_lab_examples/ppo_agent.py"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n# ppo_agent.py\n"""Proximal Policy Optimization (PPO) implementation for Isaac Lab."""\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport numpy as np\nfrom collections import deque\nimport random\n\nclass ActorCritic(nn.Module):\n    """Actor-Critic network for PPO."""\n\n    def __init__(self, state_dim, action_dim, hidden_dim=256):\n        super(ActorCritic, self).__init__()\n\n        # Shared feature extractor\n        self.feature_extractor = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU()\n        )\n\n        # Actor (policy network)\n        self.actor_mean = nn.Linear(hidden_dim, action_dim)\n        self.actor_logstd = nn.Parameter(torch.zeros(action_dim))\n\n        # Critic (value network)\n        self.critic = nn.Linear(hidden_dim, 1)\n\n    def forward(self, state):\n        """Forward pass for both actor and critic."""\n        features = self.feature_extractor(state)\n\n        # Actor: mean and std for action distribution\n        action_mean = torch.tanh(self.actor_mean(features))\n        action_std = torch.exp(self.actor_logstd)\n\n        # Critic: value estimation\n        value = self.critic(features)\n\n        return action_mean, action_std, value\n\n    def get_action(self, state):\n        """Sample action from the policy."""\n        action_mean, action_std, value = self.forward(state)\n\n        # Create distribution\n        dist = torch.distributions.Normal(action_mean, action_std)\n\n        # Sample action\n        action = dist.sample()\n        log_prob = dist.log_prob(action).sum(dim=-1)\n\n        return action, log_prob, value\n\n    def evaluate(self, state, action):\n        """Evaluate action under current policy."""\n        action_mean, action_std, value = self.forward(state)\n\n        # Create distribution\n        dist = torch.distributions.Normal(action_mean, action_std)\n\n        # Calculate log probability\n        log_prob = dist.log_prob(action).sum(dim=-1)\n\n        # Calculate entropy\n        entropy = dist.entropy().sum(dim=-1)\n\n        return log_prob, entropy, value\n\nclass PPOAgent:\n    """PPO Agent implementation."""\n\n    def __init__(self, state_dim, action_dim, lr=3e-4, gamma=0.99, eps_clip=0.2,\n                 k_epochs=4, hidden_dim=256):\n        self.gamma = gamma\n        self.eps_clip = eps_clip\n        self.k_epochs = k_epochs\n\n        # Initialize networks\n        self.policy = ActorCritic(state_dim, action_dim, hidden_dim)\n        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n\n        # Memory for storing experiences\n        self.states = []\n        self.actions = []\n        self.log_probs = []\n        self.rewards = []\n        self.is_terminals = []\n\n    def select_action(self, state):\n        """Select action using current policy."""\n        state = torch.FloatTensor(state).unsqueeze(0)\n\n        with torch.no_grad():\n            action, log_prob, _ = self.policy.get_action(state)\n\n        return action.cpu().numpy()[0], log_prob.cpu().numpy()[0]\n\n    def store_transition(self, state, action, log_prob, reward, is_terminal):\n        """Store transition in memory."""\n        self.states.append(torch.FloatTensor(state))\n        self.actions.append(torch.FloatTensor(action))\n        self.log_probs.append(torch.FloatTensor([log_prob]))\n        self.rewards.append(torch.FloatTensor([reward]))\n        self.is_terminals.append(torch.FloatTensor([is_terminal]))\n\n    def compute_returns(self):\n        """Compute discounted returns."""\n        returns = []\n        Gt = 0\n        for reward, is_terminal in zip(reversed(self.rewards), reversed(self.is_terminals)):\n            if is_terminal:\n                Gt = 0\n            Gt = reward[0] + self.gamma * Gt\n            returns.insert(0, Gt)\n\n        # Normalize returns\n        returns = torch.tensor(returns, dtype=torch.float32)\n        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n\n        return returns\n\n    def update(self):\n        """Update policy using PPO."""\n        # Convert to tensors\n        old_states = torch.stack(self.states).detach()\n        old_actions = torch.stack(self.actions).detach()\n        old_log_probs = torch.stack(self.log_probs).detach()\n\n        # Compute returns\n        returns = self.compute_returns()\n\n        # Optimize policy for K epochs\n        for _ in range(self.k_epochs):\n            # Evaluate old actions and values\n            log_probs, entropy, state_values = self.policy.evaluate(old_states, old_actions)\n\n            # Find the ratio (pi_theta / pi_theta_old)\n            ratios = torch.exp(log_probs - old_log_probs.detach())\n\n            # Compute advantage\n            advantages = returns - state_values.detach()\n\n            # Compute surrogate losses\n            surr1 = ratios * advantages\n            surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n\n            # Compute actor and critic losses\n            actor_loss = -torch.min(surr1, surr2).mean()\n            critic_loss = F.mse_loss(state_values, returns.unsqueeze(1))\n            entropy_loss = entropy.mean()\n\n            # Total loss\n            loss = actor_loss + 0.5 * critic_loss - 0.01 * entropy_loss\n\n            # Perform backward pass and optimization\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n\n        # Clear memory\n        self.states = []\n        self.actions = []\n        self.log_probs = []\n        self.rewards = []\n        self.is_terminals = []\n\n# Training function\ndef train_ppo(env, agent, num_episodes=1000):\n    """Train PPO agent."""\n    score_history = deque(maxlen=100)\n\n    for episode in range(num_episodes):\n        state = env.reset()\n        episode_reward = 0\n        episode_steps = 0\n\n        while True:\n            # Select action\n            action, log_prob = agent.select_action(state)\n\n            # Take action in environment\n            next_state, reward, done, _ = env.step(action)\n\n            # Store transition\n            agent.store_transition(state, action, log_prob, reward, done)\n\n            state = next_state\n            episode_reward += reward\n            episode_steps += 1\n\n            if done:\n                break\n\n        # Update policy after episode\n        agent.update()\n\n        # Track scores\n        score_history.append(episode_reward)\n        avg_score = np.mean(score_history)\n\n        if episode % 50 == 0:\n            print(f\'Episode {episode}, Average Score: {avg_score:.2f}, Episode Reward: {episode_reward:.2f}\')\n\n    print("Training completed!")\n\n# Example usage\nif __name__ == "__main__":\n    print("PPO Agent implementation created")\n    print("This is a template that would be used with actual RL environments")\n'})}),"\n",(0,i.jsx)(e.h3,{id:"step-2-create-a-complete-training-script",children:"Step 2: Create a complete training script"}),"\n",(0,i.jsxs)(e.p,{children:["Create ",(0,i.jsx)(e.code,{children:"~/isaac_lab_examples/train_navigation_ppo.py"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n# train_navigation_ppo.py\n"""Train navigation policy using PPO in Isaac Lab."""\n\nimport torch\nimport numpy as np\nfrom collections import deque\nimport argparse\n\n# Import your PPO agent\nfrom ppo_agent import PPOAgent\n\ndef train_navigation_ppo(env, num_episodes=2000, max_steps_per_episode=500):\n    """Train navigation policy using PPO."""\n    # Get state and action dimensions\n    state_dim = env.observation_space.shape[0]\n    action_dim = env.action_space.shape[0]\n\n    # Initialize PPO agent\n    agent = PPOAgent(\n        state_dim=state_dim,\n        action_dim=action_dim,\n        lr=3e-4,\n        gamma=0.99,\n        eps_clip=0.2,\n        k_epochs=4\n    )\n\n    # Training loop\n    score_history = deque(maxlen=100)\n    avg_score_history = []\n\n    print(f"Starting PPO training with {num_episodes} episodes...")\n\n    for episode in range(num_episodes):\n        state = env.reset()\n        episode_reward = 0\n        episode_steps = 0\n\n        for step in range(max_steps_per_episode):\n            # Select action using current policy\n            action, log_prob = agent.select_action(state)\n\n            # Take action in environment\n            next_state, reward, done, info = env.step(action)\n\n            # Store transition in agent\'s memory\n            agent.store_transition(state, action, log_prob, reward, done)\n\n            state = next_state\n            episode_reward += reward\n            episode_steps += 1\n\n            if done:\n                break\n\n        # Update policy after collecting experiences\n        agent.update()\n\n        # Track scores\n        score_history.append(episode_reward)\n        avg_score = np.mean(score_history)\n        avg_score_history.append(avg_score)\n\n        # Print progress\n        if episode % 50 == 0:\n            print(f\'Episode {episode:4d}, Average Score: {avg_score:8.2f}, \'\n                  f\'Episode Reward: {episode_reward:8.2f}, Steps: {episode_steps}\')\n\n        # Early stopping if performance plateaus\n        if episode > 100 and avg_score > 50:  # Adjust threshold as needed\n            print(f"Early stopping at episode {episode}, average score: {avg_score:.2f}")\n            break\n\n    print(f"Training completed after {episode+1} episodes")\n    print(f"Final average score: {np.mean(avg_score_history[-100:]):.2f}")\n\n    return agent, avg_score_history\n\ndef test_trained_policy(env, agent, num_episodes=10):\n    """Test the trained policy."""\n    print("\\nTesting trained policy...")\n\n    test_rewards = []\n    for episode in range(num_episodes):\n        state = env.reset()\n        episode_reward = 0\n        episode_steps = 0\n\n        for step in range(500):  # Max steps for testing\n            # Select action (without exploration)\n            with torch.no_grad():\n                action, _ = agent.select_action(state)\n\n            # Take action in environment\n            state, reward, done, info = env.step(action)\n\n            episode_reward += reward\n            episode_steps += 1\n\n            if done:\n                break\n\n        test_rewards.append(episode_reward)\n        print(f\'Test Episode {episode+1}: Reward = {episode_reward:.2f}, Steps = {episode_steps}\')\n\n    avg_test_reward = np.mean(test_rewards)\n    print(f\'Average test reward: {avg_test_reward:.2f}\')\n\n    return avg_test_reward\n\n# Main training script\nif __name__ == "__main__":\n    import sys\n    sys.path.append(\'.\')  # Add current directory to path\n\n    # Note: In a real implementation, you would import your actual navigation environment\n    # For now, this serves as a template\n\n    parser = argparse.ArgumentParser("PPO Navigation Training")\n    parser.add_argument("--num-envs", type=int, default=2048, help="Number of parallel environments")\n    parser.add_argument("--num-episodes", type=int, default=2000, help="Number of training episodes")\n    parser.add_argument("--max-steps", type=int, default=500, help="Max steps per episode")\n    args = parser.parse_args()\n\n    print("PPO Navigation Training Script")\n    print(f"Parameters: num_envs={args.num_envs}, num_episodes={args.num_episodes}")\n\n    # In a real implementation, you would:\n    # 1. Create the navigation environment\n    # 2. Train the PPO agent\n    # 3. Test the trained policy\n\n    print("\\nThis is a template. In a real implementation, you would:")\n    print("1. Create your Isaac Lab navigation environment")\n    print("2. Initialize the PPO agent with environment dimensions")\n    print("3. Train the agent using the train_navigation_ppo function")\n    print("4. Test the trained policy")\n\n    # Example of how you would use it:\n    # env = NavigationEnv(cfg)  # Your actual environment\n    # agent, scores = train_navigation_ppo(env, num_episodes=args.num_episodes)\n    # test_reward = test_trained_policy(env, agent)\n\n    print("\\nTemplate execution completed successfully")\n'})}),"\n",(0,i.jsx)(e.h2,{id:"exercise-3-domain-randomization-for-sim-to-real-transfer",children:"Exercise 3: Domain Randomization for Sim-to-Real Transfer"}),"\n",(0,i.jsx)(e.h3,{id:"step-1-implement-domain-randomization",children:"Step 1: Implement domain randomization"}),"\n",(0,i.jsxs)(e.p,{children:["Create ",(0,i.jsx)(e.code,{children:"~/isaac_lab_examples/domain_randomization.py"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\n# domain_randomization.py\n\"\"\"Domain randomization for sim-to-real transfer in Isaac Lab.\"\"\"\n\nimport torch\nimport numpy as np\nfrom omni.isaac.orbit.assets import RigidObjectCfg\nfrom omni.isaac.orbit.managers import SceneEntityCfg\nfrom omni.isaac.orbit.utils import configclass\n\n@configclass\nclass DomainRandomizationConfig:\n    \"\"\"Configuration for domain randomization.\"\"\"\n\n    # Physics randomization\n    physics_randomization = {\n        'mass_ratio_range': [0.8, 1.2],\n        'friction_range': [0.5, 1.5],\n        'restitution_range': [0.0, 0.2],\n        'dof_damping_range': [0.8, 1.2],\n        'actuator_strength_range': [0.8, 1.2],\n    }\n\n    # Visual randomization\n    visual_randomization = {\n        'lighting_intensity_range': [100, 1000],\n        'lighting_color_temperature_range': [3000, 8000],\n        'material_albedo_range': [0.1, 1.0],\n        'material_roughness_range': [0.0, 1.0],\n        'material_metallic_range': [0.0, 1.0],\n    }\n\n    # Sensor randomization\n    sensor_randomization = {\n        'noise_range': [0.0, 0.05],\n        'latency_range': [0.0, 0.02],\n        'delay_range': [0.0, 0.01],\n    }\n\n    # Randomization intervals\n    randomization_intervals = {\n        'physics': 100,  # Randomize every 100 episodes\n        'visual': 10,    # Randomize every 10 episodes\n        'sensor': 5,     # Randomize every 5 episodes\n    }\n\nclass DomainRandomizer:\n    \"\"\"Domain randomization manager.\"\"\"\n\n    def __init__(self, config: DomainRandomizationConfig):\n        self.config = config\n        self.current_physics_params = {}\n        self.current_visual_params = {}\n        self.current_sensor_params = {}\n\n        # Initialize with default parameters\n        self.randomize_physics()\n        self.randomize_visual()\n        self.randomize_sensor()\n\n    def randomize_physics(self):\n        \"\"\"Randomize physics parameters.\"\"\"\n        physics_params = self.config.physics_randomization\n\n        # Randomize mass ratios\n        self.current_physics_params['mass_ratio'] = np.random.uniform(\n            physics_params['mass_ratio_range'][0],\n            physics_params['mass_ratio_range'][1]\n        )\n\n        # Randomize friction\n        self.current_physics_params['friction'] = np.random.uniform(\n            physics_params['friction_range'][0],\n            physics_params['friction_range'][1]\n        )\n\n        # Randomize restitution\n        self.current_physics_params['restitution'] = np.random.uniform(\n            physics_params['restitution_range'][0],\n            physics_params['restitution_range'][1]\n        )\n\n        # Randomize damping\n        self.current_physics_params['damping_ratio'] = np.random.uniform(\n            physics_params['dof_damping_range'][0],\n            physics_params['dof_damping_range'][1]\n        )\n\n        # Randomize actuator strength\n        self.current_physics_params['actuator_strength'] = np.random.uniform(\n            physics_params['actuator_strength_range'][0],\n            physics_params['actuator_strength_range'][1]\n        )\n\n        print(f\"Physics randomized: mass_ratio={self.current_physics_params['mass_ratio']:.2f}, \"\n              f\"friction={self.current_physics_params['friction']:.2f}\")\n\n    def randomize_visual(self):\n        \"\"\"Randomize visual parameters.\"\"\"\n        visual_params = self.config.visual_randomization\n\n        # Randomize lighting\n        self.current_visual_params['lighting_intensity'] = np.random.uniform(\n            visual_params['lighting_intensity_range'][0],\n            visual_params['lighting_intensity_range'][1]\n        )\n\n        self.current_visual_params['lighting_color_temp'] = np.random.uniform(\n            visual_params['lighting_color_temperature_range'][0],\n            visual_params['lighting_color_temperature_range'][1]\n        )\n\n        # Randomize materials\n        self.current_visual_params['material_albedo'] = np.random.uniform(\n            visual_params['material_albedo_range'][0],\n            visual_params['material_albedo_range'][1]\n        )\n\n        self.current_visual_params['material_roughness'] = np.random.uniform(\n            visual_params['material_roughness_range'][0],\n            visual_params['material_roughness_range'][1]\n        )\n\n        self.current_visual_params['material_metallic'] = np.random.uniform(\n            visual_params['material_metallic_range'][0],\n            visual_params['material_metallic_range'][1]\n        )\n\n        print(f\"Visual randomized: intensity={self.current_visual_params['lighting_intensity']:.0f}, \"\n              f\"albedo={self.current_visual_params['material_albedo']:.2f}\")\n\n    def randomize_sensor(self):\n        \"\"\"Randomize sensor parameters.\"\"\"\n        sensor_params = self.config.sensor_randomization\n\n        # Randomize noise\n        self.current_sensor_params['noise'] = np.random.uniform(\n            sensor_params['noise_range'][0],\n            sensor_params['noise_range'][1]\n        )\n\n        # Randomize latency\n        self.current_sensor_params['latency'] = np.random.uniform(\n            sensor_params['latency_range'][0],\n            sensor_params['latency_range'][1]\n        )\n\n        # Randomize delay\n        self.current_sensor_params['delay'] = np.random.uniform(\n            sensor_params['delay_range'][0],\n            sensor_params['delay_range'][1]\n        )\n\n        print(f\"Sensor randomized: noise={self.current_sensor_params['noise']:.3f}, \"\n              f\"latency={self.current_sensor_params['latency']:.3f}s\")\n\n    def apply_randomization(self, env):\n        \"\"\"Apply current randomization parameters to environment.\"\"\"\n        # This would typically involve updating the physics engine parameters\n        # For Isaac Sim, this involves updating material properties, lighting, etc.\n\n        # Example: Update robot mass\n        if hasattr(env, 'robot') and hasattr(env.robot, 'root_physx_view'):\n            # Scale masses based on randomization\n            current_masses = env.robot.root_physx_view.get_masses()\n            scaled_masses = current_masses * self.current_physics_params['mass_ratio']\n            env.robot.root_physx_view.set_masses(scaled_masses)\n\n        # Example: Apply friction randomization\n        # This would involve updating material properties in the scene\n\n        print(\"Domain randomization applied to environment\")\n\n    def update_randomization(self, episode_count):\n        \"\"\"Update randomization based on episode count.\"\"\"\n        intervals = self.config.randomization_intervals\n\n        # Randomize physics\n        if episode_count % intervals['physics'] == 0:\n            self.randomize_physics()\n\n        # Randomize visual\n        if episode_count % intervals['visual'] == 0:\n            self.randomize_visual()\n\n        # Randomize sensor\n        if episode_count % intervals['sensor'] == 0:\n            self.randomize_sensor()\n\nclass DomainRandomizedEnv:\n    \"\"\"Environment wrapper that applies domain randomization.\"\"\"\n\n    def __init__(self, base_env, domain_randomizer):\n        self.base_env = base_env\n        self.domain_randomizer = domain_randomizer\n        self.episode_count = 0\n\n    def reset(self):\n        \"\"\"Reset environment with possible domain randomization.\"\"\"\n        # Update randomization based on episode count\n        self.domain_randomizer.update_randomization(self.episode_count)\n\n        # Apply randomization to environment\n        self.domain_randomizer.apply_randomization(self.base_env)\n\n        # Increment episode count\n        self.episode_count += 1\n\n        # Reset base environment\n        return self.base_env.reset()\n\n    def step(self, action):\n        \"\"\"Take step in environment.\"\"\"\n        # Apply sensor noise if needed\n        noisy_action = self.add_sensor_noise(action)\n\n        # Step base environment\n        obs, reward, done, info = self.base_env.step(noisy_action)\n\n        return obs, reward, done, info\n\n    def add_sensor_noise(self, action):\n        \"\"\"Add noise to actions based on randomization.\"\"\"\n        noise_level = self.domain_randomizer.current_sensor_params.get('noise', 0.0)\n        if noise_level > 0:\n            noise = np.random.normal(0, noise_level, size=action.shape)\n            return action + noise\n        return action\n\n    def __getattr__(self, name):\n        \"\"\"Delegate attribute access to base environment.\"\"\"\n        return getattr(self.base_env, name)\n\n# Example usage\nif __name__ == \"__main__\":\n    print(\"Domain Randomization System Created\")\n\n    # Example of how to use domain randomization\n    # config = DomainRandomizationConfig()\n    # randomizer = DomainRandomizer(config)\n    #\n    # # Wrap your environment with domain randomization\n    # # env = DomainRandomizedEnv(your_navigation_env, randomizer)\n    #\n    # # Training would then use the wrapped environment\n    # # This helps with sim-to-real transfer\n\n    print(\"Domain randomization system template created\")\n    print(\"This would be integrated with your actual RL training environment\")\n"})}),"\n",(0,i.jsx)(e.h2,{id:"exercise-4-ros-integration-for-rl-policies",children:"Exercise 4: ROS Integration for RL Policies"}),"\n",(0,i.jsx)(e.h3,{id:"step-1-create-ros-bridge-for-rl-policies",children:"Step 1: Create ROS bridge for RL policies"}),"\n",(0,i.jsxs)(e.p,{children:["Create ",(0,i.jsx)(e.code,{children:"~/isaac_lab_examples/rl_ros_integration.py"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\n# rl_ros_integration.py\n\"\"\"ROS integration for reinforcement learning policies.\"\"\"\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import Float32MultiArray, Bool\nfrom sensor_msgs.msg import JointState, Imu, LaserScan\nfrom geometry_msgs.msg import Twist, Pose\nfrom nav_msgs.msg import Odometry\nimport numpy as np\nimport torch\n\nclass RLPolicyNode(Node):\n    \"\"\"ROS node for reinforcement learning policy.\"\"\"\n\n    def __init__(self):\n        super().__init__('rl_policy_node')\n\n        # Declare parameters\n        self.declare_parameter('model_path', '')\n        self.declare_parameter('action_frequency', 20.0)  # Hz\n        self.declare_parameter('observation_window', 10)  # frames\n\n        # Get parameters\n        self.model_path = self.get_parameter('model_path').value\n        self.action_frequency = self.get_parameter('action_frequency').value\n        self.observation_window = self.get_parameter('observation_window').value\n\n        # Initialize RL policy\n        self.rl_policy = None\n        self.load_policy()\n\n        # Observation buffers\n        self.joint_state_buffer = []\n        self.imu_buffer = []\n        self.laser_scan_buffer = []\n        self.odometry_buffer = []\n\n        # Publishers\n        self.action_pub = self.create_publisher(Float32MultiArray, '/rl_action', 10)\n        self.status_pub = self.create_publisher(Bool, '/rl_running', 10)\n\n        # Subscribers\n        self.joint_state_sub = self.create_subscription(\n            JointState, '/joint_states', self.joint_state_callback, 10\n        )\n        self.imu_sub = self.create_subscription(\n            Imu, '/imu/data', self.imu_callback, 10\n        )\n        self.laser_scan_sub = self.create_subscription(\n            LaserScan, '/scan', self.laser_scan_callback, 10\n        )\n        self.odom_sub = self.create_subscription(\n            Odometry, '/odom', self.odom_callback, 10\n        )\n\n        # Timer for policy execution\n        self.action_timer = self.create_timer(1.0/self.action_frequency, self.execute_policy)\n\n        # Status variables\n        self.is_initialized = False\n        self.has_received_data = False\n\n        self.get_logger().info('RL Policy Node initialized')\n\n    def load_policy(self):\n        \"\"\"Load trained RL policy.\"\"\"\n        if self.model_path and self.model_path != '':\n            try:\n                # Load PyTorch model\n                self.rl_policy = torch.load(self.model_path)\n                self.is_initialized = True\n                self.get_logger().info(f'Loaded RL policy from: {self.model_path}')\n            except Exception as e:\n                self.get_logger().error(f'Failed to load RL policy: {e}')\n        else:\n            self.get_logger().warn('No model path provided, using dummy policy')\n            # Create a dummy policy for testing\n            self.rl_policy = self.create_dummy_policy()\n            self.is_initialized = True\n\n    def create_dummy_policy(self):\n        \"\"\"Create a dummy policy for testing.\"\"\"\n        class DummyPolicy:\n            def get_action(self, obs):\n                # Return random action for testing\n                return torch.randn(2)  # 2D action space\n\n        return DummyPolicy()\n\n    def joint_state_callback(self, msg):\n        \"\"\"Handle joint state messages.\"\"\"\n        self.joint_state_buffer.append({\n            'position': list(msg.position),\n            'velocity': list(msg.velocity),\n            'effort': list(msg.effort),\n            'timestamp': msg.header.stamp.sec + msg.header.stamp.nanosec * 1e-9\n        })\n\n        # Keep only recent observations\n        if len(self.joint_state_buffer) > self.observation_window:\n            self.joint_state_buffer = self.joint_state_buffer[-self.observation_window:]\n\n        self.has_received_data = True\n\n    def imu_callback(self, msg):\n        \"\"\"Handle IMU messages.\"\"\"\n        self.imu_buffer.append({\n            'orientation': [msg.orientation.x, msg.orientation.y, msg.orientation.z, msg.orientation.w],\n            'angular_velocity': [msg.angular_velocity.x, msg.angular_velocity.y, msg.angular_velocity.z],\n            'linear_acceleration': [msg.linear_acceleration.x, msg.linear_acceleration.y, msg.linear_acceleration.z],\n            'timestamp': msg.header.stamp.sec + msg.header.stamp.nanosec * 1e-9\n        })\n\n        # Keep only recent observations\n        if len(self.imu_buffer) > self.observation_window:\n            self.imu_buffer = self.imu_buffer[-self.observation_window:]\n\n    def laser_scan_callback(self, msg):\n        \"\"\"Handle laser scan messages.\"\"\"\n        self.laser_scan_buffer.append({\n            'ranges': list(msg.ranges),\n            'intensities': list(msg.intensities),\n            'angle_min': msg.angle_min,\n            'angle_max': msg.angle_max,\n            'angle_increment': msg.angle_increment,\n            'time_increment': msg.time_increment,\n            'scan_time': msg.scan_time,\n            'range_min': msg.range_min,\n            'range_max': msg.range_max,\n            'timestamp': msg.header.stamp.sec + msg.header.stamp.nanosec * 1e-9\n        })\n\n        # Keep only recent observations\n        if len(self.laser_scan_buffer) > self.observation_window:\n            self.laser_scan_buffer = self.laser_scan_buffer[-self.observation_window:]\n\n    def odom_callback(self, msg):\n        \"\"\"Handle odometry messages.\"\"\"\n        self.odometry_buffer.append({\n            'position': [msg.pose.pose.position.x, msg.pose.pose.position.y, msg.pose.pose.position.z],\n            'orientation': [msg.pose.pose.orientation.x, msg.pose.pose.orientation.y,\n                           msg.pose.pose.orientation.z, msg.pose.pose.orientation.w],\n            'linear_velocity': [msg.twist.twist.linear.x, msg.twist.twist.linear.y, msg.twist.twist.linear.z],\n            'angular_velocity': [msg.twist.twist.angular.x, msg.twist.twist.angular.y, msg.twist.twist.angular.z],\n            'timestamp': msg.header.stamp.sec + msg.header.stamp.nanosec * 1e-9\n        })\n\n        # Keep only recent observations\n        if len(self.odometry_buffer) > self.observation_window:\n            self.odometry_buffer = self.odometry_buffer[-self.observation_window:]\n\n    def construct_observation(self):\n        \"\"\"Construct observation vector from buffered sensor data.\"\"\"\n        if not self.has_received_data:\n            return None\n\n        # Get most recent data\n        joint_data = self.joint_state_buffer[-1] if self.joint_state_buffer else None\n        imu_data = self.imu_buffer[-1] if self.imu_buffer else None\n        laser_data = self.laser_scan_buffer[-1] if self.laser_scan_buffer else None\n        odom_data = self.odometry_buffer[-1] if self.odometry_buffer else None\n\n        # Construct observation vector\n        obs = []\n\n        # Add position and velocity from odometry\n        if odom_data:\n            obs.extend(odom_data['position'][:2])  # x, y position\n            obs.extend(odom_data['linear_velocity'][:2])  # x, y velocity\n            obs.extend(odom_data['angular_velocity'][:1])  # angular velocity z\n        else:\n            obs.extend([0.0, 0.0, 0.0, 0.0, 0.0])  # Default values\n\n        # Add IMU data\n        if imu_data:\n            obs.extend(imu_data['orientation'][:3])  # Only x, y, z of orientation\n            obs.extend(imu_data['angular_velocity'])\n            obs.extend(imu_data['linear_acceleration'])\n        else:\n            obs.extend([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n\n        # Add processed laser scan data (first 10 ranges for simplicity)\n        if laser_data:\n            ranges = laser_data['ranges']\n            if len(ranges) >= 10:\n                obs.extend(ranges[:10])  # First 10 range readings\n            else:\n                obs.extend(ranges + [30.0] * (10 - len(ranges)))  # Pad with max range\n        else:\n            obs.extend([30.0] * 10)  # Default max range values\n\n        # Add joint positions if available\n        if joint_data:\n            positions = joint_data['position']\n            if len(positions) >= 6:\n                obs.extend(positions[:6])  # First 6 joint positions\n            else:\n                obs.extend(positions + [0.0] * (6 - len(positions)))\n        else:\n            obs.extend([0.0] * 6)\n\n        return np.array(obs, dtype=np.float32)\n\n    def execute_policy(self):\n        \"\"\"Execute RL policy and publish action.\"\"\"\n        if not self.is_initialized:\n            return\n\n        # Construct observation\n        obs = self.construct_observation()\n        if obs is None:\n            return\n\n        try:\n            # Convert observation to tensor\n            obs_tensor = torch.FloatTensor(obs).unsqueeze(0)\n\n            # Get action from policy\n            with torch.no_grad():\n                action = self.rl_policy.get_action(obs_tensor)\n\n            # Convert action to numpy array\n            action_np = action.cpu().numpy().flatten()\n\n            # Publish action\n            action_msg = Float32MultiArray()\n            action_msg.data = action_np.tolist()\n            self.action_pub.publish(action_msg)\n\n            # Publish status\n            status_msg = Bool()\n            status_msg.data = True\n            self.status_pub.publish(status_msg)\n\n            self.get_logger().debug(f'Published action: {action_np}')\n\n        except Exception as e:\n            self.get_logger().error(f'Error executing policy: {e}')\n\n    def destroy_node(self):\n        \"\"\"Cleanup before node destruction.\"\"\"\n        super().destroy_node()\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    rl_node = RLPolicyNode()\n\n    try:\n        rclpy.spin(rl_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        rl_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(e.h2,{id:"exercise-5-policy-evaluation-and-validation",children:"Exercise 5: Policy Evaluation and Validation"}),"\n",(0,i.jsx)(e.h3,{id:"step-1-create-policy-evaluation-tools",children:"Step 1: Create policy evaluation tools"}),"\n",(0,i.jsxs)(e.p,{children:["Create ",(0,i.jsx)(e.code,{children:"~/isaac_lab_examples/policy_evaluation.py"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n# policy_evaluation.py\n"""Policy evaluation and validation tools."""\n\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nfrom collections import deque\nimport csv\nimport os\n\nclass PolicyEvaluator:\n    """Policy evaluation system."""\n\n    def __init__(self, env, policy, num_episodes=100):\n        self.env = env\n        self.policy = policy\n        self.num_episodes = num_episodes\n\n        # Metrics tracking\n        self.episode_rewards = deque(maxlen=num_episodes)\n        self.episode_lengths = deque(maxlen=num_episodes)\n        self.success_rates = deque(maxlen=num_episodes)\n        self.collision_rates = deque(maxlen=num_episodes)\n        self.energy_consumption = deque(maxlen=num_episodes)\n\n    def evaluate_policy(self):\n        """Evaluate policy performance."""\n        print(f"Evaluating policy over {self.num_episodes} episodes...")\n\n        total_successes = 0\n        total_collisions = 0\n        total_energy = 0\n\n        for episode in range(self.num_episodes):\n            state = self.env.reset()\n            episode_reward = 0\n            episode_length = 0\n            episode_energy = 0\n            has_collided = False\n            has_succeeded = False\n\n            while True:\n                # Get action from policy\n                with torch.no_grad():\n                    if isinstance(state, np.ndarray):\n                        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n                    else:\n                        state_tensor = state.unsqueeze(0)\n\n                    action = self.policy.get_action(state_tensor)\n\n                # Take action in environment\n                next_state, reward, done, info = self.env.step(action.cpu().numpy().flatten())\n\n                # Calculate energy consumption (simplified)\n                action_magnitude = np.linalg.norm(action.cpu().numpy())\n                episode_energy += action_magnitude\n\n                # Track collisions and success (implementation depends on environment)\n                if \'collision\' in info and info[\'collision\']:\n                    has_collided = True\n                    total_collisions += 1\n\n                if \'success\' in info and info[\'success\']:\n                    has_succeeded = True\n                    total_successes += 1\n                    break\n\n                state = next_state\n                episode_reward += reward\n                episode_length += 1\n\n                if done:\n                    break\n\n            # Store metrics\n            self.episode_rewards.append(episode_reward)\n            self.episode_lengths.append(episode_length)\n            self.success_rates.append(1.0 if has_succeeded else 0.0)\n            self.collision_rates.append(1.0 if has_collided else 0.0)\n            self.energy_consumption.append(episode_energy)\n\n            # Print progress\n            if episode % 20 == 0:\n                avg_reward = np.mean(list(self.episode_rewards)[-20:])\n                print(f"Episode {episode:3d}: Reward={episode_reward:8.2f}, "\n                      f"Length={episode_length:3d}, Avg Reward={avg_reward:8.2f}")\n\n        # Calculate final metrics\n        final_metrics = {\n            \'avg_reward\': np.mean(self.episode_rewards),\n            \'std_reward\': np.std(self.episode_rewards),\n            \'avg_length\': np.mean(self.episode_lengths),\n            \'success_rate\': np.mean(self.success_rates),\n            \'collision_rate\': np.mean(self.collision_rates),\n            \'avg_energy\': np.mean(self.energy_consumption),\n            \'min_reward\': np.min(self.episode_rewards),\n            \'max_reward\': np.max(self.episode_rewards)\n        }\n\n        return final_metrics\n\n    def plot_evaluation_results(self, metrics):\n        """Plot evaluation results."""\n        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 10))\n\n        # Plot reward over episodes\n        ax1.plot(list(self.episode_rewards))\n        ax1.set_title(f\'Reward per Episode\\nAvg: {metrics["avg_reward"]:.2f}\')\n        ax1.set_xlabel(\'Episode\')\n        ax1.set_ylabel(\'Reward\')\n\n        # Plot episode lengths\n        ax2.plot(list(self.episode_lengths))\n        ax2.set_title(f\'Episode Length\\nAvg: {metrics["avg_length"]:.1f}\')\n        ax2.set_xlabel(\'Episode\')\n        ax2.set_ylabel(\'Steps\')\n\n        # Plot success and collision rates over time\n        success_window = 20\n        if len(self.success_rates) >= success_window:\n            success_smooth = np.convolve(list(self.success_rates),\n                                       np.ones(success_window)/success_window, mode=\'valid\')\n            ax3.plot(success_smooth)\n        ax3.set_title(f\'Success Rate\\nOverall: {metrics["success_rate"]:.2%}\')\n        ax3.set_xlabel(\'Episode\')\n        ax3.set_ylabel(\'Success Rate\')\n\n        collision_smooth = np.convolve(list(self.collision_rates),\n                                     np.ones(success_window)/success_window, mode=\'valid\')\n        ax4.plot(collision_smooth, color=\'red\')\n        ax4.set_title(f\'Collision Rate\\nOverall: {metrics["collision_rate"]:.2%}\')\n        ax4.set_xlabel(\'Episode\')\n        ax4.set_ylabel(\'Collision Rate\')\n\n        plt.tight_layout()\n        plt.savefig(\'policy_evaluation_results.png\', dpi=300, bbox_inches=\'tight\')\n        plt.show()\n\n    def save_evaluation_results(self, metrics, filename=\'policy_evaluation.csv\'):\n        """Save evaluation results to CSV."""\n        with open(filename, \'w\', newline=\'\') as csvfile:\n            fieldnames = [\'metric\', \'value\']\n            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n            writer.writeheader()\n            for metric, value in metrics.items():\n                writer.writerow({\'metric\': metric, \'value\': value})\n\n        print(f"Evaluation results saved to {filename}")\n\nclass PolicyComparison:\n    """Compare multiple policies."""\n\n    def __init__(self):\n        self.policy_results = {}\n\n    def add_policy_results(self, policy_name, results):\n        """Add results for a policy."""\n        self.policy_results[policy_name] = results\n\n    def compare_policies(self):\n        """Compare all policies."""\n        print("\\nPolicy Comparison Results:")\n        print("=" * 60)\n\n        # Metrics to compare\n        metrics = [\'avg_reward\', \'success_rate\', \'collision_rate\', \'avg_length\']\n\n        # Print header\n        print(f"{\'Policy\':<20}", end="")\n        for metric in metrics:\n            print(f"{metric:<15}", end="")\n        print()\n        print("-" * 60)\n\n        # Print results for each policy\n        for policy_name, results in self.policy_results.items():\n            print(f"{policy_name:<20}", end="")\n            for metric in metrics:\n                if metric in results:\n                    if metric in [\'success_rate\', \'collision_rate\']:\n                        print(f"{results[metric]:<15.2%}", end="")\n                    else:\n                        print(f"{results[metric]:<15.2f}", end="")\n                else:\n                    print(f"{\'N/A\':<15}", end="")\n            print()\n\n        print("=" * 60)\n\n# Example usage\nif __name__ == "__main__":\n    print("Policy Evaluation System Created")\n\n    # Example of how to use the evaluation system\n    # evaluator = PolicyEvaluator(your_env, your_policy, num_episodes=100)\n    # metrics = evaluator.evaluate_policy()\n    # evaluator.plot_evaluation_results(metrics)\n    # evaluator.save_evaluation_results(metrics)\n\n    print("Policy evaluation system template created")\n    print("This would be used to evaluate trained RL policies")\n'})}),"\n",(0,i.jsx)(e.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,i.jsx)(e.h3,{id:"common-issues-and-solutions",children:"Common Issues and Solutions"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Training instability or divergence"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Reduce learning rate"}),"\n",(0,i.jsx)(e.li,{children:"Increase batch size"}),"\n",(0,i.jsx)(e.li,{children:"Use learning rate scheduling"}),"\n",(0,i.jsx)(e.li,{children:"Implement gradient clipping"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Slow convergence"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Check reward function design"}),"\n",(0,i.jsx)(e.li,{children:"Verify observation space normalization"}),"\n",(0,i.jsx)(e.li,{children:"Consider curriculum learning"}),"\n",(0,i.jsx)(e.li,{children:"Tune hyperparameters"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Domain randomization issues"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Start with narrow randomization ranges"}),"\n",(0,i.jsx)(e.li,{children:"Gradually increase randomization"}),"\n",(0,i.jsx)(e.li,{children:"Monitor training stability"}),"\n",(0,i.jsx)(e.li,{children:"Use validation environments"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"ROS integration problems"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Verify message type compatibility"}),"\n",(0,i.jsx)(e.li,{children:"Check network configuration"}),"\n",(0,i.jsx)(e.li,{children:"Ensure proper timing synchronization"}),"\n",(0,i.jsx)(e.li,{children:"Monitor bandwidth usage"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Memory and performance issues"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Use experience replay efficiently"}),"\n",(0,i.jsx)(e.li,{children:"Optimize neural network architecture"}),"\n",(0,i.jsx)(e.li,{children:"Consider distributed training"}),"\n",(0,i.jsx)(e.li,{children:"Profile and optimize critical paths"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"assessment-questions",children:"Assessment Questions"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"How do you design effective reward functions for continuous control tasks?"}),"\n",(0,i.jsx)(e.li,{children:"What are the key differences between PPO, SAC, and DDPG algorithms?"}),"\n",(0,i.jsx)(e.li,{children:"How does domain randomization help with sim-to-real transfer?"}),"\n",(0,i.jsx)(e.li,{children:"What metrics would you use to evaluate RL policy performance?"}),"\n",(0,i.jsx)(e.li,{children:"How do you handle sparse rewards in navigation tasks?"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"extension-exercises",children:"Extension Exercises"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"Implement curriculum learning for complex tasks"}),"\n",(0,i.jsx)(e.li,{children:"Create a hierarchical RL system for long-horizon tasks"}),"\n",(0,i.jsx)(e.li,{children:"Implement meta-learning for fast adaptation"}),"\n",(0,i.jsx)(e.li,{children:"Create a multi-agent RL environment"}),"\n",(0,i.jsx)(e.li,{children:"Implement advanced exploration strategies"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(e.p,{children:"In this lab, you successfully:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Set up Isaac Lab for reinforcement learning"}),"\n",(0,i.jsx)(e.li,{children:"Created custom RL environments for robotics tasks"}),"\n",(0,i.jsx)(e.li,{children:"Implemented PPO algorithm for navigation"}),"\n",(0,i.jsx)(e.li,{children:"Applied domain randomization for sim-to-real transfer"}),"\n",(0,i.jsx)(e.li,{children:"Integrated RL policies with ROS for real-world deployment"}),"\n",(0,i.jsx)(e.li,{children:"Evaluated and validated trained policies"}),"\n"]}),"\n",(0,i.jsx)(e.p,{children:"These skills are essential for developing autonomous robotic systems capable of learning complex behaviors through interaction with the environment. The combination of reinforcement learning, domain randomization, and ROS integration enables the creation of adaptable robotic systems that can operate effectively in real-world environments."})]})}function p(n={}){const{wrapper:e}={...(0,t.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(d,{...n})}):d(n)}},8453:(n,e,s)=>{s.d(e,{R:()=>o,x:()=>r});var i=s(6540);const t={},a=i.createContext(t);function o(n){const e=i.useContext(a);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:o(n.components),i.createElement(a.Provider,{value:e},n.children)}}}]);