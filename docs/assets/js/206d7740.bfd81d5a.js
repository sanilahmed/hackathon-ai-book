"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[9973],{174:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>r,metadata:()=>t,toc:()=>c});var s=i(4848),a=i(8453);const r={},o="Sensor Simulation in Digital Twin Environments",t={id:"modules/digital-twin/sensor-simulation",title:"Sensor Simulation in Digital Twin Environments",description:"Introduction to Sensor Simulation",source:"@site/docs/modules/digital-twin/sensor-simulation.md",sourceDirName:"modules/digital-twin",slug:"/modules/digital-twin/sensor-simulation",permalink:"/hackathon-ai-book/modules/digital-twin/sensor-simulation",draft:!1,unlisted:!1,editUrl:"https://github.com/sanilahmed/hackathon-ai-book/tree/main/docs/modules/digital-twin/sensor-simulation.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Unity Integration for Humanoid Robotics",permalink:"/hackathon-ai-book/modules/digital-twin/unity-integration"},next:{title:"ROS 2 Synchronization with Digital Twin Environments",permalink:"/hackathon-ai-book/modules/digital-twin/ros2-sync"}},l={},c=[{value:"Introduction to Sensor Simulation",id:"introduction-to-sensor-simulation",level:2},{value:"Types of Sensors in Humanoid Robotics",id:"types-of-sensors-in-humanoid-robotics",level:2},{value:"1. Range Sensors",id:"1-range-sensors",level:3},{value:"LiDAR (Light Detection and Ranging)",id:"lidar-light-detection-and-ranging",level:4},{value:"Ultrasonic Sensors",id:"ultrasonic-sensors",level:4},{value:"Infrared Sensors",id:"infrared-sensors",level:4},{value:"2. Vision Sensors",id:"2-vision-sensors",level:3},{value:"RGB Cameras",id:"rgb-cameras",level:4},{value:"Depth Cameras",id:"depth-cameras",level:4},{value:"Stereo Cameras",id:"stereo-cameras",level:4},{value:"3. Inertial Sensors",id:"3-inertial-sensors",level:3},{value:"IMU (Inertial Measurement Unit)",id:"imu-inertial-measurement-unit",level:4},{value:"Accelerometer",id:"accelerometer",level:4},{value:"Gyroscope",id:"gyroscope",level:4},{value:"4. Tactile Sensors",id:"4-tactile-sensors",level:3},{value:"Force/Torque Sensors",id:"forcetorque-sensors",level:4},{value:"Tactile Skin",id:"tactile-skin",level:4},{value:"Gazebo Sensor Simulation",id:"gazebo-sensor-simulation",level:2},{value:"LiDAR Sensor Configuration",id:"lidar-sensor-configuration",level:3},{value:"Camera Sensor Configuration",id:"camera-sensor-configuration",level:3},{value:"IMU Sensor Configuration",id:"imu-sensor-configuration",level:3},{value:"Unity Sensor Simulation",id:"unity-sensor-simulation",level:2},{value:"Synthetic Camera Setup",id:"synthetic-camera-setup",level:3},{value:"Sensor Data Processing Pipeline",id:"sensor-data-processing-pipeline",level:3},{value:"Sensor Fusion Techniques",id:"sensor-fusion-techniques",level:2},{value:"Data-Level Fusion",id:"data-level-fusion",level:3},{value:"Noise Modeling and Sensor Accuracy",id:"noise-modeling-and-sensor-accuracy",level:2},{value:"Adding Realistic Noise",id:"adding-realistic-noise",level:3},{value:"Sensor Calibration in Simulation",id:"sensor-calibration-in-simulation",level:2},{value:"Simulated Calibration Process",id:"simulated-calibration-process",level:3},{value:"Performance Evaluation Metrics",id:"performance-evaluation-metrics",level:2},{value:"Sensor Simulation Quality Assessment",id:"sensor-simulation-quality-assessment",level:3},{value:"Best Practices for Sensor Simulation",id:"best-practices-for-sensor-simulation",level:2},{value:"1. Realistic Noise Modeling",id:"1-realistic-noise-modeling",level:3},{value:"2. Proper Coordinate Systems",id:"2-proper-coordinate-systems",level:3},{value:"3. Computational Efficiency",id:"3-computational-efficiency",level:3},{value:"4. Validation and Verification",id:"4-validation-and-verification",level:3},{value:"Troubleshooting Sensor Simulation",id:"troubleshooting-sensor-simulation",level:2},{value:"Common Issues and Solutions",id:"common-issues-and-solutions",level:3},{value:"Summary",id:"summary",level:2},{value:"Learning Check",id:"learning-check",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.h1,{id:"sensor-simulation-in-digital-twin-environments",children:"Sensor Simulation in Digital Twin Environments"}),"\n",(0,s.jsx)(e.h2,{id:"introduction-to-sensor-simulation",children:"Introduction to Sensor Simulation"}),"\n",(0,s.jsx)(e.p,{children:"Sensor simulation is a critical component of digital twin environments, enabling robots to perceive and interact with virtual worlds in ways that closely mirror real-world sensor capabilities. In humanoid robotics, accurate sensor simulation allows for development and testing of perception algorithms, navigation systems, and human-robot interaction scenarios without the risks and costs associated with physical hardware."}),"\n",(0,s.jsx)(e.h2,{id:"types-of-sensors-in-humanoid-robotics",children:"Types of Sensors in Humanoid Robotics"}),"\n",(0,s.jsx)(e.h3,{id:"1-range-sensors",children:"1. Range Sensors"}),"\n",(0,s.jsx)(e.p,{children:"Range sensors provide distance measurements to objects in the environment:"}),"\n",(0,s.jsx)(e.h4,{id:"lidar-light-detection-and-ranging",children:"LiDAR (Light Detection and Ranging)"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Purpose"}),": 360-degree environmental mapping and obstacle detection"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simulation"}),": Ray tracing algorithms that calculate distances to surfaces"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Parameters"}),": Range resolution, field of view, update rate, noise models"]}),"\n"]}),"\n",(0,s.jsx)(e.h4,{id:"ultrasonic-sensors",children:"Ultrasonic Sensors"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Purpose"}),": Short-range obstacle detection"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simulation"}),": Cone-shaped detection areas with distance measurements"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Parameters"}),": Detection cone angle, maximum range, update frequency"]}),"\n"]}),"\n",(0,s.jsx)(e.h4,{id:"infrared-sensors",children:"Infrared Sensors"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Purpose"}),": Proximity detection and surface analysis"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simulation"}),": Infrared ray casting with material properties"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Parameters"}),": Wavelength, detection range, surface reflectivity"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"2-vision-sensors",children:"2. Vision Sensors"}),"\n",(0,s.jsx)(e.p,{children:"Vision sensors provide image-based perception capabilities:"}),"\n",(0,s.jsx)(e.h4,{id:"rgb-cameras",children:"RGB Cameras"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Purpose"}),": Visual scene capture and image processing"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simulation"}),": Photorealistic rendering with configurable parameters"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Parameters"}),": Resolution, field of view, focal length, frame rate"]}),"\n"]}),"\n",(0,s.jsx)(e.h4,{id:"depth-cameras",children:"Depth Cameras"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Purpose"}),": 3D scene reconstruction and spatial awareness"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simulation"}),": Depth map generation from stereo vision or structured light"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Parameters"}),": Depth range, resolution, accuracy, noise models"]}),"\n"]}),"\n",(0,s.jsx)(e.h4,{id:"stereo-cameras",children:"Stereo Cameras"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Purpose"}),": 3D reconstruction and depth estimation"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simulation"}),": Two synchronized RGB cameras with baseline distance"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Parameters"}),": Baseline, resolution, stereo matching algorithms"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"3-inertial-sensors",children:"3. Inertial Sensors"}),"\n",(0,s.jsx)(e.p,{children:"Inertial sensors provide information about the robot's motion and orientation:"}),"\n",(0,s.jsx)(e.h4,{id:"imu-inertial-measurement-unit",children:"IMU (Inertial Measurement Unit)"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Purpose"}),": Orientation, acceleration, and angular velocity measurement"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simulation"}),": Integration of virtual accelerometers and gyroscopes"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Parameters"}),": Noise characteristics, drift models, update rates"]}),"\n"]}),"\n",(0,s.jsx)(e.h4,{id:"accelerometer",children:"Accelerometer"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Purpose"}),": Linear acceleration measurement"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simulation"}),": Force-based acceleration detection"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Parameters"}),": Range, sensitivity, noise floor"]}),"\n"]}),"\n",(0,s.jsx)(e.h4,{id:"gyroscope",children:"Gyroscope"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Purpose"}),": Angular velocity measurement"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simulation"}),": Virtual rotation sensing"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Parameters"}),": Range, resolution, drift characteristics"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"4-tactile-sensors",children:"4. Tactile Sensors"}),"\n",(0,s.jsx)(e.p,{children:"Tactile sensors provide contact and force information:"}),"\n",(0,s.jsx)(e.h4,{id:"forcetorque-sensors",children:"Force/Torque Sensors"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Purpose"}),": Measurement of forces and torques at joints or end-effectors"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simulation"}),": Virtual force transducers"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Parameters"}),": Measurement range, sensitivity, update rate"]}),"\n"]}),"\n",(0,s.jsx)(e.h4,{id:"tactile-skin",children:"Tactile Skin"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Purpose"}),": Distributed contact sensing across robot surfaces"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simulation"}),": Grid of contact sensors"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Parameters"}),": Sensitivity, spatial resolution, contact detection"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"gazebo-sensor-simulation",children:"Gazebo Sensor Simulation"}),"\n",(0,s.jsx)(e.h3,{id:"lidar-sensor-configuration",children:"LiDAR Sensor Configuration"}),"\n",(0,s.jsx)(e.p,{children:"In Gazebo, LiDAR sensors are configured using SDF/XML with specific parameters:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-xml",children:'<sensor name="lidar_2d" type="ray">\n  <always_on>true</always_on>\n  <update_rate>10</update_rate>\n  <ray>\n    <scan>\n      <horizontal>\n        <samples>720</samples>\n        <resolution>1</resolution>\n        <min_angle>-3.14159</min_angle>\n        <max_angle>3.14159</max_angle>\n      </horizontal>\n    </scan>\n    <range>\n      <min>0.1</min>\n      <max>30.0</max>\n      <resolution>0.01</resolution>\n    </range>\n  </ray>\n  <plugin name="lidar_2d_controller" filename="libgazebo_ros_ray_sensor.so">\n    <ros>\n      <namespace>/humanoid</namespace>\n      <remapping>~/out:=scan</remapping>\n    </ros>\n    <output_type>sensor_msgs/LaserScan</output_type>\n    <frame_name>lidar_2d_frame</frame_name>\n  </plugin>\n</sensor>\n'})}),"\n",(0,s.jsx)(e.h3,{id:"camera-sensor-configuration",children:"Camera Sensor Configuration"}),"\n",(0,s.jsx)(e.p,{children:"RGB camera sensors in Gazebo:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-xml",children:'<sensor name="camera" type="camera">\n  <always_on>true</always_on>\n  <update_rate>30</update_rate>\n  <camera name="head">\n    <horizontal_fov>1.3962634</horizontal_fov>\n    <image>\n      <width>640</width>\n      <height>480</height>\n      <format>R8G8B8</format>\n    </image>\n    <clip>\n      <near>0.1</near>\n      <far>100</far>\n    </clip>\n    <noise>\n      <type>gaussian</type>\n      <mean>0.0</mean>\n      <stddev>0.007</stddev>\n    </noise>\n  </camera>\n  <plugin name="camera_controller" filename="libgazebo_ros_camera.so">\n    <ros>\n      <namespace>/humanoid</namespace>\n      <remapping>image_raw:=camera/image_raw</remapping>\n      <remapping>camera_info:=camera/camera_info</remapping>\n    </ros>\n    <frame_name>camera_frame</frame_name>\n  </plugin>\n</sensor>\n'})}),"\n",(0,s.jsx)(e.h3,{id:"imu-sensor-configuration",children:"IMU Sensor Configuration"}),"\n",(0,s.jsx)(e.p,{children:"IMU sensors in Gazebo:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-xml",children:'<sensor name="imu_sensor" type="imu">\n  <always_on>true</always_on>\n  <update_rate>100</update_rate>\n  <imu>\n    <angular_velocity>\n      <x>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>0.001</stddev>\n        </noise>\n      </x>\n      <y>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>0.001</stddev>\n        </noise>\n      </y>\n      <z>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>0.001</stddev>\n        </noise>\n      </z>\n    </angular_velocity>\n    <linear_acceleration>\n      <x>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>1.7e-2</stddev>\n        </noise>\n      </x>\n      <y>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>1.7e-2</stddev>\n        </noise>\n      </y>\n      <z>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>1.7e-2</stddev>\n        </noise>\n      </z>\n    </linear_acceleration>\n  </imu>\n  <plugin name="imu_controller" filename="libgazebo_ros_imu_sensor.so">\n    <ros>\n      <namespace>/humanoid</namespace>\n      <remapping>~/out:=imu</remapping>\n    </ros>\n    <frame_name>imu_frame</frame_name>\n  </plugin>\n</sensor>\n'})}),"\n",(0,s.jsx)(e.h2,{id:"unity-sensor-simulation",children:"Unity Sensor Simulation"}),"\n",(0,s.jsx)(e.h3,{id:"synthetic-camera-setup",children:"Synthetic Camera Setup"}),"\n",(0,s.jsx)(e.p,{children:"In Unity, synthetic cameras can generate realistic sensor data:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-csharp",children:"using UnityEngine;\nusing Unity.Perception.GroundTruth;\n\npublic class SyntheticCameraSetup : MonoBehaviour\n{\n    public void ConfigureSyntheticCamera()\n    {\n        var camera = GetComponent<Camera>();\n\n        // Add synthetic camera component\n        var syntheticCamera = camera.gameObject.AddComponent<SyntheticCamera>();\n\n        // Configure sensor properties\n        syntheticCamera.captureRgb = true;\n        syntheticCamera.captureDepth = true;\n        syntheticCamera.captureSegmentation = true;\n        syntheticCamera.captureOpticalFlow = true;\n\n        // Set capture frequency\n        syntheticCamera.captureFrequency = 30; // Hz\n\n        // Configure camera intrinsics\n        camera.fieldOfView = 60f; // degrees\n        camera.aspect = 16f / 9f; // aspect ratio\n    }\n}\n"})}),"\n",(0,s.jsx)(e.h3,{id:"sensor-data-processing-pipeline",children:"Sensor Data Processing Pipeline"}),"\n",(0,s.jsx)(e.p,{children:"Processing sensor data from simulation environments:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nSensor Data Processing Pipeline for Humanoid Robotics\n"""\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan, Image, Imu, PointCloud2\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport cv2\n\nclass SensorDataProcessor(Node):\n    def __init__(self):\n        super().__init__(\'sensor_data_processor\')\n\n        # Initialize CvBridge for image processing\n        self.bridge = CvBridge()\n\n        # Create subscribers for different sensor types\n        self.lidar_sub = self.create_subscription(\n            LaserScan, \'/humanoid/scan\', self.lidar_callback, 10\n        )\n        self.camera_sub = self.create_subscription(\n            Image, \'/humanoid/camera/image_raw\', self.camera_callback, 10\n        )\n        self.imu_sub = self.create_subscription(\n            Imu, \'/humanoid/imu\', self.imu_callback, 10\n        )\n\n        # Publishers for processed data\n        self.obstacle_pub = self.create_publisher(\n            PointCloud2, \'/humanoid/obstacles\', 10\n        )\n\n    def lidar_callback(self, msg):\n        """Process LiDAR data for obstacle detection"""\n        # Convert to numpy array\n        ranges = np.array(msg.ranges)\n\n        # Filter invalid ranges\n        valid_ranges = ranges[np.isfinite(ranges)]\n\n        # Detect obstacles (distance threshold)\n        obstacle_threshold = 1.0  # meters\n        obstacle_indices = np.where(ranges < obstacle_threshold)[0]\n\n        if len(obstacle_indices) > 0:\n            self.get_logger().info(f\'Detected {len(obstacle_indices)} obstacles\')\n\n            # Calculate obstacle positions in 2D\n            angles = np.array([msg.angle_min + i * msg.angle_increment\n                              for i in obstacle_indices])\n            distances = ranges[obstacle_indices]\n\n            obstacle_x = distances * np.cos(angles)\n            obstacle_y = distances * np.sin(angles)\n\n            # Publish obstacle information\n            self.publish_obstacles(obstacle_x, obstacle_y)\n\n    def camera_callback(self, msg):\n        """Process camera data for visual perception"""\n        try:\n            # Convert ROS Image to OpenCV\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'bgr8\')\n\n            # Example: Object detection using color thresholding\n            hsv = cv2.cvtColor(cv_image, cv2.COLOR_BGR2HSV)\n\n            # Define range for red color (in HSV)\n            lower_red = np.array([0, 50, 50])\n            upper_red = np.array([10, 255, 255])\n\n            # Create mask\n            mask = cv2.inRange(hsv, lower_red, upper_red)\n\n            # Find contours\n            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n            # Process detected objects\n            for contour in contours:\n                area = cv2.contourArea(contour)\n                if area > 1000:  # Filter small areas\n                    # Calculate centroid\n                    M = cv2.moments(contour)\n                    if M["m00"] != 0:\n                        cx = int(M["m10"] / M["m00"])\n                        cy = int(M["m01"] / M["m00"])\n\n                        self.get_logger().info(f\'Object detected at ({cx}, {cy})\')\n\n        except Exception as e:\n            self.get_logger().error(f\'Error processing camera image: {e}\')\n\n    def imu_callback(self, msg):\n        """Process IMU data for orientation and motion"""\n        # Extract orientation (quaternion)\n        orientation = msg.orientation\n        # Extract angular velocity\n        angular_velocity = msg.angular_velocity\n        # Extract linear acceleration\n        linear_acceleration = msg.linear_acceleration\n\n        # Convert quaternion to Euler angles (example)\n        euler = self.quaternion_to_euler(\n            orientation.x, orientation.y, orientation.z, orientation.w\n        )\n\n        self.get_logger().info(f\'Orientation: {euler}\')\n\n    def quaternion_to_euler(self, x, y, z, w):\n        """Convert quaternion to Euler angles (roll, pitch, yaw)"""\n        import math\n\n        # Roll (x-axis rotation)\n        sinr_cosp = 2 * (w * x + y * z)\n        cosr_cosp = 1 - 2 * (x * x + y * y)\n        roll = math.atan2(sinr_cosp, cosr_cosp)\n\n        # Pitch (y-axis rotation)\n        sinp = 2 * (w * y - z * x)\n        if abs(sinp) >= 1:\n            pitch = math.copysign(math.pi / 2, sinp)\n        else:\n            pitch = math.asin(sinp)\n\n        # Yaw (z-axis rotation)\n        siny_cosp = 2 * (w * z + x * y)\n        cosy_cosp = 1 - 2 * (y * y + z * z)\n        yaw = math.atan2(siny_cosp, cosy_cosp)\n\n        return (roll, pitch, yaw)\n\n    def publish_obstacles(self, x_coords, y_coords):\n        """Publish detected obstacles as PointCloud2 message"""\n        # Implementation for publishing obstacle points\n        pass\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    processor = SensorDataProcessor()\n\n    try:\n        rclpy.spin(processor)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        processor.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(e.h2,{id:"sensor-fusion-techniques",children:"Sensor Fusion Techniques"}),"\n",(0,s.jsx)(e.h3,{id:"data-level-fusion",children:"Data-Level Fusion"}),"\n",(0,s.jsx)(e.p,{children:"Combining raw sensor data from multiple sources:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'import numpy as np\n\nclass SensorFusion:\n    def __init__(self):\n        self.lidar_data = None\n        self.camera_data = None\n        self.imu_data = None\n\n    def fuse_lidar_camera(self, lidar_scan, camera_image):\n        """\n        Fuse LiDAR and camera data for enhanced perception\n        """\n        # Project LiDAR points to camera image coordinates\n        # This requires camera intrinsics and extrinsics\n        camera_intrinsics = self.get_camera_intrinsics()\n        lidar_to_camera_extrinsics = self.get_extrinsics()\n\n        # Transform LiDAR points to camera frame\n        lidar_points_3d = self.lidar_scan_to_3d_points(lidar_scan)\n        camera_frame_points = self.transform_points(\n            lidar_points_3d, lidar_to_camera_extrinsics\n        )\n\n        # Project 3D points to 2D image coordinates\n        image_coords = self.project_3d_to_2d(\n            camera_frame_points, camera_intrinsics\n        )\n\n        # Combine with visual information\n        fused_data = self.combine_data(\n            camera_image, image_coords, lidar_scan\n        )\n\n        return fused_data\n\n    def kalman_filter_fusion(self, measurements):\n        """\n        Use Kalman filter for temporal sensor fusion\n        """\n        # Initialize state vector [x, y, z, vx, vy, vz]\n        state = np.zeros(6)\n\n        # Initialize covariance matrix\n        P = np.eye(6) * 1000\n\n        # Process measurements from different sensors\n        for measurement in measurements:\n            # Prediction step\n            state, P = self.predict(state, P)\n\n            # Update step with measurement\n            state, P = self.update(state, P, measurement)\n\n        return state\n\n    def particle_filter_fusion(self, sensor_observations):\n        """\n        Use particle filter for non-linear sensor fusion\n        """\n        # Initialize particles\n        particles = self.initialize_particles()\n\n        # Weight particles based on sensor observations\n        weights = self.calculate_weights(particles, sensor_observations)\n\n        # Resample particles based on weights\n        particles = self.resample(particles, weights)\n\n        # Return estimated state\n        return self.estimate_state(particles)\n'})}),"\n",(0,s.jsx)(e.h2,{id:"noise-modeling-and-sensor-accuracy",children:"Noise Modeling and Sensor Accuracy"}),"\n",(0,s.jsx)(e.h3,{id:"adding-realistic-noise",children:"Adding Realistic Noise"}),"\n",(0,s.jsx)(e.p,{children:"Real sensors have inherent noise and inaccuracies that should be simulated:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'import numpy as np\n\nclass SensorNoiseModel:\n    def __init__(self):\n        # LiDAR noise parameters\n        self.lidar_noise_std = 0.01  # 1cm standard deviation\n        self.lidar_bias = 0.005     # 5mm bias\n\n        # Camera noise parameters\n        self.camera_noise_std = 0.5  # pixels\n        self.camera_bias = 0.1       # pixels\n\n        # IMU noise parameters\n        self.imu_acc_noise_std = 0.017  # m/s\xb2\n        self.imu_gyro_noise_std = 0.001  # rad/s\n\n    def add_lidar_noise(self, ranges):\n        """Add realistic noise to LiDAR measurements"""\n        noise = np.random.normal(\n            self.lidar_bias,\n            self.lidar_noise_std,\n            size=ranges.shape\n        )\n        noisy_ranges = ranges + noise\n\n        # Ensure no negative distances\n        noisy_ranges = np.maximum(noisy_ranges, 0.0)\n\n        return noisy_ranges\n\n    def add_camera_noise(self, image):\n        """Add realistic noise to camera images"""\n        # Add Gaussian noise\n        noise = np.random.normal(0, self.camera_noise_std, image.shape)\n        noisy_image = image + noise\n\n        # Clip values to valid range\n        noisy_image = np.clip(noisy_image, 0, 255)\n\n        return noisy_image.astype(np.uint8)\n\n    def add_imu_noise(self, linear_acc, angular_vel):\n        """Add realistic noise to IMU measurements"""\n        noisy_acc = linear_acc + np.random.normal(\n            0, self.imu_acc_noise_std, size=linear_acc.shape\n        )\n        noisy_vel = angular_vel + np.random.normal(\n            0, self.imu_gyro_noise_std, size=angular_vel.shape\n        )\n\n        return noisy_acc, noisy_vel\n'})}),"\n",(0,s.jsx)(e.h2,{id:"sensor-calibration-in-simulation",children:"Sensor Calibration in Simulation"}),"\n",(0,s.jsx)(e.h3,{id:"simulated-calibration-process",children:"Simulated Calibration Process"}),"\n",(0,s.jsx)(e.p,{children:"Even in simulation, sensors need calibration parameters:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'class SensorCalibration:\n    def __init__(self):\n        # Camera intrinsic parameters\n        self.camera_matrix = np.array([\n            [500, 0, 320],  # fx, 0, cx\n            [0, 500, 240],  # 0, fy, cy\n            [0, 0, 1]       # 0, 0, 1\n        ])\n\n        # Camera distortion coefficients\n        self.dist_coeffs = np.array([0.1, -0.2, 0, 0, 0.05])\n\n        # LiDAR extrinsic parameters (position/orientation relative to robot)\n        self.lidar_extrinsics = np.eye(4)  # 4x4 transformation matrix\n\n    def calibrate_camera(self, calibration_images):\n        """Calibrate camera using checkerboard pattern"""\n        # Prepare object points (3D points in real world space)\n        objp = np.zeros((6*9, 3), np.float32)\n        objp[:, :2] = np.mgrid[0:9, 0:6].T.reshape(-1, 2)\n\n        # Arrays to store object points and image points\n        objpoints = []  # 3D points in real world space\n        imgpoints = []  # 2D points in image plane\n\n        for img in calibration_images:\n            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n            # Find chessboard corners\n            ret, corners = cv2.findChessboardCorners(gray, (9, 6), None)\n\n            if ret:\n                objpoints.append(objp)\n                imgpoints.append(corners)\n\n        if len(objpoints) > 0:\n            # Perform calibration\n            ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(\n                objpoints, imgpoints, gray.shape[::-1], None, None\n            )\n\n            if ret:\n                self.camera_matrix = mtx\n                self.dist_coeffs = dist\n                return True\n\n        return False\n\n    def calibrate_lidar_camera(self, lidar_points, image_points):\n        """Calibrate LiDAR to camera extrinsics"""\n        # Use point-to-plane correspondences to find transformation\n        transformation = self.find_transformation(lidar_points, image_points)\n        self.lidar_extrinsics = transformation\n        return transformation\n'})}),"\n",(0,s.jsx)(e.h2,{id:"performance-evaluation-metrics",children:"Performance Evaluation Metrics"}),"\n",(0,s.jsx)(e.h3,{id:"sensor-simulation-quality-assessment",children:"Sensor Simulation Quality Assessment"}),"\n",(0,s.jsx)(e.p,{children:"Evaluate the quality of sensor simulation:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"class SensorEvaluation:\n    def __init__(self):\n        self.metrics = {}\n\n    def evaluate_lidar_accuracy(self, simulated_scan, real_scan):\n        \"\"\"Compare simulated and real LiDAR data\"\"\"\n        # Calculate RMSE between scans\n        rmse = np.sqrt(np.mean((simulated_scan.ranges - real_scan.ranges) ** 2))\n\n        # Calculate correlation\n        correlation = np.corrcoef(\n            simulated_scan.ranges, real_scan.ranges\n        )[0, 1]\n\n        # Calculate ICP alignment error\n        alignment_error = self.calculate_icp_error(simulated_scan, real_scan)\n\n        self.metrics['lidar_rmse'] = rmse\n        self.metrics['lidar_correlation'] = correlation\n        self.metrics['lidar_alignment_error'] = alignment_error\n\n        return {\n            'rmse': rmse,\n            'correlation': correlation,\n            'alignment_error': alignment_error\n        }\n\n    def evaluate_camera_accuracy(self, simulated_image, real_image):\n        \"\"\"Compare simulated and real camera images\"\"\"\n        # Calculate structural similarity (SSIM)\n        ssim = self.calculate_ssim(simulated_image, real_image)\n\n        # Calculate mean squared error\n        mse = np.mean((simulated_image - real_image) ** 2)\n\n        # Calculate peak signal-to-noise ratio\n        psnr = self.calculate_psnr(simulated_image, real_image)\n\n        self.metrics['camera_ssim'] = ssim\n        self.metrics['camera_mse'] = mse\n        self.metrics['camera_psnr'] = psnr\n\n        return {\n            'ssim': ssim,\n            'mse': mse,\n            'psnr': psnr\n        }\n\n    def evaluate_imu_accuracy(self, simulated_imu, real_imu):\n        \"\"\"Compare simulated and real IMU data\"\"\"\n        # Calculate orientation error\n        orientation_error = self.quaternion_difference(\n            simulated_imu.orientation, real_imu.orientation\n        )\n\n        # Calculate acceleration error\n        acc_error = np.linalg.norm(\n            np.array([simulated_imu.linear_acceleration.x,\n                     simulated_imu.linear_acceleration.y,\n                     simulated_imu.linear_acceleration.z]) -\n            np.array([real_imu.linear_acceleration.x,\n                     real_imu.linear_acceleration.y,\n                     real_imu.linear_acceleration.z])\n        )\n\n        return {\n            'orientation_error': orientation_error,\n            'acceleration_error': acc_error\n        }\n"})}),"\n",(0,s.jsx)(e.h2,{id:"best-practices-for-sensor-simulation",children:"Best Practices for Sensor Simulation"}),"\n",(0,s.jsx)(e.h3,{id:"1-realistic-noise-modeling",children:"1. Realistic Noise Modeling"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Include appropriate noise models for each sensor type"}),"\n",(0,s.jsx)(e.li,{children:"Consider environmental factors (temperature, humidity, lighting)"}),"\n",(0,s.jsx)(e.li,{children:"Validate noise parameters against real sensor specifications"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"2-proper-coordinate-systems",children:"2. Proper Coordinate Systems"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Maintain consistent coordinate system definitions"}),"\n",(0,s.jsx)(e.li,{children:"Clearly document frame relationships using TF"}),"\n",(0,s.jsx)(e.li,{children:"Use standard conventions (e.g., REP-103 for coordinate frames)"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"3-computational-efficiency",children:"3. Computational Efficiency"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Balance simulation accuracy with computational cost"}),"\n",(0,s.jsx)(e.li,{children:"Use appropriate update rates for different sensor types"}),"\n",(0,s.jsx)(e.li,{children:"Implement sensor data filtering and decimation when needed"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"4-validation-and-verification",children:"4. Validation and Verification"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Compare simulated data with real sensor data"}),"\n",(0,s.jsx)(e.li,{children:"Validate sensor fusion algorithms in simulation"}),"\n",(0,s.jsx)(e.li,{children:"Test edge cases and failure scenarios"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"troubleshooting-sensor-simulation",children:"Troubleshooting Sensor Simulation"}),"\n",(0,s.jsx)(e.h3,{id:"common-issues-and-solutions",children:"Common Issues and Solutions"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Sensor Data Not Publishing"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Check plugin configuration in SDF/URDF files"}),"\n",(0,s.jsx)(e.li,{children:"Verify ROS topic names and permissions"}),"\n",(0,s.jsx)(e.li,{children:"Confirm sensor is properly attached to robot links"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Incorrect Sensor Data"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Verify sensor parameters (range, resolution, noise)"}),"\n",(0,s.jsx)(e.li,{children:"Check coordinate frame transformations"}),"\n",(0,s.jsx)(e.li,{children:"Validate sensor mounting position and orientation"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Performance Issues"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Reduce sensor update rates temporarily"}),"\n",(0,s.jsx)(e.li,{children:"Lower resolution parameters"}),"\n",(0,s.jsx)(e.li,{children:"Use simplified meshes for collision detection"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(e.p,{children:"Sensor simulation in digital twin environments provides realistic perception capabilities for humanoid robots, enabling comprehensive testing and development of perception algorithms. Proper configuration of sensors in both Gazebo and Unity environments, combined with realistic noise modeling and calibration procedures, creates effective simulation environments that bridge the gap between virtual and real-world robotics. The integration of multiple sensor types through fusion techniques enhances the robot's ability to perceive and interact with its environment."}),"\n",(0,s.jsx)(e.h2,{id:"learning-check",children:"Learning Check"}),"\n",(0,s.jsx)(e.p,{children:"After studying this section, you should be able to:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Configure various sensor types in simulation environments"}),"\n",(0,s.jsx)(e.li,{children:"Implement sensor data processing pipelines"}),"\n",(0,s.jsx)(e.li,{children:"Apply sensor fusion techniques for enhanced perception"}),"\n",(0,s.jsx)(e.li,{children:"Evaluate sensor simulation quality using appropriate metrics"}),"\n",(0,s.jsx)(e.li,{children:"Troubleshoot common sensor simulation issues"}),"\n"]})]})}function m(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>o,x:()=>t});var s=i(6540);const a={},r=s.createContext(a);function o(n){const e=s.useContext(r);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function t(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:o(n.components),s.createElement(r.Provider,{value:e},n.children)}}}]);