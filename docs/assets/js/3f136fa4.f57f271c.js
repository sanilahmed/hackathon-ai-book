"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[5031],{8453:(n,e,t)=>{t.d(e,{R:()=>s,x:()=>r});var a=t(6540);const i={},o=a.createContext(i);function s(n){const e=a.useContext(o);return a.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:s(n.components),a.createElement(o.Provider,{value:e},n.children)}},9192:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>c,contentTitle:()=>s,default:()=>u,frontMatter:()=>o,metadata:()=>r,toc:()=>l});var a=t(4848),i=t(8453);const o={},s="Language-Action Mapping",r={id:"modules/vla-system/language-action-mapping",title:"Language-Action Mapping",description:"Overview",source:"@site/docs/modules/vla-system/language-action-mapping.md",sourceDirName:"modules/vla-system",slug:"/modules/vla-system/language-action-mapping",permalink:"/hackathon-ai-book/modules/vla-system/language-action-mapping",draft:!1,unlisted:!1,editUrl:"https://github.com/sanilahmed/hackathon-ai-book/tree/main/docs/modules/vla-system/language-action-mapping.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Multimodal Perception",permalink:"/hackathon-ai-book/modules/vla-system/multimodal-perception"},next:{title:"Training VLA Models",permalink:"/hackathon-ai-book/modules/vla-system/training-vla-models"}},c={},l=[{value:"Overview",id:"overview",level:2},{value:"Architecture of Language-Action Mapping",id:"architecture-of-language-action-mapping",level:2},{value:"High-Level Architecture",id:"high-level-architecture",level:3},{value:"Language Parser Component",id:"language-parser-component",level:3},{value:"Semantic Interpreter",id:"semantic-interpreter",level:3},{value:"Action Space Representation",id:"action-space-representation",level:2},{value:"Continuous Action Space",id:"continuous-action-space",level:3},{value:"Discrete Action Space",id:"discrete-action-space",level:3},{value:"Hierarchical Action Planning",id:"hierarchical-action-planning",level:2},{value:"Task Decomposition",id:"task-decomposition",level:3},{value:"Language-to-Action Transformers",id:"language-to-action-transformers",level:2},{value:"End-to-End Transformer Model",id:"end-to-end-transformer-model",level:3},{value:"Vision-Guided Language-Action Mapping",id:"vision-guided-language-action-mapping",level:2},{value:"Grounded Language Understanding",id:"grounded-language-understanding",level:3},{value:"Safety and Validation",id:"safety-and-validation",level:2},{value:"Safe Action Generation",id:"safe-action-generation",level:3},{value:"Training Language-Action Models",id:"training-language-action-models",level:2},{value:"Imitation Learning Approach",id:"imitation-learning-approach",level:3},{value:"Evaluation Metrics",id:"evaluation-metrics",level:2},{value:"Language-Action Mapping Evaluation",id:"language-action-mapping-evaluation",level:3},{value:"Integration with ROS 2",id:"integration-with-ros-2",level:2},{value:"ROS 2 Action Server for Language Commands",id:"ros-2-action-server-for-language-commands",level:3},{value:"Troubleshooting Common Issues",id:"troubleshooting-common-issues",level:2},{value:"1. Ambiguous Instructions",id:"1-ambiguous-instructions",level:3},{value:"2. Grounding Failures",id:"2-grounding-failures",level:3},{value:"3. Action Space Mismatch",id:"3-action-space-mismatch",level:3}];function d(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",hr:"hr",li:"li",p:"p",pre:"pre",ul:"ul",...(0,i.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.h1,{id:"language-action-mapping",children:"Language-Action Mapping"}),"\n",(0,a.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,a.jsx)(e.p,{children:"Language-action mapping is the critical component of Vision-Language-Action (VLA) systems that translates natural language instructions into executable robotic actions. This section covers techniques for parsing language, understanding semantic meaning, and generating appropriate motor commands for humanoid robots."}),"\n",(0,a.jsx)(e.h2,{id:"architecture-of-language-action-mapping",children:"Architecture of Language-Action Mapping"}),"\n",(0,a.jsx)(e.h3,{id:"high-level-architecture",children:"High-Level Architecture"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{children:"Natural Language Input\n        \u2193\n[Language Parser] \u2192 [Semantic Interpreter] \u2192 [Action Generator] \u2192 [Robot Actions]\n        \u2193                   \u2193                      \u2193\n  Syntactic Tree      Semantic Graph        Action Sequence    Motor Commands\n  Dependency Parse    Concept Grounding     Temporal Planning   Joint Positions\n"})}),"\n",(0,a.jsx)(e.h3,{id:"language-parser-component",children:"Language Parser Component"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"import spacy\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoTokenizer, AutoModel\n\nclass LanguageParser(nn.Module):\n    def __init__(self, model_name='bert-base-uncased'):\n        super().__init__()\n        # Load pre-trained language model\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.language_model = AutoModel.from_pretrained(model_name)\n\n        # Part-of-speech and dependency parsing\n        self.nlp = spacy.load('en_core_web_sm')\n\n        # Intent classification head\n        self.intent_classifier = nn.Linear(\n            self.language_model.config.hidden_size, 128\n        )  # 128 different intents\n\n    def forward(self, text):\n        # Tokenize and encode text\n        inputs = self.tokenizer(\n            text, return_tensors='pt', padding=True, truncation=True\n        )\n\n        # Get language embeddings\n        outputs = self.language_model(**inputs)\n        embeddings = outputs.last_hidden_state.mean(dim=1)  # Global average pooling\n\n        # Classify intent\n        intent_logits = self.intent_classifier(embeddings)\n\n        return {\n            'embeddings': embeddings,\n            'intent_logits': intent_logits,\n            'intent_probs': torch.softmax(intent_logits, dim=-1)\n        }\n\n    def parse_dependencies(self, text):\n        \"\"\"Parse syntactic dependencies using spaCy.\"\"\"\n        doc = self.nlp(text)\n\n        dependencies = []\n        for token in doc:\n            dependencies.append({\n                'text': token.text,\n                'lemma': token.lemma_,\n                'pos': token.pos_,\n                'dep': token.dep_,\n                'head': token.head.text if token.head != token else None\n            })\n\n        return dependencies\n\n    def extract_entities(self, text):\n        \"\"\"Extract named entities from text.\"\"\"\n        doc = self.nlp(text)\n\n        entities = []\n        for ent in doc.ents:\n            entities.append({\n                'text': ent.text,\n                'label': ent.label_,\n                'start': ent.start_char,\n                'end': ent.end_char\n            })\n\n        return entities\n"})}),"\n",(0,a.jsx)(e.h3,{id:"semantic-interpreter",children:"Semantic Interpreter"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"class SemanticInterpreter(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Action type classification\n        self.action_classifier = nn.Linear(512, 64)  # 64 different action types\n\n        # Object recognition in language\n        self.object_classifier = nn.Linear(512, 128)  # 128 object categories\n\n        # Spatial relation understanding\n        self.spatial_classifier = nn.Linear(512, 32)   # 32 spatial relations\n\n        # Attribute recognition\n        self.attribute_classifier = nn.Linear(512, 64) # 64 attributes\n\n    def forward(self, language_features, vision_features=None):\n        \"\"\"Interpret semantic meaning from language (with optional vision).\"\"\"\n        # If vision features provided, combine with language\n        if vision_features is not None:\n            combined_features = torch.cat([language_features, vision_features], dim=-1)\n        else:\n            combined_features = language_features\n\n        # Classify different semantic components\n        action_type = torch.softmax(\n            self.action_classifier(combined_features), dim=-1\n        )\n        object_type = torch.softmax(\n            self.object_classifier(combined_features), dim=-1\n        )\n        spatial_rel = torch.softmax(\n            self.spatial_classifier(combined_features), dim=-1\n        )\n        attributes = torch.softmax(\n            self.attribute_classifier(combined_features), dim=-1\n        )\n\n        return {\n            'action_type': action_type,\n            'object_type': object_type,\n            'spatial_relation': spatial_rel,\n            'attributes': attributes\n        }\n\n    def interpret_instruction(self, instruction, vision_features=None):\n        \"\"\"Interpret complete instruction.\"\"\"\n        # Use language parser to get embeddings\n        parser_output = self.parse_instruction(instruction)\n\n        # Interpret semantics\n        semantic_output = self.forward(\n            parser_output['embeddings'],\n            vision_features\n        )\n\n        return {\n            'instruction': instruction,\n            'intent': parser_output['intent_probs'],\n            'semantic_interpretation': semantic_output,\n            'action_candidates': self.generate_action_candidates(semantic_output)\n        }\n\n    def generate_action_candidates(self, semantic_output):\n        \"\"\"Generate potential actions based on semantic interpretation.\"\"\"\n        # Map semantic concepts to action space\n        action_candidates = []\n\n        # Example mapping logic\n        action_type_idx = torch.argmax(semantic_output['action_type'], dim=-1)\n        object_type_idx = torch.argmax(semantic_output['object_type'], dim=-1)\n\n        # Generate action based on action type and object\n        action = self.map_semantic_to_action(\n            action_type_idx.item(),\n            object_type_idx.item(),\n            semantic_output['spatial_relation'],\n            semantic_output['attributes']\n        )\n\n        action_candidates.append(action)\n\n        return action_candidates\n\n    def map_semantic_to_action(self, action_type, object_type, spatial_rel, attributes):\n        \"\"\"Map semantic interpretation to specific action.\"\"\"\n        # This is a simplified example - in practice, this would be more complex\n        action_mapping = {\n            (0, 1): 'pick_up_object',  # action_type=0 (grasp), object_type=1 (cup)\n            (1, 2): 'place_object',   # action_type=1 (place), object_type=2 (box)\n            (2, 3): 'move_to_object', # action_type=2 (move), object_type=3 (table)\n        }\n\n        # Look up action based on semantic types\n        action_key = (action_type, object_type)\n        action = action_mapping.get(action_key, 'default_action')\n\n        return {\n            'action_type': action,\n            'target_object': object_type,\n            'spatial_constraints': spatial_rel,\n            'object_attributes': attributes\n        }\n"})}),"\n",(0,a.jsx)(e.h2,{id:"action-space-representation",children:"Action Space Representation"}),"\n",(0,a.jsx)(e.h3,{id:"continuous-action-space",children:"Continuous Action Space"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'import numpy as np\n\nclass ContinuousActionSpace:\n    def __init__(self, robot_config):\n        self.joint_limits = robot_config[\'joint_limits\']\n        self.action_dim = len(robot_config[\'joint_names\'])\n        self.spatial_dim = 3  # x, y, z position\n        self.orientation_dim = 4  # quaternion\n        self.gripper_dim = 1    # gripper position\n\n    def decode_language_to_continuous(self, semantic_interpretation):\n        """Decode language instruction to continuous action space."""\n        # Extract semantic components\n        action_type = torch.argmax(semantic_interpretation[\'action_type\'], dim=-1).item()\n        target_object = torch.argmax(semantic_interpretation[\'object_type\'], dim=-1).item()\n        spatial_constraints = semantic_interpretation[\'spatial_relation\']\n\n        # Map to continuous space based on action type\n        if action_type == 0:  # Movement action\n            return self.decode_movement_action(spatial_constraints)\n        elif action_type == 1:  # Manipulation action\n            return self.decode_manipulation_action(target_object, spatial_constraints)\n        else:  # Default action\n            return self.decode_default_action()\n\n    def decode_movement_action(self, spatial_constraints):\n        """Decode movement-related language to continuous movement."""\n        # Example: "Go to the left of the table"\n        target_pos = np.zeros(3)  # x, y, z\n\n        # Interpret spatial constraints\n        spatial_idx = torch.argmax(spatial_constraints, dim=-1).item()\n        if spatial_idx == 0:  # left_of\n            target_pos[0] -= 0.5  # Move left by 0.5m\n        elif spatial_idx == 1:  # right_of\n            target_pos[0] += 0.5  # Move right by 0.5m\n        elif spatial_idx == 2:  # in_front_of\n            target_pos[1] += 0.5  # Move forward by 0.5m\n        elif spatial_idx == 3:  # behind\n            target_pos[1] -= 0.5  # Move backward by 0.5m\n\n        # Create continuous action vector\n        action = np.zeros(self.action_dim + self.spatial_dim + self.orientation_dim + self.gripper_dim)\n        action[self.action_dim:self.action_dim + self.spatial_dim] = target_pos\n\n        return action\n\n    def decode_manipulation_action(self, target_object, spatial_constraints):\n        """Decode manipulation-related language to joint positions."""\n        # Calculate target pose for manipulation\n        target_pose = self.calculate_manipulation_pose(target_object, spatial_constraints)\n\n        # Convert to joint space (using inverse kinematics)\n        joint_positions = self.inverse_kinematics(target_pose)\n\n        # Create action vector\n        action = np.zeros(self.action_dim + self.spatial_dim + self.orientation_dim + self.gripper_dim)\n        action[:self.action_dim] = joint_positions\n\n        return action\n\n    def calculate_manipulation_pose(self, target_object, spatial_constraints):\n        """Calculate target pose for manipulation based on object and constraints."""\n        # In practice, this would use object detection and spatial reasoning\n        # For now, return a placeholder\n        return {\n            \'position\': np.array([0.5, 0.0, 0.8]),  # Default position\n            \'orientation\': np.array([0, 0, 0, 1])   # Default orientation (w, x, y, z)\n        }\n\n    def inverse_kinematics(self, target_pose):\n        """Calculate joint positions for target pose."""\n        # Placeholder - in practice, use robot-specific IK solver\n        return np.zeros(self.action_dim)\n'})}),"\n",(0,a.jsx)(e.h3,{id:"discrete-action-space",children:"Discrete Action Space"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"class DiscreteActionSpace:\n    def __init__(self):\n        # Define discrete action vocabulary\n        self.action_vocabulary = [\n            'move_forward',\n            'move_backward',\n            'turn_left',\n            'turn_right',\n            'move_up',\n            'move_down',\n            'grasp',\n            'release',\n            'open_gripper',\n            'close_gripper',\n            'pick_object',\n            'place_object',\n            'push_object',\n            'pull_object',\n            'point_at_object',\n            'wave',\n            'nod',\n            'shake_head'\n        ]\n\n        self.action_to_id = {action: i for i, action in enumerate(self.action_vocabulary)}\n        self.id_to_action = {i: action for i, action in enumerate(self.action_vocabulary)}\n\n    def map_language_to_discrete_action(self, instruction):\n        \"\"\"Map natural language instruction to discrete action.\"\"\"\n        # Simple keyword-based mapping (in practice, use more sophisticated NLP)\n        instruction_lower = instruction.lower()\n\n        # Extract action keywords\n        for action in self.action_vocabulary:\n            if action.replace('_', ' ') in instruction_lower:\n                return self.action_to_id[action]\n\n        # If no direct match, use semantic similarity\n        return self.find_semantically_similar_action(instruction)\n\n    def find_semantically_similar_action(self, instruction):\n        \"\"\"Find most semantically similar action using embeddings.\"\"\"\n        # Use sentence transformers or similar approach\n        from sentence_transformers import SentenceTransformer\n\n        model = SentenceTransformer('all-MiniLM-L6-v2')\n        instruction_embedding = model.encode([instruction])\n        action_embeddings = model.encode(self.action_vocabulary)\n\n        # Calculate similarities\n        similarities = np.dot(instruction_embedding, action_embeddings.T)[0]\n        most_similar_idx = np.argmax(similarities)\n\n        return most_similar_idx\n"})}),"\n",(0,a.jsx)(e.h2,{id:"hierarchical-action-planning",children:"Hierarchical Action Planning"}),"\n",(0,a.jsx)(e.h3,{id:"task-decomposition",children:"Task Decomposition"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"class HierarchicalActionPlanner:\n    def __init__(self):\n        self.action_parser = LanguageParser()\n        self.semantic_interpreter = SemanticInterpreter()\n        self.continuous_action_space = ContinuousActionSpace({\n            'joint_names': ['joint_1', 'joint_2', 'joint_3', 'joint_4', 'joint_5', 'joint_6'],\n            'joint_limits': [(-3.14, 3.14)] * 6\n        })\n\n    def decompose_task(self, instruction):\n        \"\"\"Decompose high-level instruction into subtasks.\"\"\"\n        # Parse the instruction\n        parsed = self.action_parser(instruction)\n\n        # Interpret semantics\n        semantic = self.semantic_interpreter(\n            parsed['embeddings']\n        )\n\n        # Decompose into subtasks based on semantic interpretation\n        subtasks = self.generate_subtasks(instruction, semantic)\n\n        return {\n            'original_instruction': instruction,\n            'semantic_interpretation': semantic,\n            'subtasks': subtasks,\n            'execution_plan': self.create_execution_plan(subtasks)\n        }\n\n    def generate_subtasks(self, instruction, semantic_interpretation):\n        \"\"\"Generate subtasks for complex instructions.\"\"\"\n        subtasks = []\n\n        # Example: \"Pick up the red cup and place it on the table\"\n        if 'pick' in instruction.lower() and 'place' in instruction.lower():\n            # Decompose into pick and place\n            subtasks.extend([\n                {\n                    'type': 'pick_object',\n                    'object': self.extract_object(instruction, 'pick'),\n                    'attributes': self.extract_attributes(instruction, 'red')\n                },\n                {\n                    'type': 'navigate',\n                    'target': self.extract_target(instruction, 'table')\n                },\n                {\n                    'type': 'place_object',\n                    'target': self.extract_target(instruction, 'table'),\n                    'object': self.extract_object(instruction, 'pick')\n                }\n            ])\n        else:\n            # Single task\n            subtasks.append({\n                'type': self.infer_task_type(instruction, semantic_interpretation),\n                'object': self.extract_object(instruction),\n                'target': self.extract_target(instruction),\n                'attributes': self.extract_attributes(instruction)\n            })\n\n        return subtasks\n\n    def extract_object(self, instruction, context=None):\n        \"\"\"Extract object from instruction.\"\"\"\n        # Use NER to extract objects\n        entities = self.action_parser.extract_entities(instruction)\n        objects = [ent for ent in entities if ent['label'] in ['OBJECT', 'PRODUCT']]\n        return objects[0] if objects else None\n\n    def extract_target(self, instruction):\n        \"\"\"Extract target location from instruction.\"\"\"\n        entities = self.action_parser.extract_entities(instruction)\n        locations = [ent for ent in entities if ent['label'] in ['LOC', 'FACILITY']]\n        return locations[0] if locations else None\n\n    def extract_attributes(self, instruction, target_attribute=None):\n        \"\"\"Extract attributes like color, size, etc.\"\"\"\n        # Extract adjectives that describe objects\n        dependencies = self.action_parser.parse_dependencies(instruction)\n        attributes = [dep for dep in dependencies if dep['pos'] == 'ADJ']\n        return attributes\n\n    def create_execution_plan(self, subtasks):\n        \"\"\"Create executable plan from subtasks.\"\"\"\n        plan = []\n        for i, subtask in enumerate(subtasks):\n            action = self.map_subtask_to_action(subtask)\n            plan.append({\n                'step': i,\n                'subtask': subtask,\n                'action': action,\n                'preconditions': self.get_preconditions(subtask),\n                'postconditions': self.get_postconditions(subtask)\n            })\n\n        return plan\n\n    def map_subtask_to_action(self, subtask):\n        \"\"\"Map subtask to executable action.\"\"\"\n        # Map subtask type to action\n        action_mapping = {\n            'pick_object': self.create_pick_action,\n            'place_object': self.create_place_action,\n            'navigate': self.create_navigation_action,\n            'grasp': self.create_grasp_action,\n            'release': self.create_release_action\n        }\n\n        action_func = action_mapping.get(subtask['type'], self.create_default_action)\n        return action_func(subtask)\n\n    def create_pick_action(self, subtask):\n        \"\"\"Create pick action.\"\"\"\n        return {\n            'type': 'manipulation',\n            'action': 'pick',\n            'target_object': subtask.get('object'),\n            'approach_vector': [0, 0, -1],  # Approach from above\n            'gripper_position': 0.0  # Open gripper\n        }\n\n    def create_place_action(self, subtask):\n        \"\"\"Create place action.\"\"\"\n        return {\n            'type': 'manipulation',\n            'action': 'place',\n            'target_location': subtask.get('target'),\n            'release_height': 0.1,  # Release 10cm above surface\n            'gripper_position': 1.0  # Close gripper\n        }\n\n    def create_navigation_action(self, subtask):\n        \"\"\"Create navigation action.\"\"\"\n        return {\n            'type': 'navigation',\n            'action': 'navigate',\n            'target_pose': self.calculate_target_pose(subtask),\n            'path_planning': True\n        }\n"})}),"\n",(0,a.jsx)(e.h2,{id:"language-to-action-transformers",children:"Language-to-Action Transformers"}),"\n",(0,a.jsx)(e.h3,{id:"end-to-end-transformer-model",children:"End-to-End Transformer Model"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'import torch.nn.functional as F\n\nclass LanguageToActionTransformer(nn.Module):\n    def __init__(self, vocab_size=30522, action_dim=12, d_model=512, nhead=8, num_layers=6):\n        super().__init__()\n        self.d_model = d_model\n        self.action_dim = action_dim\n\n        # Embedding layers\n        self.token_embedding = nn.Embedding(vocab_size, d_model)\n        self.position_embedding = nn.Embedding(512, d_model)  # Max sequence length\n\n        # Transformer encoder for language processing\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=nhead,\n            dim_feedforward=d_model * 4,\n            batch_first=True\n        )\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n\n        # Action decoder\n        self.action_decoder = nn.Sequential(\n            nn.Linear(d_model, d_model * 2),\n            nn.ReLU(),\n            nn.Linear(d_model * 2, d_model),\n            nn.ReLU(),\n            nn.Linear(d_model, action_dim)\n        )\n\n        # Optional: Vision integration\n        self.vision_projection = nn.Linear(512, d_model)  # Vision feature dimension\n        self.vision_transformer = nn.TransformerEncoder(encoder_layer, 2)\n\n    def forward(self, input_ids, attention_mask=None, vision_features=None):\n        """Forward pass from language tokens to actions."""\n        batch_size, seq_len = input_ids.shape\n\n        # Token embeddings\n        token_embeds = self.token_embedding(input_ids)\n\n        # Position embeddings\n        positions = torch.arange(seq_len, device=input_ids.device).unsqueeze(0)\n        pos_embeds = self.position_embedding(positions)\n\n        # Combined embeddings\n        x = token_embeds + pos_embeds\n\n        # Apply attention mask\n        if attention_mask is not None:\n            # Convert attention mask for transformer (0 for padded, -inf for attention)\n            extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n            extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n\n        # Language transformer\n        lang_features = self.transformer_encoder(x)\n\n        # Global representation (mean pooling)\n        if attention_mask is not None:\n            # Masked mean pooling\n            masked_lang_features = lang_features * attention_mask.unsqueeze(-1)\n            global_features = masked_lang_features.sum(dim=1) / attention_mask.sum(dim=1, keepdim=True)\n        else:\n            global_features = lang_features.mean(dim=1)\n\n        # Optional: Integrate vision features\n        if vision_features is not None:\n            vision_embeds = self.vision_projection(vision_features)\n            # Concatenate or add vision features\n            combined_features = global_features + vision_embeds\n        else:\n            combined_features = global_features\n\n        # Decode to actions\n        actions = self.action_decoder(combined_features)\n\n        return actions\n\n    def generate_action_from_text(self, text, tokenizer, vision_features=None):\n        """Generate action from text using the model."""\n        # Tokenize input\n        inputs = tokenizer(text, return_tensors=\'pt\', padding=True, truncation=True)\n\n        # Forward pass\n        with torch.no_grad():\n            actions = self.forward(\n                inputs[\'input_ids\'],\n                inputs[\'attention_mask\'],\n                vision_features\n            )\n\n        return actions\n'})}),"\n",(0,a.jsx)(e.h2,{id:"vision-guided-language-action-mapping",children:"Vision-Guided Language-Action Mapping"}),"\n",(0,a.jsx)(e.h3,{id:"grounded-language-understanding",children:"Grounded Language Understanding"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class GroundedLanguageActionMapper:\n    def __init__(self):\n        self.clip_model = CLIPBasedPerception()  # From multimodal perception\n        self.language_parser = LanguageParser()\n        self.action_generator = ContinuousActionSpace({\n            \'joint_names\': [\'joint_1\', \'joint_2\', \'joint_3\', \'joint_4\', \'joint_5\', \'joint_6\'],\n            \'joint_limits\': [(-3.14, 3.14)] * 6\n        })\n\n    def grounded_action_mapping(self, image, instruction):\n        """Map language instruction to action with visual grounding."""\n        # Get CLIP features\n        image_features, text_features = self.clip_model.encode_image_text_pair(\n            image, instruction\n        )\n\n        # Parse language\n        language_output = self.language_parser(instruction)\n\n        # Combine vision and language features\n        combined_features = torch.cat([image_features, text_features], dim=-1)\n\n        # Generate grounded action\n        action = self.generate_grounded_action(\n            combined_features,\n            language_output,\n            image_features\n        )\n\n        return action\n\n    def generate_grounded_action(self, combined_features, language_output, image_features):\n        """Generate action based on grounded understanding."""\n        # Use combined features to generate action\n        # This could be a neural network or rule-based system\n        action_type = torch.argmax(language_output[\'intent_probs\'], dim=-1)\n\n        # Generate action based on action type and visual context\n        if action_type == 0:  # Navigation\n            return self.generate_navigation_action(combined_features, image_features)\n        elif action_type == 1:  # Manipulation\n            return self.generate_manipulation_action(combined_features, image_features)\n        else:  # Default action\n            return self.generate_default_action()\n\n    def generate_navigation_action(self, combined_features, image_features):\n        """Generate navigation action based on visual scene."""\n        # Analyze scene to find navigable locations\n        # Use image features to identify free space, obstacles, targets\n        target_direction = self.identify_navigation_target(image_features)\n        return self.create_navigation_action(target_direction)\n\n    def generate_manipulation_action(self, combined_features, image_features):\n        """Generate manipulation action based on visual objects."""\n        # Identify manipulable objects in scene\n        target_object = self.identify_target_object(image_features)\n        return self.create_manipulation_action(target_object)\n\n    def identify_navigation_target(self, image_features):\n        """Identify navigation target from image features."""\n        # In practice, this would use object detection and spatial reasoning\n        # For now, return a placeholder\n        return [0.5, 0.0, 0.0]  # Move forward\n\n    def identify_target_object(self, image_features):\n        """Identify target object for manipulation."""\n        # Use object detection or segmentation\n        # For now, return a placeholder\n        return {\'position\': [0.5, 0.0, 0.8], \'type\': \'unknown_object\'}\n'})}),"\n",(0,a.jsx)(e.h2,{id:"safety-and-validation",children:"Safety and Validation"}),"\n",(0,a.jsx)(e.h3,{id:"safe-action-generation",children:"Safe Action Generation"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class SafeLanguageActionMapper:\n    def __init__(self, base_mapper):\n        self.base_mapper = base_mapper\n        self.safety_checker = SafetyChecker()\n\n    def safe_map_language_to_action(self, image, instruction):\n        """Map language to action with safety validation."""\n        # Generate action using base mapper\n        raw_action = self.base_mapper.grounded_action_mapping(image, instruction)\n\n        # Validate action safety\n        if self.safety_checker.is_safe(raw_action, image, instruction):\n            return raw_action\n        else:\n            # Generate safe fallback action\n            safe_action = self.safety_checker.generate_safe_fallback(\n                raw_action, image, instruction\n            )\n            return safe_action\n\nclass SafetyChecker:\n    def __init__(self):\n        # Safety constraint models\n        self.collision_detector = CollisionDetector()\n        self.stability_checker = StabilityChecker()\n        self.ethics_checker = EthicsChecker()\n\n    def is_safe(self, action, image, instruction):\n        """Check if action is safe to execute."""\n        checks = [\n            self.check_collision(action),\n            self.check_stability(action),\n            self.check_ethics(instruction)\n        ]\n        return all(checks)\n\n    def check_collision(self, action):\n        """Check if action would cause collision."""\n        # Use robot model and environment to check collision\n        return self.collision_detector.would_collide(action)\n\n    def check_stability(self, action):\n        """Check if action maintains robot stability."""\n        return self.stability_checker.maintains_stability(action)\n\n    def check_ethics(self, instruction):\n        """Check if instruction is ethical to follow."""\n        # Use ethical AI models to check instruction\n        return self.ethics_checker.is_ethical(instruction)\n\n    def generate_safe_fallback(self, unsafe_action, image, instruction):\n        """Generate safe fallback action."""\n        # Return neutral or safe default action\n        return np.zeros_like(unsafe_action)  # Zero action (no movement)\n'})}),"\n",(0,a.jsx)(e.h2,{id:"training-language-action-models",children:"Training Language-Action Models"}),"\n",(0,a.jsx)(e.h3,{id:"imitation-learning-approach",children:"Imitation Learning Approach"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class LanguageActionTrainer:\n    def __init__(self, model, learning_rate=1e-4):\n        self.model = model\n        self.optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n        self.criterion = nn.MSELoss()\n\n    def train_step(self, language_input, vision_input, expert_actions):\n        """Single training step."""\n        # Forward pass\n        predicted_actions = self.model(\n            language_input[\'input_ids\'],\n            language_input[\'attention_mask\'],\n            vision_input\n        )\n\n        # Compute loss\n        loss = self.criterion(predicted_actions, expert_actions)\n\n        # Backward pass\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n\n        return loss.item()\n\n    def train_epoch(self, dataloader):\n        """Train for one epoch."""\n        total_loss = 0\n        num_batches = 0\n\n        for batch in dataloader:\n            language_input, vision_input, expert_actions = batch\n\n            loss = self.train_step(language_input, vision_input, expert_actions)\n            total_loss += loss\n            num_batches += 1\n\n        avg_loss = total_loss / num_batches\n        return avg_loss\n\n    def collect_demonstration_data(self, robot, environment, instructions):\n        """Collect demonstration data for training."""\n        demonstrations = []\n\n        for instruction in instructions:\n            # Reset environment\n            state = environment.reset()\n\n            # Execute instruction with human demonstration\n            expert_trajectory = self.execute_with_human_demo(\n                robot, environment, instruction\n            )\n\n            # Store demonstration\n            demonstrations.append({\n                \'instruction\': instruction,\n                \'states\': expert_trajectory[\'states\'],\n                \'actions\': expert_trajectory[\'actions\'],\n                \'language_features\': self.encode_language(instruction)\n            })\n\n        return demonstrations\n'})}),"\n",(0,a.jsx)(e.h2,{id:"evaluation-metrics",children:"Evaluation Metrics"}),"\n",(0,a.jsx)(e.h3,{id:"language-action-mapping-evaluation",children:"Language-Action Mapping Evaluation"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"def evaluate_language_action_mapping(model, test_dataset):\n    \"\"\"Evaluate language-action mapping performance.\"\"\"\n    metrics = {\n        'action_accuracy': [],\n        'semantic_alignment': [],\n        'task_success_rate': [],\n        'execution_time': [],\n        'safety_violations': []\n    }\n\n    for sample in test_dataset:\n        image, instruction, expected_action = sample\n\n        # Generate action\n        start_time = time.time()\n        generated_action = model(image, instruction)\n        execution_time = time.time() - start_time\n\n        # Evaluate action accuracy\n        action_acc = calculate_action_accuracy(generated_action, expected_action)\n        metrics['action_accuracy'].append(action_acc)\n\n        # Evaluate semantic alignment\n        semantic_align = evaluate_semantic_alignment(instruction, generated_action)\n        metrics['semantic_alignment'].append(semantic_align)\n\n        # Evaluate task success (in simulation/real robot)\n        task_success = execute_and_evaluate_task(model, image, instruction)\n        metrics['task_success_rate'].append(task_success)\n\n        # Record execution time\n        metrics['execution_time'].append(execution_time)\n\n        # Check for safety violations\n        safety_violations = check_safety_violations(generated_action)\n        metrics['safety_violations'].append(safety_violations)\n\n    # Calculate average metrics\n    avg_metrics = {k: sum(v)/len(v) for k, v in metrics.items()}\n    return avg_metrics\n\ndef calculate_action_accuracy(predicted_action, expected_action):\n    \"\"\"Calculate accuracy of predicted action.\"\"\"\n    # Use appropriate distance metric based on action space\n    if isinstance(predicted_action, torch.Tensor):\n        predicted_action = predicted_action.cpu().numpy()\n    if isinstance(expected_action, torch.Tensor):\n        expected_action = expected_action.cpu().numpy()\n\n    # Calculate normalized distance\n    distance = np.linalg.norm(predicted_action - expected_action)\n    max_distance = np.linalg.norm(expected_action) + 1e-8  # Avoid division by zero\n    accuracy = 1.0 / (1.0 + distance / max_distance)\n\n    return accuracy\n"})}),"\n",(0,a.jsx)(e.h2,{id:"integration-with-ros-2",children:"Integration with ROS 2"}),"\n",(0,a.jsx)(e.h3,{id:"ros-2-action-server-for-language-commands",children:"ROS 2 Action Server for Language Commands"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"import rclpy\nfrom rclpy.action import ActionServer\nfrom rclpy.node import Node\nfrom humanoid_robot_msgs.action import ExecuteLanguageCommand\n\nclass LanguageActionServer(Node):\n    def __init__(self):\n        super().__init__('language_action_server')\n\n        # Initialize language-action mapper\n        self.mapper = SafeLanguageActionMapper(GroundedLanguageActionMapper())\n\n        # Create action server\n        self._action_server = ActionServer(\n            self,\n            ExecuteLanguageCommand,\n            'execute_language_command',\n            self.execute_callback\n        )\n\n        # Subscribe to camera and other sensors\n        self.camera_sub = self.create_subscription(\n            Image, '/camera/image_raw', self.camera_callback, 10\n        )\n\n        self.current_image = None\n\n    def camera_callback(self, msg):\n        \"\"\"Receive camera image.\"\"\"\n        self.current_image = msg\n\n    def execute_callback(self, goal_handle):\n        \"\"\"Execute language command.\"\"\"\n        self.get_logger().info(f'Executing command: {goal_handle.request.instruction}')\n\n        feedback_msg = ExecuteLanguageCommand.Feedback()\n        result = ExecuteLanguageCommand.Result()\n\n        try:\n            # Process current image and instruction\n            if self.current_image is None:\n                raise RuntimeError('No camera image available')\n\n            # Map language to action with current visual context\n            action = self.mapper.safe_map_language_to_action(\n                self.current_image,\n                goal_handle.request.instruction\n            )\n\n            # Execute action\n            execution_result = self.execute_action(action)\n\n            if execution_result.success:\n                result.success = True\n                result.message = 'Command executed successfully'\n                goal_handle.succeed()\n            else:\n                result.success = False\n                result.message = f'Execution failed: {execution_result.error}'\n                goal_handle.abort()\n\n        except Exception as e:\n            self.get_logger().error(f'Command execution error: {e}')\n            result.success = False\n            result.message = f'Execution failed with error: {str(e)}'\n            goal_handle.abort()\n\n        return result\n\n    def execute_action(self, action):\n        \"\"\"Execute the generated action.\"\"\"\n        # Publish action to robot controller\n        # This would involve sending joint commands, navigation goals, etc.\n        pass\n"})}),"\n",(0,a.jsx)(e.h2,{id:"troubleshooting-common-issues",children:"Troubleshooting Common Issues"}),"\n",(0,a.jsx)(e.h3,{id:"1-ambiguous-instructions",children:"1. Ambiguous Instructions"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Implement disambiguation dialogues"}),"\n",(0,a.jsx)(e.li,{children:"Use context to resolve ambiguities"}),"\n",(0,a.jsx)(e.li,{children:"Provide feedback when clarification is needed"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"2-grounding-failures",children:"2. Grounding Failures"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Improve object detection and recognition"}),"\n",(0,a.jsx)(e.li,{children:"Use multiple modalities for robust grounding"}),"\n",(0,a.jsx)(e.li,{children:"Implement uncertainty estimation"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"3-action-space-mismatch",children:"3. Action Space Mismatch"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Ensure consistent action space definitions"}),"\n",(0,a.jsx)(e.li,{children:"Use appropriate action representations"}),"\n",(0,a.jsx)(e.li,{children:"Implement action space conversion utilities"}),"\n"]}),"\n",(0,a.jsx)(e.hr,{}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.a,{href:"/hackathon-ai-book/modules/vla-system/vla-architecture",children:"Next: VLA Architecture"})," | ",(0,a.jsx)(e.a,{href:"/hackathon-ai-book/modules/vla-system/multimodal-perception",children:"Previous: Multimodal Perception"})]})]})}function u(n={}){const{wrapper:e}={...(0,i.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(d,{...n})}):d(n)}}}]);