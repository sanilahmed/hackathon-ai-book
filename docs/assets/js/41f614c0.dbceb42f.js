"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[9388],{7642:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>u,frontMatter:()=>a,metadata:()=>o,toc:()=>d});var s=t(4848),i=t(8453);const a={},r="VLA Fundamentals",o={id:"modules/vla-system/vla-fundamentals",title:"VLA Fundamentals",description:"Overview",source:"@site/docs/modules/vla-system/vla-fundamentals.md",sourceDirName:"modules/vla-system",slug:"/modules/vla-system/vla-fundamentals",permalink:"/ai-robotic-book/modules/vla-system/vla-fundamentals",draft:!1,unlisted:!1,editUrl:"https://github.com/your-org/physical-ai-book/tree/main/docs/modules/vla-system/vla-fundamentals.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Module 4: Vision-Language-Action (VLA) System",permalink:"/ai-robotic-book/modules/vla-system/"},next:{title:"VLA Architecture",permalink:"/ai-robotic-book/modules/vla-system/vla-architecture"}},l={},d=[{value:"Overview",id:"overview",level:2},{value:"What is VLA?",id:"what-is-vla",level:2},{value:"Core Components of VLA Systems",id:"core-components-of-vla-systems",level:2},{value:"1. Vision Encoder",id:"1-vision-encoder",level:3},{value:"2. Language Encoder",id:"2-language-encoder",level:3},{value:"3. Multimodal Fusion",id:"3-multimodal-fusion",level:3},{value:"4. Action Decoder",id:"4-action-decoder",level:3},{value:"VLA System Architecture",id:"vla-system-architecture",level:2},{value:"End-to-End VLA Model",id:"end-to-end-vla-model",level:3},{value:"Key VLA Models and Architectures",id:"key-vla-models-and-architectures",level:2},{value:"1. CLIP-Based VLA",id:"1-clip-based-vla",level:3},{value:"2. RT-1 (Robot Transformer 1)",id:"2-rt-1-robot-transformer-1",level:3},{value:"Training Paradigms",id:"training-paradigms",level:2},{value:"1. Imitation Learning",id:"1-imitation-learning",level:3},{value:"2. Reinforcement Learning with Language Rewards",id:"2-reinforcement-learning-with-language-rewards",level:3},{value:"Challenges in VLA Systems",id:"challenges-in-vla-systems",level:2},{value:"1. Grounding Language to Perception",id:"1-grounding-language-to-perception",level:3},{value:"2. Temporal Reasoning",id:"2-temporal-reasoning",level:3},{value:"Evaluation Metrics",id:"evaluation-metrics",level:2},{value:"1. Task Success Rate",id:"1-task-success-rate",level:3},{value:"2. Language Alignment",id:"2-language-alignment",level:3},{value:"Safety Considerations",id:"safety-considerations",level:2},{value:"Integration with Robotics Stack",id:"integration-with-robotics-stack",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"vla-fundamentals",children:"VLA Fundamentals"}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"Vision-Language-Action (VLA) systems represent a significant advancement in robotics, enabling robots to understand natural language instructions and execute corresponding actions based on visual perception. This section covers the fundamental concepts, architectures, and principles underlying VLA systems for humanoid robots."}),"\n",(0,s.jsx)(n.h2,{id:"what-is-vla",children:"What is VLA?"}),"\n",(0,s.jsx)(n.p,{children:"Vision-Language-Action (VLA) refers to AI systems that integrate three key modalities:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Vision"}),": Processing visual information from cameras and sensors"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Language"}),": Understanding and generating natural language"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Action"}),": Executing robotic actions in the physical world"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"VLA systems enable robots to perform tasks based on natural language instructions while perceiving and interacting with their environment."}),"\n",(0,s.jsx)(n.h2,{id:"core-components-of-vla-systems",children:"Core Components of VLA Systems"}),"\n",(0,s.jsx)(n.h3,{id:"1-vision-encoder",children:"1. Vision Encoder"}),"\n",(0,s.jsx)(n.p,{children:"The vision encoder processes visual input from cameras and sensors:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\n\nclass VisionEncoder(nn.Module):\n    def __init__(self, backbone='resnet50', pretrained=True):\n        super().__init__()\n        # Use pre-trained vision model (e.g., ResNet, ViT)\n        self.backbone = torch.hub.load('pytorch/vision:v0.10.0', backbone, pretrained=pretrained)\n\n        # Remove classification head\n        self.features = nn.Sequential(*list(self.backbone.children())[:-1])\n\n        # Add projection layer to match language model dimensions\n        self.projection = nn.Linear(self.backbone.fc.in_features, 512)\n\n    def forward(self, images):\n        # Extract visual features\n        features = self.features(images)\n        features = torch.flatten(features, 1)\n\n        # Project to common space\n        projected_features = self.projection(features)\n\n        return projected_features\n"})}),"\n",(0,s.jsx)(n.h3,{id:"2-language-encoder",children:"2. Language Encoder"}),"\n",(0,s.jsx)(n.p,{children:"The language encoder processes natural language instructions:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\nfrom transformers import AutoTokenizer, AutoModel\n\nclass LanguageEncoder(nn.Module):\n    def __init__(self, model_name='bert-base-uncased'):\n        super().__init__()\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModel.from_pretrained(model_name)\n\n        # Add projection layer\n        self.projection = nn.Linear(self.model.config.hidden_size, 512)\n\n    def forward(self, text):\n        # Tokenize input text\n        inputs = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n\n        # Get language embeddings\n        outputs = self.model(**inputs)\n        embeddings = outputs.last_hidden_state.mean(dim=1)  # Average pooling\n\n        # Project to common space\n        projected_embeddings = self.projection(embeddings)\n\n        return projected_embeddings\n"})}),"\n",(0,s.jsx)(n.h3,{id:"3-multimodal-fusion",children:"3. Multimodal Fusion"}),"\n",(0,s.jsx)(n.p,{children:"The fusion module combines vision and language information:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class MultimodalFusion(nn.Module):\n    def __init__(self, feature_dim=512):\n        super().__init__()\n        # Cross-attention mechanism\n        self.cross_attention = nn.MultiheadAttention(\n            embed_dim=feature_dim,\n            num_heads=8,\n            batch_first=True\n        )\n\n        # Feed-forward network\n        self.ffn = nn.Sequential(\n            nn.Linear(feature_dim * 2, feature_dim * 4),\n            nn.ReLU(),\n            nn.Linear(feature_dim * 4, feature_dim)\n        )\n\n        # Layer normalization\n        self.norm1 = nn.LayerNorm(feature_dim)\n        self.norm2 = nn.LayerNorm(feature_dim)\n\n    def forward(self, vision_features, language_features):\n        # Concatenate vision and language features\n        combined_features = torch.cat([vision_features.unsqueeze(1),\n                                      language_features.unsqueeze(1)], dim=1)\n\n        # Self-attention within modalities\n        attended_features, _ = self.cross_attention(\n            combined_features, combined_features, combined_features\n        )\n\n        # Residual connection and normalization\n        fused_features = self.norm1(combined_features + attended_features)\n\n        # Feed-forward network\n        output = self.norm2(fused_features + self.ffn(fused_features))\n\n        return output\n"})}),"\n",(0,s.jsx)(n.h3,{id:"4-action-decoder",children:"4. Action Decoder"}),"\n",(0,s.jsx)(n.p,{children:"The action decoder generates robotic actions from fused representations:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class ActionDecoder(nn.Module):\n    def __init__(self, action_space_dim, feature_dim=512):\n        super().__init__()\n        self.action_space_dim = action_space_dim\n\n        # Decode fused features to action space\n        self.decoder = nn.Sequential(\n            nn.Linear(feature_dim, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Linear(512, action_space_dim)\n        )\n\n        # Optional: separate heads for different action types\n        self.position_head = nn.Linear(512, 3)  # x, y, z\n        self.orientation_head = nn.Linear(512, 4)  # quaternion\n        self.gripper_head = nn.Linear(512, 1)  # gripper position\n\n    def forward(self, fused_features):\n        # Decode to continuous action space\n        actions = self.decoder(fused_features)\n\n        # Alternative: use separate heads for different action components\n        position = self.position_head(fused_features)\n        orientation = self.orientation_head(fused_features)\n        gripper = torch.sigmoid(self.gripper_head(fused_features))  # normalized to [0,1]\n\n        return {\n            'actions': actions,\n            'position': position,\n            'orientation': orientation,\n            'gripper': gripper\n        }\n"})}),"\n",(0,s.jsx)(n.h2,{id:"vla-system-architecture",children:"VLA System Architecture"}),"\n",(0,s.jsx)(n.h3,{id:"end-to-end-vla-model",children:"End-to-End VLA Model"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class VLAModel(nn.Module):\n    def __init__(self, vision_backbone=\'resnet50\', language_model=\'bert-base-uncased\'):\n        super().__init__()\n\n        # Initialize components\n        self.vision_encoder = VisionEncoder(vision_backbone)\n        self.language_encoder = LanguageEncoder(language_model)\n        self.fusion_module = MultimodalFusion()\n        self.action_decoder = ActionDecoder(action_space_dim=12)  # Example: 12 DoF\n\n        # Optional: memory module for temporal reasoning\n        self.temporal_encoder = nn.LSTM(\n            input_size=512,\n            hidden_size=512,\n            num_layers=2,\n            batch_first=True\n        )\n\n    def forward(self, images, text, prev_actions=None):\n        # Encode visual input\n        vision_features = self.vision_encoder(images)\n\n        # Encode language input\n        language_features = self.language_encoder(text)\n\n        # Fuse multimodal information\n        fused_features = self.fusion_module(vision_features, language_features)\n\n        # Optional: incorporate temporal context\n        if prev_actions is not None:\n            temporal_features, _ = self.temporal_encoder(prev_actions)\n            fused_features = fused_features + temporal_features[-1]  # Add last temporal state\n\n        # Decode to actions\n        actions = self.action_decoder(fused_features)\n\n        return actions\n\n    def process_instruction(self, image, instruction):\n        """Process a single instruction with corresponding image."""\n        with torch.no_grad():\n            actions = self.forward(image.unsqueeze(0), [instruction])\n            return actions\n'})}),"\n",(0,s.jsx)(n.h2,{id:"key-vla-models-and-architectures",children:"Key VLA Models and Architectures"}),"\n",(0,s.jsx)(n.h3,{id:"1-clip-based-vla",children:"1. CLIP-Based VLA"}),"\n",(0,s.jsx)(n.p,{children:"CLIP (Contrastive Language-Image Pre-training) provides a foundation for VLA systems:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import clip\nfrom PIL import Image\n\nclass CLIPLanguageAction(nn.Module):\n    def __init__(self, device='cuda'):\n        super().__init__()\n        self.clip_model, self.preprocess = clip.load('ViT-B/32', device=device)\n        self.device = device\n\n        # Action prediction head\n        self.action_head = nn.Linear(512, 12)  # Example: 12 DoF actions\n\n    def forward(self, image, text):\n        # Preprocess image\n        image_input = self.preprocess(image).unsqueeze(0).to(self.device)\n\n        # Tokenize text\n        text_input = clip.tokenize([text]).to(self.device)\n\n        # Get CLIP embeddings\n        with torch.no_grad():\n            image_features = self.clip_model.encode_image(image_input)\n            text_features = self.clip_model.encode_text(text_input)\n\n        # Combine features (simple concatenation or more sophisticated fusion)\n        combined_features = torch.cat([image_features, text_features], dim=1)\n\n        # Predict actions\n        actions = self.action_head(combined_features)\n\n        return actions\n"})}),"\n",(0,s.jsx)(n.h3,{id:"2-rt-1-robot-transformer-1",children:"2. RT-1 (Robot Transformer 1)"}),"\n",(0,s.jsx)(n.p,{children:"RT-1 is a transformer-based VLA model for language-conditioned robot learning:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\nfrom transformers import T5EncoderModel, T5Tokenizer\n\nclass RT1Model(nn.Module):\n    def __init__(self, num_actions=12):\n        super().__init__()\n\n        # T5 for language understanding\n        self.t5_tokenizer = T5Tokenizer.from_pretrained('t5-base')\n        self.t5_encoder = T5EncoderModel.from_pretrained('t5-base')\n\n        # Vision encoder\n        self.vision_encoder = VisionEncoder('resnet50')\n\n        # Transformer for temporal reasoning\n        self.transformer = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(\n                d_model=512,\n                nhead=8,\n                dim_feedforward=2048,\n                batch_first=True\n            ),\n            num_layers=6\n        )\n\n        # Action prediction head\n        self.action_head = nn.Linear(512, num_actions)\n\n    def forward(self, images, instructions, timesteps=None):\n        # Encode language\n        text_inputs = self.t5_tokenizer(\n            instructions,\n            return_tensors='pt',\n            padding=True,\n            truncation=True\n        )\n        text_embeddings = self.t5_encoder(**text_inputs).last_hidden_state.mean(dim=1)\n\n        # Encode vision\n        vision_embeddings = self.vision_encoder(images)\n\n        # Combine modalities\n        combined_embeddings = torch.cat([\n            text_embeddings.unsqueeze(1),\n            vision_embeddings.unsqueeze(1)\n        ], dim=1).squeeze(1)\n\n        # Apply transformer for temporal reasoning\n        if timesteps is not None:\n            combined_embeddings = self.transformer(combined_embeddings)\n\n        # Predict actions\n        actions = self.action_head(combined_embeddings)\n\n        return actions\n"})}),"\n",(0,s.jsx)(n.h2,{id:"training-paradigms",children:"Training Paradigms"}),"\n",(0,s.jsx)(n.h3,{id:"1-imitation-learning",children:"1. Imitation Learning"}),"\n",(0,s.jsx)(n.p,{children:"VLA systems are often trained using imitation learning from human demonstrations:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class VLAImitationTrainer:\n    def __init__(self, model, learning_rate=1e-4):\n        self.model = model\n        self.optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n        self.criterion = nn.MSELoss()\n\n    def train_step(self, batch):\n        images, instructions, expert_actions = batch\n\n        # Forward pass\n        predicted_actions = self.model(images, instructions)\n\n        # Compute loss\n        loss = self.criterion(predicted_actions['actions'], expert_actions)\n\n        # Backward pass\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n\n        return loss.item()\n"})}),"\n",(0,s.jsx)(n.h3,{id:"2-reinforcement-learning-with-language-rewards",children:"2. Reinforcement Learning with Language Rewards"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class VLAReinforcementTrainer:\n    def __init__(self, model, reward_fn):\n        self.model = model\n        self.reward_fn = reward_fn\n\n    def compute_language_reward(self, instruction, achieved_state, desired_state):\n        """Compute reward based on language instruction and task completion."""\n        # Use language model to assess if instruction was satisfied\n        reward = self.reward_fn(instruction, achieved_state, desired_state)\n        return reward\n\n    def train_with_language_reward(self, instruction, env):\n        """Train using language-conditioned rewards."""\n        state = env.reset()\n        total_reward = 0\n\n        for step in range(100):  # Max steps\n            # Get action from VLA model\n            action = self.model.process_instruction(state.image, instruction)\n\n            # Execute action\n            next_state, _, done, _ = env.step(action)\n\n            # Compute language-based reward\n            reward = self.compute_language_reward(\n                instruction,\n                next_state,\n                desired_state_from_instruction(instruction)\n            )\n\n            total_reward += reward\n\n            if done:\n                break\n\n        return total_reward\n'})}),"\n",(0,s.jsx)(n.h2,{id:"challenges-in-vla-systems",children:"Challenges in VLA Systems"}),"\n",(0,s.jsx)(n.h3,{id:"1-grounding-language-to-perception",children:"1. Grounding Language to Perception"}),"\n",(0,s.jsx)(n.p,{children:"One of the main challenges is grounding abstract language concepts to concrete visual perceptions:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class LanguageGrounding:\n    def __init__(self):\n        # Object detection model for grounding\n        self.object_detector = ObjectDetector()\n        # Spatial relation understanding\n        self.spatial_reasoner = SpatialReasoner()\n\n    def ground_language_to_objects(self, instruction, image):\n        """Ground language elements to visual objects."""\n        # Detect objects in image\n        objects = self.object_detector.detect(image)\n\n        # Parse instruction for object references\n        object_references = self.parse_object_references(instruction)\n\n        # Ground language to visual objects\n        grounded_objects = {}\n        for ref in object_references:\n            # Find corresponding visual object\n            visual_object = self.find_visual_object(ref, objects)\n            grounded_objects[ref] = visual_object\n\n        return grounded_objects\n'})}),"\n",(0,s.jsx)(n.h3,{id:"2-temporal-reasoning",children:"2. Temporal Reasoning"}),"\n",(0,s.jsx)(n.p,{children:"VLA systems need to handle multi-step instructions and temporal dependencies:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class TemporalVLA(nn.Module):\n    def __init__(self, model):\n        super().__init__()\n        self.base_model = model\n        # Memory for temporal context\n        self.memory = nn.LSTM(512, 512, batch_first=True)\n        # Instruction parser for multi-step tasks\n        self.instruction_parser = InstructionParser()\n\n    def forward(self, images, instruction, memory_state=None):\n        # Parse multi-step instruction\n        subtasks = self.instruction_parser.parse(instruction)\n\n        # Process each subtask with temporal context\n        all_actions = []\n        current_memory = memory_state\n\n        for subtask in subtasks:\n            # Get action for subtask\n            actions = self.base_model(images, subtask)\n\n            # Update memory with current state\n            memory_output, current_memory = self.memory(\n                actions['actions'].unsqueeze(1),\n                current_memory\n            )\n\n            all_actions.append(actions)\n\n        return all_actions\n"})}),"\n",(0,s.jsx)(n.h2,{id:"evaluation-metrics",children:"Evaluation Metrics"}),"\n",(0,s.jsx)(n.h3,{id:"1-task-success-rate",children:"1. Task Success Rate"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def evaluate_task_success(model, test_instructions, test_env):\n    """Evaluate VLA model on task success rate."""\n    successes = 0\n    total = len(test_instructions)\n\n    for instruction in test_instructions:\n        success = execute_instruction(model, test_env, instruction)\n        if success:\n            successes += 1\n\n    success_rate = successes / total\n    return success_rate\n'})}),"\n",(0,s.jsx)(n.h3,{id:"2-language-alignment",children:"2. Language Alignment"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def evaluate_language_alignment(model, instruction, execution_trace):\n    """Evaluate how well actions align with language instruction."""\n    # Use language model to assess alignment\n    alignment_score = compute_alignment_score(instruction, execution_trace)\n    return alignment_score\n'})}),"\n",(0,s.jsx)(n.h2,{id:"safety-considerations",children:"Safety Considerations"}),"\n",(0,s.jsx)(n.p,{children:"VLA systems must include safety mechanisms to prevent harmful actions:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class SafeVLA:\n    def __init__(self, base_model, safety_checker):\n        self.model = base_model\n        self.safety_checker = safety_checker\n\n    def safe_execute_instruction(self, image, instruction):\n        """Execute instruction with safety checks."""\n        # Get predicted actions\n        actions = self.model(image, instruction)\n\n        # Check safety\n        if self.safety_checker.is_safe(actions):\n            return actions\n        else:\n            # Return safe fallback action\n            return self.safety_checker.get_safe_action()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"integration-with-robotics-stack",children:"Integration with Robotics Stack"}),"\n",(0,s.jsx)(n.p,{children:"VLA systems integrate with the broader robotics stack:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   VLA Model    \u2502\u2500\u2500\u2500\u2500\u2502  ROS 2 Bridge   \u2502\u2500\u2500\u2500\u2500\u2502   Robot Driver  \u2502\n\u2502                 \u2502    \u2502                 \u2502    \u2502                 \u2502\n\u2502 Vision Encoder  \u2502    \u2502 Action Commands \u2502    \u2502 Joint Commands  \u2502\n\u2502 Language Enc.   \u2502    \u2502 State Feedback  \u2502    \u2502 Motor Control   \u2502\n\u2502 Fusion Module   \u2502    \u2502 TF Transforms   \u2502    \u2502 Safety Systems  \u2502\n\u2502 Action Decoder  \u2502    \u2502 Navigation      \u2502    \u2502                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,s.jsx)(n.p,{children:"This integration enables VLA systems to work with existing robotic infrastructure while providing natural language interfaces for human-robot interaction."}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.a,{href:"/ai-robotic-book/modules/vla-system/multimodal-perception",children:"Next: Multimodal Perception"})," | ",(0,s.jsx)(n.a,{href:"/ai-robotic-book/modules/vla-system/",children:"Previous: Module 4 Index"})]})]})}function u(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>o});var s=t(6540);const i={},a=s.createContext(i);function r(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);