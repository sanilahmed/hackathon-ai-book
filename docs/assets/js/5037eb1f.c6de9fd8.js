"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[7618],{5156:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>c,contentTitle:()=>r,default:()=>m,frontMatter:()=>a,metadata:()=>s,toc:()=>l});var i=t(4848),o=t(8453);const a={},r="Lab 4.3: Action Mapping in Vision-Language-Action Systems",s={id:"modules/lab-exercises/lab-4-3-action-mapping",title:"Lab 4.3: Action Mapping in Vision-Language-Action Systems",description:"Overview",source:"@site/docs/modules/lab-exercises/lab-4-3-action-mapping.md",sourceDirName:"modules/lab-exercises",slug:"/modules/lab-exercises/lab-4-3-action-mapping",permalink:"/hackathon-ai-book/modules/lab-exercises/lab-4-3-action-mapping",draft:!1,unlisted:!1,editUrl:"https://github.com/sanilahmed/hackathon-ai-book/tree/main/docs/modules/lab-exercises/lab-4-3-action-mapping.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Lab 4.2: Multimodal Perception in Vision-Language-Action Systems",permalink:"/hackathon-ai-book/modules/lab-exercises/lab-4-2-multimodal-perception"},next:{title:"Vision-Language-Action (VLA) System References",permalink:"/hackathon-ai-book/modules/vla-system/references"}},c={},l=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Theory Background",id:"theory-background",level:2},{value:"Lab Exercise",id:"lab-exercise",level:2},{value:"Part 1: Action Space Representation",id:"part-1-action-space-representation",level:3},{value:"Part 2: Action Mapping Network",id:"part-2-action-mapping-network",level:3},{value:"Part 3: Hierarchical Action Planner",id:"part-3-hierarchical-action-planner",level:3},{value:"Part 4: Policy Gradient Action Learner",id:"part-4-policy-gradient-action-learner",level:3},{value:"Part 5: Action Execution and Control",id:"part-5-action-execution-and-control",level:3},{value:"Part 6: Integration with ROS 2",id:"part-6-integration-with-ros-2",level:3},{value:"Implementation Steps",id:"implementation-steps",level:2},{value:"Expected Outcomes",id:"expected-outcomes",level:2},{value:"Troubleshooting Tips",id:"troubleshooting-tips",level:2},{value:"Further Exploration",id:"further-exploration",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.h1,{id:"lab-43-action-mapping-in-vision-language-action-systems",children:"Lab 4.3: Action Mapping in Vision-Language-Action Systems"}),"\n",(0,i.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,i.jsx)(e.p,{children:"This lab exercise focuses on implementing action mapping systems that convert high-level vision-language understanding into low-level robot control commands. Students will learn to bridge the gap between perceptual understanding and executable actions for humanoid robots."}),"\n",(0,i.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsx)(e.p,{children:"By the end of this lab, students will be able to:"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"Implement action mapping networks that convert multimodal representations to motor commands"}),"\n",(0,i.jsx)(e.li,{children:"Design inverse kinematics systems for humanoid robot control"}),"\n",(0,i.jsx)(e.li,{children:"Create hierarchical action planning and execution frameworks"}),"\n",(0,i.jsx)(e.li,{children:"Evaluate action mapping performance using trajectory accuracy metrics"}),"\n",(0,i.jsx)(e.li,{children:"Integrate action mapping with perception and language understanding systems"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Completion of Lab 4.1: VLA Fundamentals and Lab 4.2: Multimodal Perception"}),"\n",(0,i.jsx)(e.li,{children:"Understanding of robotics kinematics and control theory"}),"\n",(0,i.jsx)(e.li,{children:"Knowledge of deep reinforcement learning concepts"}),"\n",(0,i.jsx)(e.li,{children:"Familiarity with ROS 2 and robot control interfaces"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"theory-background",children:"Theory Background"}),"\n",(0,i.jsx)(e.p,{children:"Action mapping in VLA systems involves translating abstract goals derived from vision and language into concrete motor commands. Key components include:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Motor Primitives"}),": Low-level movement patterns that can be combined for complex behaviors"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Inverse Kinematics"}),": Mathematical methods to compute joint angles for desired end-effector positions"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Action Spaces"}),": Representation of possible robot actions (discrete or continuous)"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Policy Networks"}),": Neural networks that map states to actions"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Trajectory Optimization"}),": Methods to generate smooth, feasible robot trajectories"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"lab-exercise",children:"Lab Exercise"}),"\n",(0,i.jsx)(e.h3,{id:"part-1-action-space-representation",children:"Part 1: Action Space Representation"}),"\n",(0,i.jsx)(e.p,{children:"First, let's define different action space representations for humanoid robots:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\nclass ActionSpaceConverter:\n    def __init__(self, robot_dof=14):\n        """\n        Convert between different action space representations\n\n        Args:\n            robot_dof: Number of degrees of freedom in the robot\n        """\n        self.robot_dof = robot_dof\n\n    def joint_space_to_cartesian(self, joint_positions, link_names):\n        """\n        Convert joint space positions to Cartesian coordinates (simplified)\n\n        Args:\n            joint_positions: [batch_size, robot_dof] - Joint angle positions\n            link_names: List of link names to compute positions for\n\n        Returns:\n            cartesian_positions: [batch_size, len(link_names), 3] - Cartesian coordinates\n        """\n        # This is a simplified representation - in practice, use FK solvers like KDL or Pinocchio\n        batch_size = joint_positions.shape[0]\n\n        # For demonstration, we\'ll simulate a simple transformation\n        # In reality, this would involve complex forward kinematics\n        cartesian_positions = torch.zeros(batch_size, len(link_names), 3)\n\n        # Simulated FK computation\n        for i in range(len(link_names)):\n            # Simplified transformation based on joint angles\n            scale_factor = 0.1 * (i + 1)\n            cartesian_positions[:, i, 0] = torch.sin(joint_positions[:, 0]) * scale_factor\n            cartesian_positions[:, i, 1] = torch.cos(joint_positions[:, 1]) * scale_factor\n            cartesian_positions[:, i, 2] = joint_positions[:, 2] * scale_factor\n\n        return cartesian_positions\n\n    def cartesian_to_joint_space(self, target_positions, current_joints):\n        """\n        Convert Cartesian target positions to joint space (inverse kinematics - simplified)\n\n        Args:\n            target_positions: [batch_size, num_targets, 3] - Desired Cartesian positions\n            current_joints: [batch_size, robot_dof] - Current joint configuration\n\n        Returns:\n            joint_commands: [batch_size, robot_dof] - Joint angle commands\n        """\n        # Simplified IK - in practice, use numerical IK solvers\n        batch_size = target_positions.shape[0]\n\n        # Initialize joint commands with current joints\n        joint_commands = current_joints.clone()\n\n        # Apply simplified inverse kinematics\n        for i in range(target_positions.shape[1]):\n            # Map Cartesian error to joint adjustments\n            pos_error = target_positions[:, i, :] - self.joint_space_to_cartesian(\n                current_joints, [\'target\']\n            ).squeeze(1)\n\n            # Simple Jacobian-based update (simplified)\n            joint_delta = torch.zeros_like(current_joints)\n            joint_delta[:, :3] = pos_error * 0.1  # Map position error to first 3 joints\n\n            joint_commands = current_joints + joint_delta\n\n        return joint_commands\n\n# Test the action space converter\ndef test_action_space_converter():\n    batch_size, robot_dof = 4, 14\n    link_names = [\'left_hand\', \'right_hand\', \'head\']\n\n    converter = ActionSpaceConverter(robot_dof)\n\n    joint_positions = torch.randn(batch_size, robot_dof)\n    cartesian_pos = converter.joint_space_to_cartesian(joint_positions, link_names)\n\n    print(f"Joint positions shape: {joint_positions.shape}")\n    print(f"Cartesian positions shape: {cartesian_pos.shape}")\n\n    target_positions = torch.randn(batch_size, 2, 3)\n    joint_commands = converter.cartesian_to_joint_space(target_positions, joint_positions)\n\n    print(f"Target positions shape: {target_positions.shape}")\n    print(f"Joint commands shape: {joint_commands.shape}")\n\n    return joint_commands\n\nif __name__ == "__main__":\n    test_action_space_converter()\n'})}),"\n",(0,i.jsx)(e.h3,{id:"part-2-action-mapping-network",children:"Part 2: Action Mapping Network"}),"\n",(0,i.jsx)(e.p,{children:"Now let's create a neural network that maps multimodal representations to action commands:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'class ActionMappingNetwork(nn.Module):\n    def __init__(self, input_dim, action_dim, hidden_dims=[512, 256, 128]):\n        super().__init__()\n        self.input_dim = input_dim\n        self.action_dim = action_dim\n        self.hidden_dims = hidden_dims\n\n        # Build the network layers\n        layers = []\n        prev_dim = input_dim\n\n        for hidden_dim in hidden_dims:\n            layers.extend([\n                nn.Linear(prev_dim, hidden_dim),\n                nn.ReLU(),\n                nn.Dropout(0.1)\n            ])\n            prev_dim = hidden_dim\n\n        # Output layer for action prediction\n        layers.append(nn.Linear(prev_dim, action_dim))\n\n        self.network = nn.Sequential(*layers)\n\n        # Action normalization parameters\n        self.register_buffer(\'action_mean\', torch.zeros(action_dim))\n        self.register_buffer(\'action_std\', torch.ones(action_dim))\n\n    def forward(self, multimodal_features):\n        """\n        Args:\n            multimodal_features: [batch_size, input_dim] - Fused vision-language features\n        Returns:\n            action_pred: [batch_size, action_dim] - Predicted action commands\n        """\n        raw_action = self.network(multimodal_features)\n\n        # Normalize action outputs\n        normalized_action = (raw_action - self.action_mean) / (self.action_std + 1e-8)\n\n        return normalized_action\n\n    def set_action_normalization(self, mean, std):\n        """Set action normalization parameters"""\n        self.action_mean.copy_(torch.tensor(mean))\n        self.action_std.copy_(torch.tensor(std))\n\n# Test the action mapping network\ndef test_action_mapping_network():\n    batch_size, input_dim, action_dim = 8, 256, 14  # 14 DOF humanoid\n\n    multimodal_features = torch.randn(batch_size, input_dim)\n\n    action_mapper = ActionMappingNetwork(input_dim, action_dim)\n    action_pred = action_mapper(multimodal_features)\n\n    print(f"Input features shape: {multimodal_features.shape}")\n    print(f"Action prediction shape: {action_pred.shape}")\n    print(f"Action prediction range: [{action_pred.min():.3f}, {action_pred.max():.3f}]")\n\n    return action_pred\n\nif __name__ == "__main__":\n    test_action_mapping_network()\n'})}),"\n",(0,i.jsx)(e.h3,{id:"part-3-hierarchical-action-planner",children:"Part 3: Hierarchical Action Planner"}),"\n",(0,i.jsx)(e.p,{children:"Let's implement a hierarchical action planner that breaks down high-level goals into executable steps:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"class HierarchicalActionPlanner:\n    def __init__(self, max_steps=100):\n        self.max_steps = max_steps\n\n    def plan_from_command(self, command_str, current_state, goal_state):\n        \"\"\"\n        Plan a sequence of actions to achieve a goal from a natural language command\n\n        Args:\n            command_str: Natural language command\n            current_state: Current robot state\n            goal_state: Desired goal state\n\n        Returns:\n            action_sequence: List of action tensors representing the plan\n        \"\"\"\n        # Parse the command to identify high-level goal\n        parsed_goal = self.parse_command(command_str)\n\n        # Generate intermediate waypoints\n        waypoints = self.generate_waypoints(current_state, goal_state)\n\n        # Create action sequence\n        action_sequence = self.create_action_sequence(waypoints, parsed_goal)\n\n        return action_sequence\n\n    def parse_command(self, command_str):\n        \"\"\"Parse natural language command into structured goal\"\"\"\n        command_lower = command_str.lower()\n\n        # Simple keyword-based parsing (in practice, use NLP models)\n        if 'move' in command_lower or 'go' in command_lower:\n            return {'type': 'navigation', 'direction': self.extract_direction(command_str)}\n        elif 'pick' in command_lower or 'grasp' in command_lower:\n            return {'type': 'manipulation', 'object': self.extract_object(command_str)}\n        elif 'turn' in command_lower or 'rotate' in command_lower:\n            return {'type': 'orientation', 'angle': self.extract_angle(command_str)}\n        else:\n            return {'type': 'unknown', 'command': command_str}\n\n    def extract_direction(self, command_str):\n        \"\"\"Extract direction from command\"\"\"\n        if 'forward' in command_str.lower():\n            return 'forward'\n        elif 'backward' in command_str.lower():\n            return 'backward'\n        elif 'left' in command_str.lower():\n            return 'left'\n        elif 'right' in command_str.lower():\n            return 'right'\n        else:\n            return 'forward'  # default\n\n    def extract_object(self, command_str):\n        \"\"\"Extract object from command\"\"\"\n        # Simple extraction - in practice, use NER models\n        words = command_str.lower().split()\n        objects = ['box', 'ball', 'cup', 'book', 'object']\n\n        for word in words:\n            if word in objects:\n                return word\n\n        return 'object'  # default\n\n    def extract_angle(self, command_str):\n        \"\"\"Extract angle from command\"\"\"\n        import re\n        # Look for angle specifications\n        angle_match = re.search(r'(\\d+)\\s*(degrees|deg)', command_str.lower())\n        if angle_match:\n            return int(angle_match.group(1))\n        return 90  # default angle\n\n    def generate_waypoints(self, current_state, goal_state):\n        \"\"\"Generate intermediate waypoints between current and goal states\"\"\"\n        # Simple linear interpolation (in practice, use motion planning algorithms)\n        num_waypoints = 10\n        waypoints = []\n\n        for i in range(num_waypoints + 1):\n            ratio = i / num_waypoints\n            waypoint = current_state + ratio * (goal_state - current_state)\n            waypoints.append(waypoint)\n\n        return waypoints\n\n    def create_action_sequence(self, waypoints, parsed_goal):\n        \"\"\"Convert waypoints to action sequence\"\"\"\n        action_sequence = []\n\n        for i in range(len(waypoints) - 1):\n            # Calculate difference between consecutive waypoints\n            delta = waypoints[i + 1] - waypoints[i]\n            action_tensor = torch.tensor(delta, dtype=torch.float32)\n            action_sequence.append(action_tensor)\n\n        return action_sequence\n\n# Test the hierarchical planner\ndef test_hierarchical_planner():\n    planner = HierarchicalActionPlanner()\n\n    current_state = torch.randn(14)  # 14 DOF humanoid state\n    goal_state = torch.randn(14)     # Target state\n    command = \"Move the robot forward to pick up the red box\"\n\n    action_sequence = planner.plan_from_command(command, current_state, goal_state)\n\n    print(f\"Command: {command}\")\n    print(f\"Parsed goal: {planner.parse_command(command)}\")\n    print(f\"Number of actions in sequence: {len(action_sequence)}\")\n    if action_sequence:\n        print(f\"First action shape: {action_sequence[0].shape}\")\n        print(f\"First action: {action_sequence[0][:5]}...\")  # Show first 5 elements\n\n    return action_sequence\n\nif __name__ == \"__main__\":\n    test_hierarchical_planner()\n"})}),"\n",(0,i.jsx)(e.h3,{id:"part-4-policy-gradient-action-learner",children:"Part 4: Policy Gradient Action Learner"}),"\n",(0,i.jsx)(e.p,{children:"Now let's implement a reinforcement learning approach for learning action mappings:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'class PolicyGradientActionLearner(nn.Module):\n    def __init__(self, state_dim, action_dim, hidden_dim=256):\n        super().__init__()\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n\n        # Actor network (policy)\n        self.actor = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, action_dim),\n            nn.Tanh()  # Actions in [-1, 1]\n        )\n\n        # Critic network (value function)\n        self.critic = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)\n        )\n\n        # Action distribution parameters\n        self.log_std = nn.Parameter(torch.zeros(action_dim))\n\n    def forward(self, state):\n        """Forward pass through actor and critic"""\n        action_mean = self.actor(state)\n        state_value = self.critic(state)\n        return action_mean, state_value\n\n    def get_action(self, state, deterministic=False):\n        """Sample action from the policy"""\n        action_mean, _ = self.forward(state)\n\n        if deterministic:\n            return action_mean, None\n\n        # Sample from Gaussian distribution\n        action_std = torch.exp(self.log_std)\n        dist = torch.distributions.Normal(action_mean, action_std)\n        action = dist.sample()\n        log_prob = dist.log_prob(action).sum(dim=-1)\n\n        return action, log_prob\n\n    def evaluate_actions(self, states, actions):\n        """Evaluate actions for policy gradient computation"""\n        action_means, state_values = self.forward(states)\n\n        action_std = torch.exp(self.log_std)\n        dist = torch.distributions.Normal(action_means, action_std)\n        log_probs = dist.log_prob(actions).sum(dim=-1)\n        entropy = dist.entropy().sum(dim=-1)\n\n        return log_probs, state_values.squeeze(), entropy\n\n# Test the policy gradient learner\ndef test_policy_gradient_learner():\n    batch_size, state_dim, action_dim = 16, 256, 14  # State from VLA system, 14 DOF\n\n    states = torch.randn(batch_size, state_dim)\n    actions = torch.randn(batch_size, action_dim)\n\n    learner = PolicyGradientActionLearner(state_dim, action_dim)\n\n    # Test sampling actions\n    sampled_actions, log_probs = learner.get_action(states[0:1], deterministic=False)\n    print(f"Sampled action shape: {sampled_actions.shape}")\n    print(f"Log probability shape: {log_probs.shape}")\n\n    # Test action evaluation\n    log_probs, state_values, entropy = learner.evaluate_actions(states, actions)\n    print(f"Log probs shape: {log_probs.shape}")\n    print(f"State values shape: {state_values.shape}")\n    print(f"Entropy shape: {entropy.shape}")\n\n    return learner\n\nif __name__ == "__main__":\n    test_policy_gradient_learner()\n'})}),"\n",(0,i.jsx)(e.h3,{id:"part-5-action-execution-and-control",children:"Part 5: Action Execution and Control"}),"\n",(0,i.jsx)(e.p,{children:"Let's create a system for executing actions on the robot:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'class ActionExecutionController:\n    def __init__(self, robot_interface):\n        self.robot_interface = robot_interface\n        self.action_history = []\n\n    def execute_action_sequence(self, action_sequence, max_duration=10.0):\n        """\n        Execute a sequence of actions on the robot\n\n        Args:\n            action_sequence: List of action tensors\n            max_duration: Maximum time to execute the sequence\n\n        Returns:\n            execution_result: Dictionary with execution metrics\n        """\n        start_time = time.time()\n        success_count = 0\n        total_actions = len(action_sequence)\n\n        for i, action in enumerate(action_sequence):\n            if time.time() - start_time > max_duration:\n                break\n\n            # Execute individual action\n            success = self.execute_single_action(action)\n            if success:\n                success_count += 1\n\n            # Store in history\n            self.action_history.append({\n                \'step\': i,\n                \'action\': action.numpy(),\n                \'success\': success,\n                \'timestamp\': time.time()\n            })\n\n        execution_result = {\n            \'success_rate\': success_count / total_actions if total_actions > 0 else 0,\n            \'total_executed\': success_count,\n            \'total_requested\': total_actions,\n            \'execution_time\': time.time() - start_time\n        }\n\n        return execution_result\n\n    def execute_single_action(self, action):\n        """Execute a single action on the robot"""\n        try:\n            # Convert action to robot command format\n            robot_cmd = self.convert_action_to_command(action)\n\n            # Send command to robot\n            self.robot_interface.send_command(robot_cmd)\n\n            # Wait for execution\n            success = self.wait_for_execution_completion()\n\n            return success\n        except Exception as e:\n            print(f"Error executing action: {e}")\n            return False\n\n    def convert_action_to_command(self, action):\n        """Convert action tensor to robot command format"""\n        # This would typically convert to joint positions, velocities, or torques\n        # depending on the robot interface\n        command = {\n            \'joint_positions\': action.tolist()[:7],  # Example: first 7 joints\n            \'velocities\': [0.0] * 7,  # Zero velocity for position control\n            \'effort\': [0.0] * 7       # Zero effort (torque) control\n        }\n        return command\n\n    def wait_for_execution_completion(self, timeout=2.0):\n        """Wait for action execution to complete"""\n        # In practice, this would check robot feedback\n        import time\n        time.sleep(0.1)  # Simulate waiting\n        return True  # Simulate success\n\n# Robot interface mock for testing\nclass MockRobotInterface:\n    def __init__(self):\n        self.current_joint_positions = torch.zeros(14)\n\n    def send_command(self, command):\n        """Send command to robot (mock implementation)"""\n        print(f"Sending command: {command}")\n        # Update simulated joint positions\n        if \'joint_positions\' in command:\n            self.current_joint_positions[:len(command[\'joint_positions\'])] = \\\n                torch.tensor(command[\'joint_positions\'], dtype=torch.float32)\n\n    def get_current_state(self):\n        """Get current robot state"""\n        return self.current_joint_positions\n\n# Test the action execution controller\ndef test_action_execution_controller():\n    import time\n\n    robot_interface = MockRobotInterface()\n    controller = ActionExecutionController(robot_interface)\n\n    # Create a simple action sequence\n    action_sequence = [\n        torch.randn(14) * 0.1,  # Small random movements\n        torch.randn(14) * 0.1,\n        torch.randn(14) * 0.1\n    ]\n\n    result = controller.execute_action_sequence(action_sequence, max_duration=5.0)\n\n    print("Execution Result:")\n    for key, value in result.items():\n        print(f"  {key}: {value}")\n\n    print(f"Action history length: {len(controller.action_history)}")\n\n    return result\n\nif __name__ == "__main__":\n    test_action_execution_controller()\n'})}),"\n",(0,i.jsx)(e.h3,{id:"part-6-integration-with-ros-2",children:"Part 6: Integration with ROS 2"}),"\n",(0,i.jsx)(e.p,{children:"Finally, let's create a ROS 2 node that integrates the action mapping system:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import Float32MultiArray\nfrom sensor_msgs.msg import JointState\nfrom control_msgs.msg import FollowJointTrajectoryAction, FollowJointTrajectoryGoal\nfrom trajectory_msgs.msg import JointTrajectory, JointTrajectoryPoint\nimport torch\nimport numpy as np\nimport time\n\nclass ActionMappingNode(Node):\n    def __init__(self):\n        super().__init__('action_mapping_node')\n\n        # Initialize action mapping network\n        self.initialize_action_mapper()\n\n        # Publishers and subscribers\n        self.perception_sub = self.create_subscription(\n            Float32MultiArray,\n            '/multimodal_features',\n            self.perception_callback,\n            10\n        )\n\n        self.action_pub = self.create_publisher(\n            JointTrajectory,\n            '/joint_trajectory_controller/joint_trajectory',\n            10\n        )\n\n        self.joint_state_sub = self.create_subscription(\n            JointState,\n            '/joint_states',\n            self.joint_state_callback,\n            10\n        )\n\n        # Timer for processing loop\n        self.timer = self.create_timer(0.05, self.process_loop)  # 20 Hz\n\n        # Internal state\n        self.current_features = None\n        self.current_joint_state = None\n        self.action_history = []\n\n    def initialize_action_mapper(self):\n        \"\"\"Initialize the action mapping network\"\"\"\n        # Create action mapper (using the classes defined above)\n        self.action_mapper = ActionMappingNetwork(input_dim=256, action_dim=14)\n        self.planner = HierarchicalActionPlanner()\n\n        # Set to evaluation mode\n        self.action_mapper.eval()\n        self.get_logger().info('Action mapping network initialized')\n\n    def perception_callback(self, msg):\n        \"\"\"Process incoming multimodal features\"\"\"\n        try:\n            # Convert Float32MultiArray to tensor\n            features = torch.tensor(list(msg.data), dtype=torch.float32).unsqueeze(0)\n\n            if features.shape[1] != 256:  # Expected feature dimension\n                self.get_logger().warn(f'Unexpected feature dimension: {features.shape[1]}')\n                return\n\n            self.current_features = features\n        except Exception as e:\n            self.get_logger().error(f'Error processing perception: {str(e)}')\n\n    def joint_state_callback(self, msg):\n        \"\"\"Process incoming joint states\"\"\"\n        try:\n            self.current_joint_state = {\n                'position': msg.position,\n                'velocity': msg.velocity,\n                'effort': msg.effort\n            }\n        except Exception as e:\n            self.get_logger().error(f'Error processing joint state: {str(e)}')\n\n    def process_loop(self):\n        \"\"\"Main processing loop\"\"\"\n        if self.current_features is not None and self.current_joint_state is not None:\n            try:\n                # Generate action using the action mapping network\n                with torch.no_grad():\n                    predicted_action = self.action_mapper(self.current_features)\n\n                # Convert action to joint trajectory\n                joint_trajectory = self.create_joint_trajectory(predicted_action)\n\n                # Publish the trajectory\n                self.action_pub.publish(joint_trajectory)\n\n                # Store in history\n                self.action_history.append({\n                    'timestamp': time.time(),\n                    'action': predicted_action.numpy()[0],\n                    'published': True\n                })\n\n                # Log action\n                self.get_logger().info(f'Published action with norm: {torch.norm(predicted_action).item():.3f}')\n\n            except Exception as e:\n                self.get_logger().error(f'Error in processing loop: {str(e)}')\n\n    def create_joint_trajectory(self, action_tensor):\n        \"\"\"Create a joint trajectory message from action tensor\"\"\"\n        traj_msg = JointTrajectory()\n\n        # Define joint names (these should match your robot's joint names)\n        joint_names = [\n            'torso_joint', 'head_joint',\n            'left_shoulder_joint', 'left_elbow_joint', 'left_wrist_joint',\n            'right_shoulder_joint', 'right_elbow_joint', 'right_wrist_joint',\n            'left_hip_joint', 'left_knee_joint', 'left_ankle_joint',\n            'right_hip_joint', 'right_knee_joint', 'right_ankle_joint'\n        ]\n\n        # Ensure we have enough joint names\n        if len(joint_names) > action_tensor.shape[1]:\n            # Pad with zeros for missing joints\n            padded_action = torch.zeros(len(joint_names))\n            padded_action[:action_tensor.shape[1]] = action_tensor[0]\n            action_values = padded_action.numpy()\n        else:\n            # Truncate to match joint names\n            action_values = action_tensor[0, :len(joint_names)].numpy()\n\n        traj_msg.joint_names = joint_names\n\n        # Create trajectory point\n        point = JointTrajectoryPoint()\n        point.positions = action_values.tolist()\n        point.velocities = [0.0] * len(action_values)  # Zero velocity\n        point.accelerations = [0.0] * len(action_values)  # Zero acceleration\n\n        # Set timing (execute immediately)\n        point.time_from_start.sec = 0\n        point.time_from_start.nanosec = 50000000  # 50 ms\n\n        traj_msg.points = [point]\n\n        return traj_msg\n\n    def destroy_node(self):\n        \"\"\"Cleanup when node is destroyed\"\"\"\n        # Save action history to file\n        import json\n        with open('/tmp/action_history.json', 'w') as f:\n            json.dump(self.action_history, f, indent=2)\n\n        super().destroy_node()\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    action_mapping_node = ActionMappingNode()\n\n    try:\n        rclpy.spin(action_mapping_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        action_mapping_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(e.h2,{id:"implementation-steps",children:"Implementation Steps"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"Create the action mapping files in your workspace"}),"\n",(0,i.jsx)(e.li,{children:"Implement the action space converter for different representations"}),"\n",(0,i.jsx)(e.li,{children:"Build the action mapping neural network"}),"\n",(0,i.jsx)(e.li,{children:"Create the hierarchical action planner"}),"\n",(0,i.jsx)(e.li,{children:"Implement the policy gradient learner"}),"\n",(0,i.jsx)(e.li,{children:"Develop the action execution controller"}),"\n",(0,i.jsx)(e.li,{children:"Deploy the ROS 2 integration node"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"expected-outcomes",children:"Expected Outcomes"}),"\n",(0,i.jsx)(e.p,{children:"After completing this lab, you should have:"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"A working action space converter that can transform between joint and Cartesian spaces"}),"\n",(0,i.jsx)(e.li,{children:"An action mapping network that converts multimodal representations to motor commands"}),"\n",(0,i.jsx)(e.li,{children:"A hierarchical planner that breaks down high-level commands into executable actions"}),"\n",(0,i.jsx)(e.li,{children:"A reinforcement learning component for adaptive action selection"}),"\n",(0,i.jsx)(e.li,{children:"An execution controller that manages action sequencing and monitoring"}),"\n",(0,i.jsx)(e.li,{children:"A ROS 2 node that integrates action mapping with the robot control system"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"troubleshooting-tips",children:"Troubleshooting Tips"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"If actions are unstable, check action space bounds and normalization"}),"\n",(0,i.jsx)(e.li,{children:"If planning fails, verify waypoint generation and collision checking"}),"\n",(0,i.jsx)(e.li,{children:"If RL training is unstable, adjust learning rates and reward shaping"}),"\n",(0,i.jsx)(e.li,{children:"Monitor robot safety limits during execution"}),"\n",(0,i.jsx)(e.li,{children:"Verify joint limits and singularity avoidance"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"further-exploration",children:"Further Exploration"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Implement advanced motion planning algorithms (RRT*, CHOMP, TrajOpt)"}),"\n",(0,i.jsx)(e.li,{children:"Add tactile feedback integration for manipulation tasks"}),"\n",(0,i.jsx)(e.li,{children:"Create learned action priors using human demonstrations"}),"\n",(0,i.jsx)(e.li,{children:"Implement model predictive control for dynamic action adjustment"}),"\n"]})]})}function m(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(d,{...n})}):d(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>r,x:()=>s});var i=t(6540);const o={},a=i.createContext(o);function r(n){const e=i.useContext(a);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:r(n.components),i.createElement(a.Provider,{value:e},n.children)}}}]);