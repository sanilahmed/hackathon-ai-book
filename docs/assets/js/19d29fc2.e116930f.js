"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[5849],{5607:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>b,frontMatter:()=>a,metadata:()=>s,toc:()=>c});var o=r(4848),t=r(8453);const a={},i="Lab 3.4: Reinforcement Learning",s={id:"modules/lab-exercises/lab-3-4-reinforcement-learning",title:"Lab 3.4: Reinforcement Learning",description:"Reinforcement learning for robotics.",source:"@site/docs/modules/lab-exercises/lab-3-4-reinforcement-learning.md",sourceDirName:"modules/lab-exercises",slug:"/modules/lab-exercises/lab-3-4-reinforcement-learning",permalink:"/hackathon-ai-book/modules/lab-exercises/lab-3-4-reinforcement-learning",draft:!1,unlisted:!1,editUrl:"https://github.com/your-org/physical-ai-book/tree/main/docs/modules/lab-exercises/lab-3-4-reinforcement-learning.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Lab 3.3: Planning and Control",permalink:"/hackathon-ai-book/modules/lab-exercises/lab-3-3-planning-control"},next:{title:"Lab 3.5: Sim-to-Real Transfer",permalink:"/hackathon-ai-book/modules/lab-exercises/lab-3-5-sim2real-transfer"}},l={},c=[];function m(e){const n={h1:"h1",p:"p",...(0,t.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h1,{id:"lab-34-reinforcement-learning",children:"Lab 3.4: Reinforcement Learning"}),"\n",(0,o.jsx)(n.p,{children:"Reinforcement learning for robotics."})]})}function b(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(m,{...e})}):m(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>i,x:()=>s});var o=r(6540);const t={},a=o.createContext(t);function i(e){const n=o.useContext(a);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:i(e.components),o.createElement(a.Provider,{value:n},e.children)}}}]);