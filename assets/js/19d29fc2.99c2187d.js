"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[5849],{5607(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>_,frontMatter:()=>s,metadata:()=>i,toc:()=>c});var a=t(4848),r=t(8453);const s={sidebar_label:"Lab 3.4: Reinforcement Learning"},o="Lab Exercise 3.4: Reinforcement Learning in AI-Robot Brain",i={id:"modules/lab-exercises/lab-3-4-reinforcement-learning",title:"Lab Exercise 3.4: Reinforcement Learning in AI-Robot Brain",description:"This lab exercise covers implementing reinforcement learning for robot control using NVIDIA Isaac.",source:"@site/docs/modules/lab-exercises/lab-3-4-reinforcement-learning.md",sourceDirName:"modules/lab-exercises",slug:"/modules/lab-exercises/lab-3-4-reinforcement-learning",permalink:"/hackathon-ai-book/modules/lab-exercises/lab-3-4-reinforcement-learning",draft:!1,unlisted:!1,editUrl:"https://github.com/sanilahmed/hackathon-ai-book/tree/main/docs/modules/lab-exercises/lab-3-4-reinforcement-learning.md",tags:[],version:"current",frontMatter:{sidebar_label:"Lab 3.4: Reinforcement Learning"},sidebar:"tutorialSidebar",previous:{title:"Lab 3.3: Planning and Control",permalink:"/hackathon-ai-book/modules/lab-exercises/lab-3-3-planning-control"},next:{title:"Lab 3.5: Sim-to-Real Transfer",permalink:"/hackathon-ai-book/modules/lab-exercises/lab-3-5-sim2real-transfer"}},l={},c=[{value:"Objectives",id:"objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"RL Environment Setup",id:"rl-environment-setup",level:2},{value:"Isaac RL Environment Class",id:"isaac-rl-environment-class",level:3},{value:"Deep Q-Network (DQN) Implementation",id:"deep-q-network-dqn-implementation",level:2},{value:"DQN Neural Network",id:"dqn-neural-network",level:3},{value:"DQN Agent",id:"dqn-agent",level:3},{value:"Deep Deterministic Policy Gradient (DDPG)",id:"deep-deterministic-policy-gradient-ddpg",level:2},{value:"Actor and Critic Networks",id:"actor-and-critic-networks",level:3},{value:"DDPG Agent",id:"ddpg-agent",level:3},{value:"Isaac RL Training Loop",id:"isaac-rl-training-loop",level:2},{value:"Training Script",id:"training-script",level:3},{value:"Isaac Sim RL Integration",id:"isaac-sim-rl-integration",level:2},{value:"Isaac RL Training Node",id:"isaac-rl-training-node",level:3},{value:"NVIDIA Isaac Gym Integration",id:"nvidia-isaac-gym-integration",level:2},{value:"Isaac Gym Environment Wrapper",id:"isaac-gym-environment-wrapper",level:3},{value:"Exercise Tasks",id:"exercise-tasks",level:2},{value:"Evaluation Metrics",id:"evaluation-metrics",level:2},{value:"RL Performance Evaluation",id:"rl-performance-evaluation",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues",id:"common-issues",level:3},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h1,{id:"lab-exercise-34-reinforcement-learning-in-ai-robot-brain",children:"Lab Exercise 3.4: Reinforcement Learning in AI-Robot Brain"}),"\n",(0,a.jsx)(n.p,{children:"This lab exercise covers implementing reinforcement learning for robot control using NVIDIA Isaac."}),"\n",(0,a.jsx)(n.h2,{id:"objectives",children:"Objectives"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Set up RL environment in Isaac Sim"}),"\n",(0,a.jsx)(n.li,{children:"Implement DQN for discrete action spaces"}),"\n",(0,a.jsx)(n.li,{children:"Implement DDPG for continuous control"}),"\n",(0,a.jsx)(n.li,{children:"Train agents for navigation and manipulation tasks"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Isaac Sim installed and configured"}),"\n",(0,a.jsx)(n.li,{children:"ROS 2 Humble with necessary packages"}),"\n",(0,a.jsx)(n.li,{children:"Python with PyTorch or TensorFlow"}),"\n",(0,a.jsx)(n.li,{children:"Completed perception and control labs"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"rl-environment-setup",children:"RL Environment Setup"}),"\n",(0,a.jsx)(n.h3,{id:"isaac-rl-environment-class",children:"Isaac RL Environment Class"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.robots import Robot\nfrom omni.isaac.core.articulations import Articulation\nfrom omni.isaac.core.objects import DynamicCuboid\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport random\nfrom collections import deque\nimport gym\n\nclass IsaacRLEnvironment:\n    def __init__(self):\n        self.world = World(stage_units_in_meters=1.0)\n        self.robot = None\n        self.target = None\n        self.obstacles = []\n\n        # RL parameters\n        self.state_dim = 8  # Example: [robot_x, robot_y, robot_yaw, target_x, target_y, vel_x, vel_y, ang_vel]\n        self.action_dim = 4  # Example: [forward, backward, left, right] for discrete or [lin_vel, ang_vel] for continuous\n        self.max_steps = 500\n\n        self.setup_environment()\n\n    def setup_environment(self):\n        # Add ground plane\n        add_reference_to_stage(\n            usd_path="/Isaac/Environments/Simple_Room/simple_room.usd",\n            prim_path="/World/Room"\n        )\n\n        # Add robot (example with TurtleBot)\n        self.robot = self.world.scene.add(\n            Robot(\n                prim_path="/World/Robot",\n                name="turtlebot",\n                usd_path="/Isaac/Robots/Turtlebot/turtlebot3_differential.usd",\n                position=np.array([0.0, 0.0, 0.1])\n            )\n        )\n\n        # Add target object\n        self.target = self.world.scene.add(\n            DynamicCuboid(\n                prim_path="/World/Target",\n                name="target",\n                position=np.array([2.0, 2.0, 0.1]),\n                size=0.2,\n                color=np.array([0.0, 1.0, 0.0])\n            )\n        )\n\n        # Add obstacles\n        for i in range(3):\n            obstacle = self.world.scene.add(\n                DynamicCuboid(\n                    prim_path=f"/World/Obstacle_{i}",\n                    name=f"obstacle_{i}",\n                    position=np.array([1.0 + i*0.5, 0.5 + i*0.3, 0.1]),\n                    size=0.3,\n                    color=np.array([1.0, 0.0, 0.0])\n                )\n            )\n            self.obstacles.append(obstacle)\n\n    def reset(self):\n        """Reset the environment to initial state"""\n        self.world.reset()\n\n        # Reset robot position\n        self.robot.set_world_pose(position=np.array([0.0, 0.0, 0.1]))\n        self.robot.set_linear_velocity(np.array([0.0, 0.0, 0.0]))\n        self.robot.set_angular_velocity(np.array([0.0, 0.0, 0.0]))\n\n        # Reset target position\n        self.target.set_world_pose(position=np.array([2.0, 2.0, 0.1]))\n\n        # Get initial state\n        state = self.get_state()\n        return state\n\n    def get_state(self):\n        """Get current state of the environment"""\n        # Get robot position and orientation\n        robot_pos, robot_orn = self.robot.get_world_pose()\n        robot_lin_vel, robot_ang_vel = self.robot.get_linear_velocity(), self.robot.get_angular_velocity()\n\n        # Get target position\n        target_pos, _ = self.target.get_world_pose()\n\n        # Create state vector\n        state = np.array([\n            robot_pos[0],  # x\n            robot_pos[1],  # y\n            robot_orn[2],  # yaw (simplified)\n            target_pos[0], # target x\n            target_pos[1], # target y\n            robot_lin_vel[0], # linear vel x\n            robot_lin_vel[1], # linear vel y\n            robot_ang_vel[2]  # angular vel z\n        ])\n\n        return state\n\n    def step(self, action):\n        """Execute action and return (next_state, reward, done, info)"""\n        # Apply action to robot\n        self.apply_action(action)\n\n        # Step simulation\n        self.world.step(render=True)\n\n        # Get next state\n        next_state = self.get_state()\n\n        # Calculate reward\n        reward = self.calculate_reward()\n\n        # Check if episode is done\n        done = self.is_done()\n\n        # Additional info\n        info = {}\n\n        return next_state, reward, done, info\n\n    def apply_action(self, action):\n        """Apply action to the robot"""\n        # Example: discrete actions [forward, backward, left, right]\n        linear_vel = 0.0\n        angular_vel = 0.0\n\n        if action == 0:  # Forward\n            linear_vel = 0.5\n        elif action == 1:  # Backward\n            linear_vel = -0.5\n        elif action == 2:  # Left\n            angular_vel = 0.5\n        elif action == 3:  # Right\n            angular_vel = -0.5\n\n        # Apply velocities to robot (this is simplified - actual implementation depends on robot type)\n        # For differential drive robots, you would set wheel velocities\n        pass\n\n    def calculate_reward(self):\n        """Calculate reward based on current state"""\n        robot_pos, _ = self.robot.get_world_pose()\n        target_pos, _ = self.target.get_world_pose()\n\n        # Distance to target\n        dist_to_target = np.linalg.norm(robot_pos[:2] - target_pos[:2])\n\n        # Reward based on distance (closer = higher reward)\n        reward = -dist_to_target  # Negative because we want to minimize distance\n\n        # Bonus for reaching target\n        if dist_to_target < 0.3:\n            reward += 100\n            return reward\n\n        # Penalty for collisions (simplified check)\n        if self.check_collision():\n            reward -= 10\n\n        return reward\n\n    def check_collision(self):\n        """Check if robot is colliding with obstacles"""\n        # Simplified collision detection\n        robot_pos, _ = self.robot.get_world_pose()\n\n        for obstacle in self.obstacles:\n            obs_pos, _ = obstacle.get_world_pose()\n            dist = np.linalg.norm(robot_pos[:2] - obs_pos[:2])\n            if dist < 0.4:  # Collision threshold\n                return True\n\n        return False\n\n    def is_done(self):\n        """Check if episode is done"""\n        robot_pos, _ = self.robot.get_world_pose()\n        target_pos, _ = self.target.get_world_pose()\n\n        # Done if reached target or exceeded max steps\n        dist_to_target = np.linalg.norm(robot_pos[:2] - target_pos[:2])\n\n        if dist_to_target < 0.3:\n            return True\n\n        # Check if out of bounds\n        if abs(robot_pos[0]) > 10 or abs(robot_pos[1]) > 10:\n            return True\n\n        return False\n'})}),"\n",(0,a.jsx)(n.h2,{id:"deep-q-network-dqn-implementation",children:"Deep Q-Network (DQN) Implementation"}),"\n",(0,a.jsx)(n.h3,{id:"dqn-neural-network",children:"DQN Neural Network"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"class DQN(nn.Module):\n    def __init__(self, state_dim, action_dim, hidden_dim=128):\n        super(DQN, self).__init__()\n\n        self.network = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, action_dim)\n        )\n\n    def forward(self, state):\n        return self.network(state)\n"})}),"\n",(0,a.jsx)(n.h3,{id:"dqn-agent",children:"DQN Agent"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'class DQNAgent:\n    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.lr = lr\n        self.gamma = gamma\n        self.epsilon = epsilon\n        self.epsilon_decay = epsilon_decay\n        self.epsilon_min = epsilon_min\n\n        # Neural networks\n        self.q_network = DQN(state_dim, action_dim)\n        self.target_network = DQN(state_dim, action_dim)\n        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n\n        # Replay buffer\n        self.memory = deque(maxlen=10000)\n        self.batch_size = 32\n\n        # Update target network\n        self.update_target_network()\n\n    def update_target_network(self):\n        """Copy weights from main network to target network"""\n        self.target_network.load_state_dict(self.q_network.state_dict())\n\n    def remember(self, state, action, reward, next_state, done):\n        """Store experience in replay buffer"""\n        self.memory.append((state, action, reward, next_state, done))\n\n    def act(self, state):\n        """Choose action using epsilon-greedy policy"""\n        if np.random.random() <= self.epsilon:\n            return random.randrange(self.action_dim)\n\n        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n        q_values = self.q_network(state_tensor)\n        return np.argmax(q_values.cpu().data.numpy())\n\n    def replay(self):\n        """Train the model on a batch of experiences"""\n        if len(self.memory) < self.batch_size:\n            return\n\n        batch = random.sample(self.memory, self.batch_size)\n        states = torch.FloatTensor([e[0] for e in batch])\n        actions = torch.LongTensor([e[1] for e in batch])\n        rewards = torch.FloatTensor([e[2] for e in batch])\n        next_states = torch.FloatTensor([e[3] for e in batch])\n        dones = torch.BoolTensor([e[4] for e in batch])\n\n        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))\n        next_q_values = self.target_network(next_states).max(1)[0].detach()\n        target_q_values = rewards + (self.gamma * next_q_values * ~dones)\n\n        loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)\n\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n\n        # Decay epsilon\n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay\n'})}),"\n",(0,a.jsx)(n.h2,{id:"deep-deterministic-policy-gradient-ddpg",children:"Deep Deterministic Policy Gradient (DDPG)"}),"\n",(0,a.jsx)(n.h3,{id:"actor-and-critic-networks",children:"Actor and Critic Networks"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"class Actor(nn.Module):\n    def __init__(self, state_dim, action_dim, max_action, hidden_dim=256):\n        super(Actor, self).__init__()\n\n        self.network = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, action_dim),\n            nn.Tanh()\n        )\n\n        self.max_action = max_action\n\n    def forward(self, state):\n        return self.max_action * self.network(state)\n\nclass Critic(nn.Module):\n    def __init__(self, state_dim, action_dim, hidden_dim=256):\n        super(Critic, self).__init__()\n\n        self.network = nn.Sequential(\n            nn.Linear(state_dim + action_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)\n        )\n\n    def forward(self, state, action):\n        return self.network(torch.cat([state, action], dim=1))\n"})}),"\n",(0,a.jsx)(n.h3,{id:"ddpg-agent",children:"DDPG Agent"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'class DDPGAgent:\n    def __init__(self, state_dim, action_dim, max_action, lr_actor=1e-4, lr_critic=1e-3, gamma=0.99, tau=0.005):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.max_action = max_action\n        self.gamma = gamma\n        self.tau = tau\n\n        # Networks\n        self.actor = Actor(state_dim, action_dim, max_action)\n        self.actor_target = Actor(state_dim, action_dim, max_action)\n        self.critic = Critic(state_dim, action_dim)\n        self.critic_target = Critic(state_dim, action_dim)\n\n        # Optimizers\n        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor)\n        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_critic)\n\n        # Replay buffer\n        self.memory = deque(maxlen=100000)\n        self.batch_size = 100\n\n        # Initialize target networks\n        self.actor_target.load_state_dict(self.actor.state_dict())\n        self.critic_target.load_state_dict(self.critic.state_dict())\n\n    def update_target_networks(self):\n        """Soft update target networks"""\n        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):\n            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n\n        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n\n    def remember(self, state, action, reward, next_state, done):\n        """Store experience in replay buffer"""\n        self.memory.append((state, action, reward, next_state, done))\n\n    def act(self, state, add_noise=True):\n        """Choose action with optional noise for exploration"""\n        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n        action = self.actor(state_tensor).cpu().data.numpy().flatten()\n\n        if add_noise:\n            noise = np.random.normal(0, 0.1, size=self.action_dim)\n            action = action + noise\n            action = np.clip(action, -self.max_action, self.max_action)\n\n        return action\n\n    def replay(self):\n        """Train the networks on a batch of experiences"""\n        if len(self.memory) < self.batch_size:\n            return\n\n        batch = random.sample(self.memory, self.batch_size)\n        states = torch.FloatTensor([e[0] for e in batch])\n        actions = torch.FloatTensor([e[1] for e in batch])\n        rewards = torch.FloatTensor([e[2] for e in batch]).unsqueeze(1)\n        next_states = torch.FloatTensor([e[3] for e in batch])\n        dones = torch.BoolTensor([e[4] for e in batch]).unsqueeze(1)\n\n        # Critic update\n        next_actions = self.actor_target(next_states)\n        next_q_values = self.critic_target(next_states, next_actions)\n        target_q_values = rewards + (self.gamma * next_q_values * ~dones)\n\n        current_q_values = self.critic(states, actions)\n        critic_loss = nn.MSELoss()(current_q_values, target_q_values.detach())\n\n        self.critic_optimizer.zero_grad()\n        critic_loss.backward()\n        self.critic_optimizer.step()\n\n        # Actor update\n        actor_loss = -self.critic(states, self.actor(states)).mean()\n\n        self.actor_optimizer.zero_grad()\n        actor_loss.backward()\n        self.actor_optimizer.step()\n\n        # Update target networks\n        self.update_target_networks()\n'})}),"\n",(0,a.jsx)(n.h2,{id:"isaac-rl-training-loop",children:"Isaac RL Training Loop"}),"\n",(0,a.jsx)(n.h3,{id:"training-script",children:"Training Script"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'def train_dqn_agent():\n    # Create environment\n    env = IsaacRLEnvironment()\n\n    # Initialize agent\n    agent = DQNAgent(\n        state_dim=env.state_dim,\n        action_dim=env.action_dim,\n        lr=1e-3,\n        gamma=0.99,\n        epsilon=1.0,\n        epsilon_decay=0.995,\n        epsilon_min=0.01\n    )\n\n    # Training parameters\n    episodes = 1000\n    max_steps = 500\n    target_update_freq = 100\n\n    scores = []\n\n    for episode in range(episodes):\n        state = env.reset()\n        total_reward = 0\n\n        for step in range(max_steps):\n            # Choose action\n            action = agent.act(state)\n\n            # Take action\n            next_state, reward, done, _ = env.step(action)\n\n            # Store experience\n            agent.remember(state, action, reward, next_state, done)\n\n            # Train agent\n            agent.replay()\n\n            state = next_state\n            total_reward += reward\n\n            if done:\n                break\n\n        scores.append(total_reward)\n\n        # Update target network periodically\n        if episode % target_update_freq == 0:\n            agent.update_target_network()\n\n        # Print progress\n        if episode % 100 == 0:\n            avg_score = np.mean(scores[-100:])\n            print(f"Episode {episode}, Average Score: {avg_score:.2f}, Epsilon: {agent.epsilon:.3f}")\n\n    return agent, scores\n\ndef train_ddpg_agent():\n    # Create continuous environment (modify IsaacRLEnvironment for continuous actions)\n    env = IsaacRLEnvironment()\n\n    # Initialize DDPG agent\n    agent = DDPGAgent(\n        state_dim=env.state_dim,\n        action_dim=2,  # [linear_vel, angular_vel]\n        max_action=1.0\n    )\n\n    # Training parameters\n    episodes = 1000\n    max_steps = 500\n\n    scores = []\n\n    for episode in range(episodes):\n        state = env.reset()\n        total_reward = 0\n\n        for step in range(max_steps):\n            # Choose action with noise for exploration\n            action = agent.act(state, add_noise=True)\n\n            # Take action\n            next_state, reward, done, _ = env.step(action)\n\n            # Store experience\n            agent.remember(state, action, reward, next_state, done)\n\n            # Train agent\n            agent.replay()\n\n            state = next_state\n            total_reward += reward\n\n            if done:\n                break\n\n        scores.append(total_reward)\n\n        # Print progress\n        if episode % 100 == 0:\n            avg_score = np.mean(scores[-100:])\n            print(f"Episode {episode}, Average Score: {avg_score:.2f}")\n\n    return agent, scores\n'})}),"\n",(0,a.jsx)(n.h2,{id:"isaac-sim-rl-integration",children:"Isaac Sim RL Integration"}),"\n",(0,a.jsx)(n.h3,{id:"isaac-rl-training-node",children:"Isaac RL Training Node"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import Float32, Bool\nfrom geometry_msgs.msg import Twist\nimport numpy as np\n\nclass IsaacRLTrainingNode(Node):\n    def __init__(self):\n        super().__init__(\'isaac_rl_training\')\n\n        # Publishers\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n        self.reward_pub = self.create_publisher(Float32, \'/rl/reward\', 10)\n        self.done_pub = self.create_publisher(Bool, \'/rl/done\', 10)\n\n        # RL agent\n        self.rl_agent = None\n        self.current_state = None\n        self.episode_step = 0\n        self.max_episode_steps = 500\n\n        # Timer for RL control loop\n        self.control_timer = self.create_timer(0.1, self.rl_control_loop)\n\n        # Initialize RL agent\n        self.initialize_agent()\n\n    def initialize_agent(self):\n        """Initialize the RL agent"""\n        # Initialize your RL agent here\n        # This would typically load a pre-trained model or create a new one\n        pass\n\n    def rl_control_loop(self):\n        """Main RL control loop"""\n        if self.rl_agent is None:\n            return\n\n        # Get current state from sensors (this would come from perception system)\n        current_state = self.get_current_state()\n\n        if current_state is not None:\n            # Get action from RL agent\n            action = self.rl_agent.act(current_state)\n\n            # Convert action to robot command\n            cmd_vel = self.convert_action_to_cmd_vel(action)\n\n            # Publish command\n            self.cmd_vel_pub.publish(cmd_vel)\n\n            # Store state for next iteration\n            self.current_state = current_state\n            self.episode_step += 1\n\n    def get_current_state(self):\n        """Get current state from perception system"""\n        # This would integrate with perception system to get robot state\n        # For example: [robot_x, robot_y, robot_yaw, target_x, target_y, obstacles_distances, ...]\n        return None  # Placeholder\n\n    def convert_action_to_cmd_vel(self, action):\n        """Convert RL action to Twist message"""\n        cmd_vel = Twist()\n\n        # Example: action is [linear_vel, angular_vel] for continuous control\n        if len(action) >= 2:\n            cmd_vel.linear.x = float(action[0])\n            cmd_vel.angular.z = float(action[1])\n\n        return cmd_vel\n\n    def calculate_reward(self):\n        """Calculate reward based on current state and goal"""\n        # Implement reward calculation based on task\n        # For navigation: reward based on distance to goal\n        # For manipulation: reward based on object grasp success\n        return 0.0  # Placeholder\n'})}),"\n",(0,a.jsx)(n.h2,{id:"nvidia-isaac-gym-integration",children:"NVIDIA Isaac Gym Integration"}),"\n",(0,a.jsx)(n.h3,{id:"isaac-gym-environment-wrapper",children:"Isaac Gym Environment Wrapper"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from omni.isaac.gym.vec_env import VecEnvBase\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nimport torch\nimport numpy as np\n\nclass IsaacGymEnv(VecEnvBase):\n    def __init__(self, headless):\n        super().__init__(headless=headless)\n\n        self._world = World(stage_units_in_meters=1.0)\n        self._setup_scene()\n\n        self.set_defaults()\n\n    def _setup_scene(self):\n        """Setup the scene with robots and objects"""\n        # Add your specific scene setup here\n        pass\n\n    def set_defaults(self):\n        """Set default values for observations, actions, etc."""\n        # Define action and observation spaces\n        self.num_actions = 2  # Example: linear and angular velocity\n        self.num_observations = 8  # Example: various state parameters\n\n    def reset(self):\n        """Reset the environment"""\n        # Reset robot positions, targets, etc.\n        self._world.reset()\n\n        # Return observations\n        obs = torch.zeros((self.num_envs, self.num_observations), device=self._device)\n        return obs\n\n    def step(self, actions):\n        """Execute actions and return results"""\n        # Apply actions to robots\n        # Step simulation\n        # Calculate rewards\n        # Check if done\n        # Return (observations, rewards, dones, info)\n\n        obs = torch.zeros((self.num_envs, self.num_observations), device=self._device)\n        rewards = torch.zeros(self.num_envs, device=self._device)\n        dones = torch.zeros(self.num_envs, dtype=torch.bool, device=self._device)\n        info = {}\n\n        return obs, rewards, dones, info\n'})}),"\n",(0,a.jsx)(n.h2,{id:"exercise-tasks",children:"Exercise Tasks"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Set up the Isaac RL environment with proper state and action spaces"}),"\n",(0,a.jsx)(n.li,{children:"Implement the DQN algorithm and train it for a simple navigation task"}),"\n",(0,a.jsx)(n.li,{children:"Implement the DDPG algorithm for continuous control"}),"\n",(0,a.jsx)(n.li,{children:"Create a training loop that runs in Isaac Sim"}),"\n",(0,a.jsx)(n.li,{children:"Evaluate the trained agent's performance"}),"\n",(0,a.jsx)(n.li,{children:"Compare discrete vs. continuous action spaces for your specific task"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"evaluation-metrics",children:"Evaluation Metrics"}),"\n",(0,a.jsx)(n.h3,{id:"rl-performance-evaluation",children:"RL Performance Evaluation"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'class RLEvaluation:\n    def __init__(self):\n        self.episode_rewards = []\n        self.episode_lengths = []\n        self.success_rates = []\n\n    def evaluate_agent(self, agent, env, num_episodes=100):\n        """Evaluate trained agent"""\n        total_rewards = []\n        episode_lengths = []\n        successes = 0\n\n        for episode in range(num_episodes):\n            state = env.reset()\n            total_reward = 0\n            step_count = 0\n            done = False\n\n            while not done and step_count < env.max_steps:\n                action = agent.act(state, add_noise=False)  # No exploration noise during evaluation\n                state, reward, done, _ = env.step(action)\n                total_reward += reward\n                step_count += 1\n\n            total_rewards.append(total_reward)\n            episode_lengths.append(step_count)\n\n            # Check if task was completed successfully\n            if self.check_success(state):\n                successes += 1\n\n        success_rate = successes / num_episodes\n\n        return {\n            \'avg_reward\': np.mean(total_rewards),\n            \'std_reward\': np.std(total_rewards),\n            \'avg_length\': np.mean(episode_lengths),\n            \'success_rate\': success_rate\n        }\n\n    def check_success(self, state):\n        """Check if the task was completed successfully"""\n        # Define success criteria based on your task\n        return False  # Placeholder\n'})}),"\n",(0,a.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,a.jsx)(n.h3,{id:"common-issues",children:"Common Issues"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Training instability"}),": Adjust learning rates and add experience replay"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Slow convergence"}),": Increase network capacity or adjust hyperparameters"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Exploration issues"}),": Tune epsilon decay or noise parameters"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Isaac Sim integration"}),": Check GPU memory and simulation parameters"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(n.p,{children:"In this lab, you learned to implement reinforcement learning for robot control using NVIDIA Isaac. You created DQN and DDPG agents, set up RL environments in Isaac Sim, and trained agents for navigation tasks. These techniques are fundamental for developing adaptive robotic systems."})]})}function _(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453(e,n,t){t.d(n,{R:()=>o,x:()=>i});var a=t(6540);const r={},s=a.createContext(r);function o(e){const n=a.useContext(s);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),a.createElement(s.Provider,{value:n},e.children)}}}]);