"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[4972],{4563(e,n,s){s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>t,metadata:()=>r,toc:()=>d});var i=s(4848),a=s(8453);const t={sidebar_label:"Lab 4.1: VLA Fundamentals"},o="Lab Exercise 4.1: Vision-Language-Action Fundamentals",r={id:"modules/lab-exercises/lab-4-1-vla-fundamentals",title:"Lab Exercise 4.1: Vision-Language-Action Fundamentals",description:"This lab exercise covers the fundamental concepts of Vision-Language-Action systems and their implementation.",source:"@site/docs/modules/lab-exercises/lab-4-1-vla-fundamentals.md",sourceDirName:"modules/lab-exercises",slug:"/modules/lab-exercises/lab-4-1-vla-fundamentals",permalink:"/hackathon-ai-book/modules/lab-exercises/lab-4-1-vla-fundamentals",draft:!1,unlisted:!1,editUrl:"https://github.com/sanilahmed/hackathon-ai-book/tree/main/docs/modules/lab-exercises/lab-4-1-vla-fundamentals.md",tags:[],version:"current",frontMatter:{sidebar_label:"Lab 4.1: VLA Fundamentals"},sidebar:"tutorialSidebar",previous:{title:"VLA Integration",permalink:"/hackathon-ai-book/modules/vla-system/vla-integration"},next:{title:"Lab 4.2: Multimodal Perception",permalink:"/hackathon-ai-book/modules/lab-exercises/lab-4-2-multimodal-perception"}},l={},d=[{value:"Objectives",id:"objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"VLA System Architecture",id:"vla-system-architecture",level:2},{value:"Multi-Modal Integration",id:"multi-modal-integration",level:3},{value:"Basic VLA Pipeline",id:"basic-vla-pipeline",level:3},{value:"Vision Processing Component",id:"vision-processing-component",level:2},{value:"Vision Encoder Implementation",id:"vision-encoder-implementation",level:3},{value:"Language Processing Component",id:"language-processing-component",level:2},{value:"Language Encoder Implementation",id:"language-encoder-implementation",level:3},{value:"Multi-Modal Fusion",id:"multi-modal-fusion",level:2},{value:"Cross-Modal Attention Implementation",id:"cross-modal-attention-implementation",level:3},{value:"Action Generation Component",id:"action-generation-component",level:2},{value:"Action Decoder Implementation",id:"action-decoder-implementation",level:3},{value:"Complete VLA System",id:"complete-vla-system",level:2},{value:"Integrated VLA Model",id:"integrated-vla-model",level:3},{value:"ROS 2 Integration",id:"ros-2-integration",level:2},{value:"VLA ROS Node",id:"vla-ros-node",level:3},{value:"Example Usage and Testing",id:"example-usage-and-testing",level:2},{value:"VLA Testing Script",id:"vla-testing-script",level:3},{value:"Exercise Tasks",id:"exercise-tasks",level:2},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues",id:"common-issues",level:3},{value:"Summary",id:"summary",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h1,{id:"lab-exercise-41-vision-language-action-fundamentals",children:"Lab Exercise 4.1: Vision-Language-Action Fundamentals"}),"\n",(0,i.jsx)(n.p,{children:"This lab exercise covers the fundamental concepts of Vision-Language-Action systems and their implementation."}),"\n",(0,i.jsx)(n.h2,{id:"objectives",children:"Objectives"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Understand the multi-modal integration in VLA systems"}),"\n",(0,i.jsx)(n.li,{children:"Implement basic vision-language fusion"}),"\n",(0,i.jsx)(n.li,{children:"Create simple action generation from language commands"}),"\n",(0,i.jsx)(n.li,{children:"Test VLA pipeline with basic commands"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Python with PyTorch/TensorFlow"}),"\n",(0,i.jsx)(n.li,{children:"Basic understanding of computer vision and NLP"}),"\n",(0,i.jsx)(n.li,{children:"ROS 2 Humble with necessary packages"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"vla-system-architecture",children:"VLA System Architecture"}),"\n",(0,i.jsx)(n.h3,{id:"multi-modal-integration",children:"Multi-Modal Integration"}),"\n",(0,i.jsx)(n.p,{children:"Vision-Language-Action systems integrate three key modalities:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Vision"}),": Understanding visual scene information"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Language"}),": Processing natural language commands"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Action"}),": Executing appropriate robot behaviors"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"basic-vla-pipeline",children:"Basic VLA Pipeline"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Visual Input \u2192 Vision Encoder \u2192 Multi-Modal Fusion \u2192 Language Encoder \u2192 Action Decoder \u2192 Robot Actions\n"})}),"\n",(0,i.jsx)(n.h2,{id:"vision-processing-component",children:"Vision Processing Component"}),"\n",(0,i.jsx)(n.h3,{id:"vision-encoder-implementation",children:"Vision Encoder Implementation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\nimport torchvision.transforms as T\nfrom torchvision.models import resnet50\nimport clip\n\nclass VisionEncoder(nn.Module):\n    def __init__(self, model_type='resnet', pretrained=True):\n        super().__init__()\n\n        if model_type == 'resnet':\n            self.backbone = resnet50(pretrained=pretrained)\n            self.feature_dim = 2048\n            # Remove the final classification layer\n            self.backbone.fc = nn.Identity()\n        elif model_type == 'clip':\n            self.backbone, _ = clip.load(\"ViT-B/32\", device='cpu')\n            self.feature_dim = 512\n\n        self.normalize = T.Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225]\n        )\n\n    def forward(self, images):\n        if isinstance(self.backbone, type(clip.load(\"ViT-B/32\", device='cpu')[0])):\n            # CLIP model\n            features = self.backbone.encode_image(images)\n        else:\n            # ResNet or other model\n            features = self.backbone(images)\n\n        return features\n\nclass VisionProcessor:\n    def __init__(self, model_type='resnet'):\n        self.encoder = VisionEncoder(model_type)\n        self.transform = T.Compose([\n            T.Resize((224, 224)),\n            T.ToTensor(),\n            self.encoder.normalize\n        ])\n\n    def process_image(self, image):\n        \"\"\"Process a single image and extract features\"\"\"\n        transformed_image = self.transform(image).unsqueeze(0)  # Add batch dimension\n        features = self.encoder(transformed_image)\n        return features\n"})}),"\n",(0,i.jsx)(n.h2,{id:"language-processing-component",children:"Language Processing Component"}),"\n",(0,i.jsx)(n.h3,{id:"language-encoder-implementation",children:"Language Encoder Implementation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\nfrom transformers import AutoTokenizer, AutoModel\nimport clip\n\nclass LanguageEncoder(nn.Module):\n    def __init__(self, model_name='bert-base-uncased', use_clip=False):\n        super().__init__()\n\n        self.use_clip = use_clip\n\n        if use_clip:\n            # Use CLIP's text encoder\n            clip_model, _ = clip.load(\"ViT-B/32\", device='cpu')\n            self.backbone = clip_model.encode_text\n            self.feature_dim = 512\n        else:\n            # Use transformer model\n            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n            self.backbone = AutoModel.from_pretrained(model_name)\n            self.feature_dim = self.backbone.config.hidden_size\n\n    def forward(self, text_inputs):\n        if self.use_clip:\n            # CLIP expects tokenized text\n            if isinstance(text_inputs, list):\n                # Tokenize multiple texts\n                texts = clip.tokenize(text_inputs)\n                return self.backbone(texts)\n            else:\n                # Single text\n                text = clip.tokenize([text_inputs])\n                return self.backbone(text)\n        else:\n            # Transformer model\n            outputs = self.backbone(**text_inputs)\n            # Use [CLS] token representation\n            return outputs.last_hidden_state[:, 0, :]\n\nclass LanguageProcessor:\n    def __init__(self, model_name='bert-base-uncased', use_clip=False):\n        self.encoder = LanguageEncoder(model_name, use_clip)\n        self.use_clip = use_clip\n\n        if not use_clip:\n            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n    def process_text(self, text):\n        \"\"\"Process text and extract features\"\"\"\n        if self.use_clip:\n            # CLIP handles tokenization internally\n            features = self.encoder(text)\n        else:\n            # Tokenize for transformer\n            inputs = self.tokenizer(\n                text,\n                return_tensors='pt',\n                padding=True,\n                truncation=True,\n                max_length=128\n            )\n            features = self.encoder(inputs)\n\n        return features\n"})}),"\n",(0,i.jsx)(n.h2,{id:"multi-modal-fusion",children:"Multi-Modal Fusion"}),"\n",(0,i.jsx)(n.h3,{id:"cross-modal-attention-implementation",children:"Cross-Modal Attention Implementation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\n\nclass CrossModalAttention(nn.Module):\n    def __init__(self, feature_dim, num_heads=8):\n        super().__init__()\n\n        self.feature_dim = feature_dim\n        self.num_heads = num_heads\n        self.head_dim = feature_dim // num_heads\n\n        assert self.head_dim * num_heads == feature_dim, "Feature dim must be divisible by num heads"\n\n        # Query, Key, Value projections for vision and language\n        self.vision_q = nn.Linear(feature_dim, feature_dim)\n        self.vision_k = nn.Linear(feature_dim, feature_dim)\n        self.vision_v = nn.Linear(feature_dim, feature_dim)\n\n        self.language_q = nn.Linear(feature_dim, feature_dim)\n        self.language_k = nn.Linear(feature_dim, feature_dim)\n        self.language_v = nn.Linear(feature_dim, feature_dim)\n\n        self.out_proj = nn.Linear(feature_dim, feature_dim)\n        self.dropout = nn.Dropout(0.1)\n\n    def forward(self, vision_features, language_features):\n        batch_size = vision_features.size(0)\n\n        # Project features\n        v_q = self.vision_q(vision_features).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        v_k = self.vision_k(vision_features).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        v_v = self.vision_v(vision_features).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n\n        l_q = self.language_q(language_features).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        l_k = self.language_k(language_features).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        l_v = self.language_v(language_features).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n\n        # Cross-attention: vision attending to language\n        v_attn = torch.matmul(v_q, l_k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n        v_attn = torch.softmax(v_attn, dim=-1)\n        v_attn = self.dropout(v_attn)\n        v_out = torch.matmul(v_attn, l_v)\n        v_out = v_out.transpose(1, 2).contiguous().view(batch_size, -1, self.feature_dim)\n\n        # Cross-attention: language attending to vision\n        l_attn = torch.matmul(l_q, v_k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n        l_attn = torch.softmax(l_attn, dim=-1)\n        l_attn = self.dropout(l_attn)\n        l_out = torch.matmul(l_attn, v_v)\n        l_out = l_out.transpose(1, 2).contiguous().view(batch_size, -1, self.feature_dim)\n\n        # Combine outputs\n        combined_features = torch.cat([v_out, l_out], dim=-1)\n        output = self.out_proj(combined_features)\n\n        return output\n\nclass MultiModalFusion(nn.Module):\n    def __init__(self, vision_feature_dim, language_feature_dim, fusion_dim=512):\n        super().__init__()\n\n        self.vision_proj = nn.Linear(vision_feature_dim, fusion_dim)\n        self.language_proj = nn.Linear(language_feature_dim, fusion_dim)\n\n        # Cross-modal attention\n        self.cross_attention = CrossModalAttention(fusion_dim)\n\n        # Fusion layers\n        self.fusion_layers = nn.Sequential(\n            nn.Linear(fusion_dim * 2, fusion_dim),\n            nn.ReLU(),\n            nn.Linear(fusion_dim, fusion_dim)\n        )\n\n    def forward(self, vision_features, language_features):\n        # Project features to common space\n        vision_proj = self.vision_proj(vision_features)\n        language_proj = self.language_proj(language_features)\n\n        # Apply cross-modal attention\n        attended_features = self.cross_attention(vision_proj, language_proj)\n\n        # Global average pooling if needed\n        if len(attended_features.shape) > 2:\n            attended_features = attended_features.mean(dim=1)\n\n        # Final fusion\n        fused_features = self.fusion_layers(attended_features)\n\n        return fused_features\n'})}),"\n",(0,i.jsx)(n.h2,{id:"action-generation-component",children:"Action Generation Component"}),"\n",(0,i.jsx)(n.h3,{id:"action-decoder-implementation",children:"Action Decoder Implementation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\n\nclass ActionDecoder(nn.Module):\n    def __init__(self, fusion_dim, action_space_dim, hidden_dim=512):\n        super().__init__()\n\n        self.action_network = nn.Sequential(\n            nn.Linear(fusion_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, action_space_dim)\n        )\n\n        # For continuous action spaces (e.g., robot joint commands)\n        self.continuous_head = nn.Sequential(\n            nn.Linear(action_space_dim, action_space_dim),\n            nn.Tanh()  # Keep actions in reasonable range\n        )\n\n        # For discrete action spaces (e.g., navigation commands)\n        self.discrete_head = nn.Linear(action_space_dim, action_space_dim)\n\n    def forward(self, fused_features, action_type='continuous'):\n        action_features = self.action_network(fused_features)\n\n        if action_type == 'continuous':\n            actions = self.continuous_head(action_features)\n        else:  # discrete\n            actions = self.discrete_head(action_features)\n            actions = torch.softmax(actions, dim=-1)\n\n        return actions\n\nclass ActionProcessor:\n    def __init__(self, fusion_dim, action_space_dim):\n        self.decoder = ActionDecoder(fusion_dim, action_space_dim)\n\n    def generate_action(self, fused_features, action_type='continuous'):\n        \"\"\"Generate action from fused features\"\"\"\n        return self.decoder(fused_features, action_type)\n"})}),"\n",(0,i.jsx)(n.h2,{id:"complete-vla-system",children:"Complete VLA System"}),"\n",(0,i.jsx)(n.h3,{id:"integrated-vla-model",children:"Integrated VLA Model"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\n\nclass VLAModel(nn.Module):\n    def __init__(self,\n                 vision_feature_dim=2048,\n                 language_feature_dim=768,\n                 fusion_dim=512,\n                 action_space_dim=4):\n        super().__init__()\n\n        # Components\n        self.vision_encoder = VisionEncoder()\n        self.language_encoder = LanguageEncoder()\n        self.fusion_module = MultiModalFusion(\n            vision_feature_dim,\n            language_feature_dim,\n            fusion_dim\n        )\n        self.action_decoder = ActionDecoder(fusion_dim, action_space_dim)\n\n        # Dimension parameters\n        self.action_space_dim = action_space_dim\n\n    def forward(self, images, texts, action_type=\'continuous\'):\n        # Encode vision\n        vision_features = self.vision_encoder(images)\n\n        # Encode language\n        language_features = self.language_encoder(texts)\n\n        # Fuse modalities\n        fused_features = self.fusion_module(vision_features, language_features)\n\n        # Generate action\n        actions = self.action_decoder(fused_features, action_type)\n\n        return actions\n\n    def process_command(self, image, command_text, action_type=\'continuous\'):\n        """Process a single image-command pair"""\n        # Add batch dimension\n        image_batch = image.unsqueeze(0) if len(image.shape) == 3 else image\n        text_batch = [command_text] if isinstance(command_text, str) else command_text\n\n        # Forward pass\n        actions = self.forward(image_batch, text_batch, action_type)\n\n        return actions.squeeze(0)  # Remove batch dimension\n\nclass VLAProcessor:\n    def __init__(self, action_space_dim=4):\n        self.vla_model = VLAModel(action_space_dim=action_space_dim)\n        self.vision_processor = VisionProcessor()\n        self.language_processor = LanguageProcessor()\n\n    def execute_command(self, image, command):\n        """Execute a vision-language command"""\n        # Process image\n        vision_features = self.vision_processor.process_image(image)\n\n        # Process language command\n        language_features = self.language_processor.process_text(command)\n\n        # Generate action through the model\n        action = self.vla_model(image.unsqueeze(0), [command])\n\n        return action\n'})}),"\n",(0,i.jsx)(n.h2,{id:"ros-2-integration",children:"ROS 2 Integration"}),"\n",(0,i.jsx)(n.h3,{id:"vla-ros-node",children:"VLA ROS Node"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nfrom cv_bridge import CvBridge\nimport torch\nfrom PIL import Image as PILImage\n\nclass VLAROSNode(Node):\n    def __init__(self):\n        super().__init__(\'vla_ros_node\')\n\n        # Initialize CV bridge\n        self.cv_bridge = CvBridge()\n\n        # Publishers and subscribers\n        self.image_sub = self.create_subscription(\n            Image, \'/camera/image_raw\', self.image_callback, 10\n        )\n        self.command_sub = self.create_subscription(\n            String, \'/vla/command\', self.command_callback, 10\n        )\n        self.action_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n\n        # Initialize VLA processor\n        self.vla_processor = VLAProcessor(action_space_dim=2)  # [linear_x, angular_z]\n\n        # Store latest image and command\n        self.latest_image = None\n        self.pending_command = None\n\n    def image_callback(self, msg):\n        """Process incoming image"""\n        try:\n            # Convert ROS image to PIL image\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding=\'bgr8\')\n            pil_image = PILImage.fromarray(cv_image)\n\n            # Store for processing with command\n            self.latest_image = pil_image\n\n            # If we have a pending command, process now\n            if self.pending_command:\n                self.process_vla_request()\n\n        except Exception as e:\n            self.get_logger().error(f\'Error processing image: {e}\')\n\n    def command_callback(self, msg):\n        """Process incoming command"""\n        self.pending_command = msg.data\n\n        # If we have a recent image, process now\n        if self.latest_image:\n            self.process_vla_request()\n\n    def process_vla_request(self):\n        """Process image-command pair"""\n        if self.latest_image is None or self.pending_command is None:\n            return\n\n        try:\n            # Convert PIL image to tensor\n            transform = self.vla_processor.vision_processor.transform\n            image_tensor = transform(self.latest_image)\n\n            # Process with VLA model\n            action = self.vla_processor.execute_command(\n                image_tensor,\n                self.pending_command\n            )\n\n            # Convert action to robot command (Twist message)\n            cmd_vel = self.convert_action_to_twist(action)\n\n            # Publish command\n            self.action_pub.publish(cmd_vel)\n\n            # Clear pending command\n            self.pending_command = None\n\n            self.get_logger().info(f\'Executed command: {self.pending_command}\')\n\n        except Exception as e:\n            self.get_logger().error(f\'Error processing VLA request: {e}\')\n\n    def convert_action_to_twist(self, action_tensor):\n        """Convert action tensor to Twist message"""\n        cmd_vel = Twist()\n\n        # Assuming action tensor has [linear_x, angular_z]\n        action_values = action_tensor.detach().cpu().numpy()\n\n        cmd_vel.linear.x = float(action_values[0]) if len(action_values) > 0 else 0.0\n        cmd_vel.angular.z = float(action_values[1]) if len(action_values) > 1 else 0.0\n\n        return cmd_vel\n'})}),"\n",(0,i.jsx)(n.h2,{id:"example-usage-and-testing",children:"Example Usage and Testing"}),"\n",(0,i.jsx)(n.h3,{id:"vla-testing-script",children:"VLA Testing Script"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def test_vla_system():\n    """Test the VLA system with sample inputs"""\n    import numpy as np\n    from PIL import Image as PILImage\n\n    # Initialize VLA processor\n    vla_processor = VLAProcessor(action_space_dim=2)\n\n    # Create a dummy image (in practice, this would come from camera)\n    dummy_image = PILImage.new(\'RGB\', (224, 224), color=\'red\')\n\n    # Test commands\n    test_commands = [\n        "move forward",\n        "turn left",\n        "go to the blue object",\n        "navigate to the kitchen"\n    ]\n\n    print("Testing VLA System:")\n    for command in test_commands:\n        print(f"\\nCommand: \'{command}\'")\n\n        # Process with VLA\n        action = vla_processor.execute_command(dummy_image, command)\n        print(f"Generated action: {action.detach().cpu().numpy()}")\n\ndef main():\n    rclpy.init()\n\n    # Create and run VLA node\n    vla_node = VLAROSNode()\n\n    try:\n        rclpy.spin(vla_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        vla_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"exercise-tasks",children:"Exercise Tasks"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Implement the Vision, Language, and Action components of the VLA system"}),"\n",(0,i.jsx)(n.li,{children:"Create the multi-modal fusion module with cross-attention"}),"\n",(0,i.jsx)(n.li,{children:"Integrate the components into a complete VLA model"}),"\n",(0,i.jsx)(n.li,{children:"Test the system with simple image-command pairs"}),"\n",(0,i.jsx)(n.li,{children:"Create a ROS 2 node that processes VLA commands"}),"\n",(0,i.jsx)(n.li,{children:"Evaluate the system's ability to follow basic commands"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,i.jsx)(n.h3,{id:"common-issues",children:"Common Issues"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Dimension mismatches"}),": Ensure all feature dimensions align between components"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Memory issues"}),": Use appropriate batch sizes for your GPU memory"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Tokenization problems"}),": Handle text preprocessing carefully"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"ROS integration"}),": Verify message types and topic names"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(n.p,{children:"In this lab, you learned to implement the fundamental components of a Vision-Language-Action system. You created vision and language encoders, implemented multi-modal fusion with cross-attention, and connected the system to robot action generation. This forms the core architecture for advanced robotic systems that can understand and execute natural language commands based on visual input."})]})}function m(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},8453(e,n,s){s.d(n,{R:()=>o,x:()=>r});var i=s(6540);const a={},t=i.createContext(a);function o(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);