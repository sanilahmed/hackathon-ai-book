"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[7618],{5156(e,n,t){t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>d,frontMatter:()=>o,metadata:()=>r,toc:()=>l});var a=t(4848),i=t(8453);const o={sidebar_label:"Lab 4.3: Action Mapping"},s="Lab Exercise 4.3: Action Mapping in VLA Systems",r={id:"modules/lab-exercises/lab-4-3-action-mapping",title:"Lab Exercise 4.3: Action Mapping in VLA Systems",description:"This lab exercise covers implementing action mapping systems that translate multimodal perceptions into executable robot actions.",source:"@site/docs/modules/lab-exercises/lab-4-3-action-mapping.md",sourceDirName:"modules/lab-exercises",slug:"/modules/lab-exercises/lab-4-3-action-mapping",permalink:"/hackathon-ai-book/modules/lab-exercises/lab-4-3-action-mapping",draft:!1,unlisted:!1,editUrl:"https://github.com/sanilahmed/hackathon-ai-book/tree/main/docs/modules/lab-exercises/lab-4-3-action-mapping.md",tags:[],version:"current",frontMatter:{sidebar_label:"Lab 4.3: Action Mapping"},sidebar:"tutorialSidebar",previous:{title:"Lab 4.2: Multimodal Perception",permalink:"/hackathon-ai-book/modules/lab-exercises/lab-4-2-multimodal-perception"},next:{title:"References",permalink:"/hackathon-ai-book/modules/vla-system/references"}},c={},l=[{value:"Objectives",id:"objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Action Mapping Architecture",id:"action-mapping-architecture",level:2},{value:"Language-Action Translation Module",id:"language-action-translation-module",level:3},{value:"Task Planning Module",id:"task-planning-module",level:2},{value:"Hierarchical Task Planner",id:"hierarchical-task-planner",level:3},{value:"Action Execution System",id:"action-execution-system",level:2},{value:"Robot Action Executor",id:"robot-action-executor",level:3},{value:"Closed-Loop VLA System",id:"closed-loop-vla-system",level:2},{value:"Complete VLA Integration",id:"complete-vla-integration",level:3},{value:"VLA Integration Node",id:"vla-integration-node",level:2},{value:"ROS 2 VLA Node",id:"ros-2-vla-node",level:3},{value:"Action Learning and Adaptation",id:"action-learning-and-adaptation",level:2},{value:"Online Action Learning",id:"online-action-learning",level:3},{value:"Performance Evaluation",id:"performance-evaluation",level:2},{value:"VLA System Evaluation",id:"vla-system-evaluation",level:3},{value:"Exercise Tasks",id:"exercise-tasks",level:2},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues",id:"common-issues",level:3},{value:"Summary",id:"summary",level:2}];function p(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h1,{id:"lab-exercise-43-action-mapping-in-vla-systems",children:"Lab Exercise 4.3: Action Mapping in VLA Systems"}),"\n",(0,a.jsx)(n.p,{children:"This lab exercise covers implementing action mapping systems that translate multimodal perceptions into executable robot actions."}),"\n",(0,a.jsx)(n.h2,{id:"objectives",children:"Objectives"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Implement language-to-action mapping"}),"\n",(0,a.jsx)(n.li,{children:"Create task planning from multimodal inputs"}),"\n",(0,a.jsx)(n.li,{children:"Integrate action execution with perception"}),"\n",(0,a.jsx)(n.li,{children:"Test closed-loop VLA system performance"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Completed multimodal perception lab"}),"\n",(0,a.jsx)(n.li,{children:"Understanding of robot control and planning"}),"\n",(0,a.jsx)(n.li,{children:"ROS 2 Humble with control packages"}),"\n",(0,a.jsx)(n.li,{children:"PyTorch/TensorFlow knowledge"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"action-mapping-architecture",children:"Action Mapping Architecture"}),"\n",(0,a.jsx)(n.h3,{id:"language-action-translation-module",children:"Language-Action Translation Module"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\nimport numpy as np\nfrom typing import Dict, List, Optional, Tuple\nimport re\n\nclass LanguageActionMapper(nn.Module):\n    def __init__(self, vocab_size=10000, hidden_dim=512, action_space_dim=10):\n        super().__init__()\n\n        # Language processing\n        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n        self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n        self.language_encoder = nn.Linear(hidden_dim, hidden_dim)\n\n        # Action space mapping\n        self.action_projector = nn.Linear(hidden_dim, action_space_dim)\n        self.action_classifier = nn.Linear(hidden_dim, action_space_dim)\n\n        # Action type classifier (navigation, manipulation, etc.)\n        self.action_type_classifier = nn.Linear(hidden_dim, 5)  # 5 action types\n\n        self.hidden_dim = hidden_dim\n        self.action_space_dim = action_space_dim\n\n    def forward(self, language_input, vision_features=None):\n        \"\"\"\n        language_input: tokenized language input\n        vision_features: optional visual context\n        \"\"\"\n        # Process language\n        embedded = self.embedding(language_input)\n        lstm_out, (hidden, _) = self.lstm(embedded)\n\n        # Use final hidden state as language representation\n        lang_repr = hidden[-1]  # (batch, hidden_dim)\n\n        # If vision features provided, fuse with language\n        if vision_features is not None:\n            combined_repr = torch.cat([lang_repr, vision_features], dim=-1)\n            combined_repr = nn.Linear(lang_repr.size(-1) + vision_features.size(-1), self.hidden_dim)(combined_repr)\n        else:\n            combined_repr = lang_repr\n\n        # Project to action space\n        action_logits = self.action_classifier(combined_repr)\n        action_probs = torch.softmax(action_logits, dim=-1)\n\n        # Classify action type\n        action_type_logits = self.action_type_classifier(combined_repr)\n        action_type_probs = torch.softmax(action_type_logits, dim=-1)\n\n        return {\n            'action_probs': action_probs,\n            'action_type_probs': action_type_probs,\n            'action_logits': action_logits,\n            'action_type_logits': action_type_logits\n        }\n\n    def extract_action_primitives(self, command: str) -> List[Dict]:\n        \"\"\"Extract action primitives from natural language command\"\"\"\n        action_primitives = []\n\n        # Navigation actions\n        nav_patterns = [\n            (r'go to|move to|navigate to|go (.+)', 'navigation'),\n            (r'forward|backward|left|right', 'navigation'),\n            (r'kitchen|bedroom|living room|office', 'navigation')\n        ]\n\n        # Manipulation actions\n        manipulation_patterns = [\n            (r'pick up|grasp|take|lift (.+)', 'manipulation'),\n            (r'put down|place|drop (.+)', 'manipulation'),\n            (r'open|close (.+)', 'manipulation')\n        ]\n\n        # Interaction actions\n        interaction_patterns = [\n            (r'wave|greet|hello|hi (.+)', 'interaction'),\n            (r'follow|accompany (.+)', 'interaction'),\n            (r'stop|wait|pause', 'interaction')\n        ]\n\n        all_patterns = nav_patterns + manipulation_patterns + interaction_patterns\n\n        for pattern, action_type in all_patterns:\n            matches = re.findall(pattern, command, re.IGNORECASE)\n            for match in matches:\n                action_primitives.append({\n                    'type': action_type,\n                    'command': command,\n                    'target': match if match else None,\n                    'confidence': 0.8  # Placeholder confidence\n                })\n\n        return action_primitives\n"})}),"\n",(0,a.jsx)(n.h2,{id:"task-planning-module",children:"Task Planning Module"}),"\n",(0,a.jsx)(n.h3,{id:"hierarchical-task-planner",children:"Hierarchical Task Planner"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"class TaskPlanner:\n    def __init__(self):\n        self.action_library = self._initialize_action_library()\n        self.task_graph = {}\n\n    def _initialize_action_library(self):\n        \"\"\"Initialize library of primitive actions\"\"\"\n        return {\n            # Navigation actions\n            'move_forward': {\n                'primitive': 'base_controller',\n                'params': {'linear_vel': 0.3, 'angular_vel': 0.0},\n                'preconditions': ['robot_enabled'],\n                'effects': ['robot_moved_forward']\n            },\n            'turn_left': {\n                'primitive': 'base_controller',\n                'params': {'linear_vel': 0.0, 'angular_vel': 0.5},\n                'preconditions': ['robot_enabled'],\n                'effects': ['robot_turned_left']\n            },\n            'navigate_to': {\n                'primitive': 'navigation_stack',\n                'params': {'goal_tolerance': 0.2},\n                'preconditions': ['map_available', 'goal_valid'],\n                'effects': ['reached_goal']\n            },\n\n            # Manipulation actions\n            'grasp_object': {\n                'primitive': 'manipulation_controller',\n                'params': {'gripper_force': 50},\n                'preconditions': ['object_detected', 'arm_free'],\n                'effects': ['object_grasped']\n            },\n            'release_object': {\n                'primitive': 'manipulation_controller',\n                'params': {'gripper_position': 100},\n                'preconditions': ['object_grasped'],\n                'effects': ['object_released']\n            },\n\n            # Perception actions\n            'detect_object': {\n                'primitive': 'perception_system',\n                'params': {'detection_threshold': 0.7},\n                'preconditions': ['camera_enabled'],\n                'effects': ['object_location_known']\n            }\n        }\n\n    def plan_task(self, command: str, world_state: Dict) -> List[Dict]:\n        \"\"\"Generate task plan from command and world state\"\"\"\n        # Extract action primitives\n        action_primitives = self.language_action_mapper.extract_action_primitives(command)\n\n        if not action_primitives:\n            return []\n\n        # Create task plan\n        task_plan = []\n        for primitive in action_primitives:\n            action_sequence = self._decompose_action(primitive, world_state)\n            task_plan.extend(action_sequence)\n\n        return task_plan\n\n    def _decompose_action(self, primitive: Dict, world_state: Dict) -> List[Dict]:\n        \"\"\"Decompose high-level action into primitive actions\"\"\"\n        action_type = primitive['type']\n        target = primitive['target']\n\n        if action_type == 'navigation':\n            return self._create_navigation_plan(target, world_state)\n        elif action_type == 'manipulation':\n            return self._create_manipulation_plan(target, world_state)\n        elif action_type == 'interaction':\n            return self._create_interaction_plan(target, world_state)\n        else:\n            return []\n\n    def _create_navigation_plan(self, target: str, world_state: Dict) -> List[Dict]:\n        \"\"\"Create navigation plan to target location\"\"\"\n        if target in world_state.get('known_locations', {}):\n            goal_pose = world_state['known_locations'][target]\n        else:\n            # Use perception to find target location\n            return [\n                {\n                    'action': 'detect_object',\n                    'params': {'object_name': target},\n                    'description': f'Detect {target} location'\n                },\n                {\n                    'action': 'navigate_to',\n                    'params': {'goal_pose': 'detected_pose'},\n                    'description': f'Navigate to {target}'\n                }\n            ]\n\n        return [\n            {\n                'action': 'navigate_to',\n                'params': {'goal_pose': goal_pose},\n                'description': f'Navigate to {target}'\n            }\n        ]\n\n    def _create_manipulation_plan(self, target: str, world_state: Dict) -> List[Dict]:\n        \"\"\"Create manipulation plan for target object\"\"\"\n        plan = []\n\n        # Detect object if not already known\n        if target not in world_state.get('object_locations', {}):\n            plan.append({\n                'action': 'detect_object',\n                'params': {'object_name': target},\n                'description': f'Detect {target} object'\n            })\n\n        # Navigate to object\n        plan.append({\n            'action': 'navigate_to',\n            'params': {'goal_pose': 'object_pose'},\n            'description': f'Navigate to {target}'\n        })\n\n        # Grasp object\n        plan.append({\n            'action': 'grasp_object',\n            'params': {'object_name': target},\n            'description': f'Grasp {target}'\n        })\n\n        return plan\n\n    def _create_interaction_plan(self, target: str, world_state: Dict) -> List[Dict]:\n        \"\"\"Create interaction plan\"\"\"\n        plan = []\n\n        # Navigate to target if it's a person/location\n        if target in world_state.get('known_people', {}) or target in world_state.get('known_locations', {}):\n            plan.append({\n                'action': 'navigate_to',\n                'params': {'goal_pose': 'target_pose'},\n                'description': f'Navigate to {target}'\n            })\n\n        # Perform interaction\n        plan.append({\n            'action': 'perform_interaction',\n            'params': {'interaction_type': 'wave'},\n            'description': f'Interact with {target}'\n        })\n\n        return plan\n"})}),"\n",(0,a.jsx)(n.h2,{id:"action-execution-system",children:"Action Execution System"}),"\n",(0,a.jsx)(n.h3,{id:"robot-action-executor",children:"Robot Action Executor"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Twist, Pose, Point\nfrom std_msgs.msg import String, Bool\nfrom action_msgs.msg import GoalStatus\nfrom rclpy.action import ActionClient\nfrom geometry_msgs.msg import PoseStamped\nfrom nav2_msgs.action import NavigateToPose\nfrom control_msgs.action import FollowJointTrajectory\nfrom trajectory_msgs.msg import JointTrajectory, JointTrajectoryPoint\n\nclass ActionExecutor(Node):\n    def __init__(self):\n        super().__init__('action_executor')\n\n        # Publishers for different action types\n        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\n        self.status_pub = self.create_publisher(String, '/action_status', 10)\n\n        # Action clients\n        self.nav_client = ActionClient(self, NavigateToPose, 'navigate_to_pose')\n        self.traj_client = ActionClient(self, FollowJointTrajectory, 'follow_joint_trajectory')\n\n        # Current task execution\n        self.current_task = None\n        self.task_queue = []\n        self.is_executing = False\n\n        # Timer for action execution\n        self.action_timer = self.create_timer(0.1, self.execute_next_action)\n\n    def execute_task_plan(self, task_plan: List[Dict]):\n        \"\"\"Execute a sequence of tasks\"\"\"\n        self.task_queue.extend(task_plan)\n        self.get_logger().info(f'Added {len(task_plan)} tasks to execution queue')\n\n    def execute_next_action(self):\n        \"\"\"Execute the next action in the queue\"\"\"\n        if not self.task_queue or self.is_executing:\n            return\n\n        task = self.task_queue.pop(0)\n        self.is_executing = True\n\n        action_type = task['action']\n        params = task.get('params', {})\n\n        self.get_logger().info(f'Executing action: {action_type}')\n\n        # Execute action based on type\n        if action_type == 'move_forward':\n            self.execute_move_forward(params)\n        elif action_type == 'turn_left':\n            self.execute_turn_left(params)\n        elif action_type == 'navigate_to':\n            self.execute_navigate_to(params)\n        elif action_type == 'grasp_object':\n            self.execute_grasp_object(params)\n        elif action_type == 'release_object':\n            self.execute_release_object(params)\n        else:\n            self.get_logger().warn(f'Unknown action type: {action_type}')\n            self.is_executing = False\n\n    def execute_move_forward(self, params):\n        \"\"\"Execute move forward action\"\"\"\n        cmd_vel = Twist()\n        cmd_vel.linear.x = params.get('linear_vel', 0.3)\n        cmd_vel.angular.z = params.get('angular_vel', 0.0)\n\n        self.cmd_vel_pub.publish(cmd_vel)\n\n        # Simple timeout-based completion\n        timer = self.create_timer(\n            params.get('duration', 2.0),\n            lambda: self.action_completed('move_forward')\n        )\n\n    def execute_turn_left(self, params):\n        \"\"\"Execute turn left action\"\"\"\n        cmd_vel = Twist()\n        cmd_vel.linear.x = params.get('linear_vel', 0.0)\n        cmd_vel.angular.z = params.get('angular_vel', 0.5)\n\n        self.cmd_vel_pub.publish(cmd_vel)\n\n        timer = self.create_timer(\n            params.get('duration', 1.0),\n            lambda: self.action_completed('turn_left')\n        )\n\n    def execute_navigate_to(self, params):\n        \"\"\"Execute navigation action using Nav2\"\"\"\n        goal_msg = NavigateToPose.Goal()\n\n        # Set goal pose\n        pose = params.get('goal_pose', Pose())\n        goal_msg.pose.header.frame_id = 'map'\n        goal_msg.pose.pose = pose\n\n        # Send goal\n        self.nav_client.send_goal_async(\n            goal_msg,\n            feedback_callback=self.navigation_feedback_callback\n        ).add_done_callback(self.navigation_goal_response_callback)\n\n    def execute_grasp_object(self, params):\n        \"\"\"Execute grasp action\"\"\"\n        # This would interface with manipulation stack\n        object_name = params.get('object_name', 'unknown')\n\n        self.get_logger().info(f'Attempting to grasp {object_name}')\n\n        # Simulate grasp completion\n        timer = self.create_timer(\n            3.0,  # Grasp takes 3 seconds\n            lambda: self.action_completed('grasp_object')\n        )\n\n    def execute_release_object(self, params):\n        \"\"\"Execute release action\"\"\"\n        self.get_logger().info('Releasing object')\n\n        timer = self.create_timer(\n            2.0,\n            lambda: self.action_completed('release_object')\n        )\n\n    def navigation_feedback_callback(self, feedback_msg):\n        \"\"\"Handle navigation feedback\"\"\"\n        remaining_distance = feedback_msg.feedback.distance_remaining\n        self.get_logger().debug(f'Distance remaining: {remaining_distance:.2f}m')\n\n    def navigation_goal_response_callback(self, future):\n        \"\"\"Handle navigation goal response\"\"\"\n        goal_handle = future.result()\n        if not goal_handle.accepted:\n            self.get_logger().error('Navigation goal rejected')\n            self.is_executing = False\n            return\n\n        self.get_logger().info('Navigation goal accepted')\n        goal_handle.get_result_async().add_done_callback(self.navigation_result_callback)\n\n    def navigation_result_callback(self, future):\n        \"\"\"Handle navigation result\"\"\"\n        result = future.result().result\n        status = future.result().status\n\n        if status == GoalStatus.STATUS_SUCCEEDED:\n            self.get_logger().info('Navigation succeeded')\n            self.action_completed('navigate_to')\n        else:\n            self.get_logger().error(f'Navigation failed with status: {status}')\n            self.is_executing = False\n\n    def action_completed(self, action_name):\n        \"\"\"Mark action as completed\"\"\"\n        self.get_logger().info(f'Action completed: {action_name}')\n        self.is_executing = False\n\n        # Publish completion status\n        status_msg = String()\n        status_msg.data = f'completed_{action_name}'\n        self.status_pub.publish(status_msg)\n"})}),"\n",(0,a.jsx)(n.h2,{id:"closed-loop-vla-system",children:"Closed-Loop VLA System"}),"\n",(0,a.jsx)(n.h3,{id:"complete-vla-integration",children:"Complete VLA Integration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"class ClosedLoopVLA:\n    def __init__(self):\n        # Initialize perception system\n        self.perception_system = MultimodalPerceptionSystem()\n\n        # Initialize language-action mapper\n        self.language_mapper = LanguageActionMapper()\n\n        # Initialize task planner\n        self.task_planner = TaskPlanner()\n\n        # Initialize action executor\n        self.action_executor = ActionExecutor()\n\n        # World state representation\n        self.world_state = {\n            'robot_pose': None,\n            'object_locations': {},\n            'known_locations': {},\n            'known_people': {},\n            'robot_status': 'idle'\n        }\n\n    def execute_command(self, image, command, depth_map=None):\n        \"\"\"Execute a complete VLA command\"\"\"\n        # Step 1: Multimodal perception\n        perception_results = self.perception_system(\n            image=image,\n            command=command,\n            depth_map=depth_map\n        )\n\n        # Update world state with perception results\n        self.update_world_state(perception_results)\n\n        # Step 2: Language to action mapping\n        action_mapping = self.language_mapper(\n            language_input=command,\n            vision_features=perception_results.get('visual_features')\n        )\n\n        # Extract action primitives\n        action_primitives = self.language_mapper.extract_action_primitives(command)\n\n        # Step 3: Task planning\n        task_plan = self.task_planner.plan_task(command, self.world_state)\n\n        # Step 4: Action execution\n        self.action_executor.execute_task_plan(task_plan)\n\n        return {\n            'perception_results': perception_results,\n            'action_mapping': action_mapping,\n            'task_plan': task_plan,\n            'execution_status': 'started'\n        }\n\n    def update_world_state(self, perception_results):\n        \"\"\"Update world state based on perception results\"\"\"\n        # Update object locations\n        if 'detections' in perception_results:\n            for detection in perception_results['detections']:\n                obj_name = detection.get('class', 'unknown')\n                obj_pose = detection.get('pose', None)\n                if obj_pose:\n                    self.world_state['object_locations'][obj_name] = obj_pose\n\n        # Update robot pose if available\n        if 'robot_pose' in perception_results:\n            self.world_state['robot_pose'] = perception_results['robot_pose']\n\n    def get_execution_feedback(self):\n        \"\"\"Get feedback on current execution status\"\"\"\n        return {\n            'world_state': self.world_state,\n            'execution_queue_size': len(self.action_executor.task_queue),\n            'is_executing': self.action_executor.is_executing\n        }\n"})}),"\n",(0,a.jsx)(n.h2,{id:"vla-integration-node",children:"VLA Integration Node"}),"\n",(0,a.jsx)(n.h3,{id:"ros-2-vla-node",children:"ROS 2 VLA Node"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import PoseStamped\nfrom cv_bridge import CvBridge\nimport numpy as np\n\nclass VLARosNode(Node):\n    def __init__(self):\n        super().__init__(\'vla_system_node\')\n\n        # Initialize CV bridge\n        self.cv_bridge = CvBridge()\n\n        # Publishers and subscribers\n        self.image_sub = self.create_subscription(\n            Image, \'/camera/rgb/image_raw\', self.image_callback, 10\n        )\n        self.command_sub = self.create_subscription(\n            String, \'/vla/command\', self.command_callback, 10\n        )\n        self.status_pub = self.create_publisher(String, \'/vla/status\', 10)\n\n        # Initialize complete VLA system\n        self.vla_system = ClosedLoopVLA()\n\n        # Store latest image and command\n        self.latest_image = None\n        self.pending_command = None\n\n    def image_callback(self, msg):\n        """Process incoming image"""\n        try:\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding=\'bgr8\')\n            self.latest_image = cv_image\n\n            # If we have a pending command, execute now\n            if self.pending_command:\n                self.execute_vla_command()\n\n        except Exception as e:\n            self.get_logger().error(f\'Error processing image: {e}\')\n\n    def command_callback(self, msg):\n        """Process incoming command"""\n        self.pending_command = msg.data\n\n        # If we have a recent image, execute now\n        if self.latest_image:\n            self.execute_vla_command()\n\n    def execute_vla_command(self):\n        """Execute the VLA command with current image"""\n        if self.latest_image is None or self.pending_command is None:\n            return\n\n        try:\n            # Convert image to tensor\n            from PIL import Image as PILImage\n            pil_image = PILImage.fromarray(self.latest_image)\n\n            # Execute VLA command\n            results = self.vla_system.execute_command(\n                image=pil_image,\n                command=self.pending_command\n            )\n\n            # Publish status\n            status_msg = String()\n            status_msg.data = f"VLA command executed: {self.pending_command}"\n            self.status_pub.publish(status_msg)\n\n            self.get_logger().info(f\'Executed VLA command: {self.pending_command}\')\n\n            # Clear pending command\n            self.pending_command = None\n\n        except Exception as e:\n            self.get_logger().error(f\'Error executing VLA command: {e}\')\n\n    def get_system_status(self):\n        """Get current system status"""\n        feedback = self.vla_system.get_execution_feedback()\n        return feedback\n'})}),"\n",(0,a.jsx)(n.h2,{id:"action-learning-and-adaptation",children:"Action Learning and Adaptation"}),"\n",(0,a.jsx)(n.h3,{id:"online-action-learning",children:"Online Action Learning"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"class OnlineActionLearner:\n    def __init__(self, action_space_dim=10):\n        self.action_space_dim = action_space_dim\n\n        # Experience replay buffer\n        self.experience_buffer = {\n            'states': [],\n            'actions': [],\n            'rewards': [],\n            'next_states': [],\n            'dones': []\n        }\n\n        # Action refinement network\n        self.refinement_network = nn.Sequential(\n            nn.Linear(action_space_dim * 2, 128),  # current + desired actions\n            nn.ReLU(),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, action_space_dim),\n            nn.Tanh()\n        )\n\n        self.optimizer = torch.optim.Adam(self.refinement_network.parameters())\n        self.criterion = nn.MSELoss()\n\n    def record_experience(self, state, action, reward, next_state, done):\n        \"\"\"Record experience for learning\"\"\"\n        self.experience_buffer['states'].append(state)\n        self.experience_buffer['actions'].append(action)\n        self.experience_buffer['rewards'].append(reward)\n        self.experience_buffer['next_states'].append(next_state)\n        self.experience_buffer['dones'].append(done)\n\n        # Keep buffer size manageable\n        if len(self.experience_buffer['states']) > 10000:\n            for key in self.experience_buffer:\n                self.experience_buffer[key] = self.experience_buffer[key][-5000:]\n\n    def refine_action(self, current_action, desired_action):\n        \"\"\"Refine action based on experience\"\"\"\n        # Combine current and desired actions\n        combined_input = torch.cat([current_action, desired_action], dim=-1)\n\n        # Get refined action\n        refined_action = self.refinement_network(combined_input)\n\n        return refined_action\n\n    def update_model(self, batch_size=32):\n        \"\"\"Update the refinement model\"\"\"\n        if len(self.experience_buffer['states']) < batch_size:\n            return\n\n        # Sample batch\n        indices = np.random.choice(len(self.experience_buffer['states']), batch_size)\n\n        states = torch.stack([self.experience_buffer['states'][i] for i in indices])\n        actions = torch.stack([self.experience_buffer['actions'][i] for i in indices])\n        rewards = torch.tensor([self.experience_buffer['rewards'][i] for i in indices])\n\n        # Use rewards to guide action refinement\n        # Higher reward actions should be reinforced\n        target_actions = actions + rewards.unsqueeze(1) * 0.1  # Simple reward shaping\n\n        # Train refinement network\n        self.optimizer.zero_grad()\n        predicted_actions = self.refinement_network(torch.cat([states, actions], dim=1))\n        loss = self.criterion(predicted_actions, target_actions)\n        loss.backward()\n        self.optimizer.step()\n\n        return loss.item()\n"})}),"\n",(0,a.jsx)(n.h2,{id:"performance-evaluation",children:"Performance Evaluation"}),"\n",(0,a.jsx)(n.h3,{id:"vla-system-evaluation",children:"VLA System Evaluation"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"class VLAEvaluator:\n    def __init__(self):\n        self.metrics = {\n            'command_success_rate': [],\n            'action_accuracy': [],\n            'perception_accuracy': [],\n            'execution_time': [],\n            'task_completion_rate': []\n        }\n\n    def evaluate_command_execution(self, command, expected_outcome, actual_outcome):\n        \"\"\"Evaluate command execution performance\"\"\"\n        # Calculate success based on expected vs actual outcome\n        success = self.compare_outcomes(expected_outcome, actual_outcome)\n        self.metrics['command_success_rate'].append(1.0 if success else 0.0)\n\n        return success\n\n    def evaluate_action_accuracy(self, predicted_actions, ground_truth_actions):\n        \"\"\"Evaluate action prediction accuracy\"\"\"\n        correct = 0\n        total = len(ground_truth_actions)\n\n        for pred, gt in zip(predicted_actions, ground_truth_actions):\n            if self.actions_match(pred, gt):\n                correct += 1\n\n        accuracy = correct / total if total > 0 else 0\n        self.metrics['action_accuracy'].append(accuracy)\n        return accuracy\n\n    def evaluate_perception_grounding(self, perception_results, ground_truth_objects):\n        \"\"\"Evaluate perception grounding accuracy\"\"\"\n        correct = 0\n        total = len(ground_truth_objects)\n\n        for gt_obj in ground_truth_objects:\n            for pred_obj in perception_results.get('detections', []):\n                if self.objects_match(gt_obj, pred_obj):\n                    correct += 1\n                    break\n\n        accuracy = correct / total if total > 0 else 0\n        self.metrics['perception_accuracy'].append(accuracy)\n        return accuracy\n\n    def objects_match(self, obj1, obj2):\n        \"\"\"Check if two objects match\"\"\"\n        # Check spatial overlap and semantic similarity\n        iou = self.calculate_iou(obj1.get('bbox', [0,0,0,0]), obj2.get('bbox', [0,0,0,0]))\n        semantic_sim = self.calculate_semantic_similarity(\n            obj1.get('description', ''),\n            obj2.get('description', '')\n        )\n        return iou > 0.5 and semantic_sim > 0.7\n\n    def actions_match(self, action1, action2):\n        \"\"\"Check if two actions match\"\"\"\n        return (action1.get('type') == action2.get('type') and\n                action1.get('target') == action2.get('target'))\n\n    def compare_outcomes(self, expected, actual):\n        \"\"\"Compare expected vs actual outcomes\"\"\"\n        # This would depend on your specific outcome representation\n        # For now, return simple comparison\n        return expected == actual\n\n    def get_summary_metrics(self):\n        \"\"\"Get summary of all metrics\"\"\"\n        summary = {}\n        for metric_name, values in self.metrics.items():\n            if values:\n                summary[metric_name] = {\n                    'mean': np.mean(values),\n                    'std': np.std(values),\n                    'count': len(values)\n                }\n        return summary\n"})}),"\n",(0,a.jsx)(n.h2,{id:"exercise-tasks",children:"Exercise Tasks"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Implement the language-to-action mapping system"}),"\n",(0,a.jsx)(n.li,{children:"Create a hierarchical task planning module"}),"\n",(0,a.jsx)(n.li,{children:"Build the action execution system with ROS 2 integration"}),"\n",(0,a.jsx)(n.li,{children:"Implement the complete closed-loop VLA system"}),"\n",(0,a.jsx)(n.li,{children:"Add online learning capabilities for action refinement"}),"\n",(0,a.jsx)(n.li,{children:"Evaluate the system performance with various commands"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,a.jsx)(n.h3,{id:"common-issues",children:"Common Issues"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Action execution failures"}),": Check robot state and preconditions"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Perception-Action mismatch"}),": Improve grounding between modalities"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Task planning errors"}),": Verify world state representation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Timing issues"}),": Ensure proper synchronization between components"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(n.p,{children:"In this lab, you implemented the action mapping component of VLA systems. You created language-to-action translation, task planning, action execution, and closed-loop integration. You also implemented online learning for action refinement and evaluation metrics. This completes the full VLA pipeline from vision-language input to robot action execution."})]})}function d(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(p,{...e})}):p(e)}},8453(e,n,t){t.d(n,{R:()=>s,x:()=>r});var a=t(6540);const i={},o=a.createContext(i);function s(e){const n=a.useContext(o);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),a.createElement(o.Provider,{value:n},e.children)}}}]);