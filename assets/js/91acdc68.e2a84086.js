"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[3388],{8453(e,n,t){t.d(n,{R:()=>o,x:()=>r});var s=t(6540);const i={},a=s.createContext(i);function o(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),s.createElement(a.Provider,{value:n},e.children)}},9636(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>u,frontMatter:()=>a,metadata:()=>r,toc:()=>d});var s=t(4848),i=t(8453);const a={sidebar_label:"Lab 4.2: Multimodal Perception"},o="Lab Exercise 4.2: Multimodal Perception in VLA Systems",r={id:"modules/lab-exercises/lab-4-2-multimodal-perception",title:"Lab Exercise 4.2: Multimodal Perception in VLA Systems",description:"This lab exercise covers implementing multimodal perception systems that integrate vision, language, and other sensory inputs for VLA systems.",source:"@site/docs/modules/lab-exercises/lab-4-2-multimodal-perception.md",sourceDirName:"modules/lab-exercises",slug:"/modules/lab-exercises/lab-4-2-multimodal-perception",permalink:"/hackathon-ai-book/modules/lab-exercises/lab-4-2-multimodal-perception",draft:!1,unlisted:!1,editUrl:"https://github.com/sanilahmed/hackathon-ai-book/tree/main/docs/modules/lab-exercises/lab-4-2-multimodal-perception.md",tags:[],version:"current",frontMatter:{sidebar_label:"Lab 4.2: Multimodal Perception"},sidebar:"tutorialSidebar",previous:{title:"Lab 4.1: VLA Fundamentals",permalink:"/hackathon-ai-book/modules/lab-exercises/lab-4-1-vla-fundamentals"},next:{title:"Lab 4.3: Action Mapping",permalink:"/hackathon-ai-book/modules/lab-exercises/lab-4-3-action-mapping"}},l={},d=[{value:"Objectives",id:"objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Multimodal Perception Architecture",id:"multimodal-perception-architecture",level:2},{value:"Sensor Integration Framework",id:"sensor-integration-framework",level:3},{value:"Cross-Modal Attention Mechanisms",id:"cross-modal-attention-mechanisms",level:2},{value:"Vision-Language Attention",id:"vision-language-attention",level:3},{value:"Spatial-Temporal Perception",id:"spatial-temporal-perception",level:2},{value:"Spatio-Temporal Feature Integration",id:"spatio-temporal-feature-integration",level:3},{value:"Multimodal Object Detection",id:"multimodal-object-detection",level:2},{value:"Vision-Language Object Detection",id:"vision-language-object-detection",level:3},{value:"Multimodal Perception Node",id:"multimodal-perception-node",level:2},{value:"ROS 2 Integration",id:"ros-2-integration",level:3},{value:"Scene Understanding and Grounding",id:"scene-understanding-and-grounding",level:2},{value:"Multimodal Scene Graph",id:"multimodal-scene-graph",level:3},{value:"Exercise Tasks",id:"exercise-tasks",level:2},{value:"Evaluation Metrics",id:"evaluation-metrics",level:2},{value:"Multimodal Perception Evaluation",id:"multimodal-perception-evaluation",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues",id:"common-issues",level:3},{value:"Summary",id:"summary",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"lab-exercise-42-multimodal-perception-in-vla-systems",children:"Lab Exercise 4.2: Multimodal Perception in VLA Systems"}),"\n",(0,s.jsx)(n.p,{children:"This lab exercise covers implementing multimodal perception systems that integrate vision, language, and other sensory inputs for VLA systems."}),"\n",(0,s.jsx)(n.h2,{id:"objectives",children:"Objectives"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Implement multimodal feature extraction"}),"\n",(0,s.jsx)(n.li,{children:"Create cross-modal attention mechanisms"}),"\n",(0,s.jsx)(n.li,{children:"Integrate multiple sensor modalities"}),"\n",(0,s.jsx)(n.li,{children:"Test perception accuracy with multimodal inputs"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Completed VLA fundamentals lab"}),"\n",(0,s.jsx)(n.li,{children:"Understanding of computer vision and NLP"}),"\n",(0,s.jsx)(n.li,{children:"PyTorch/TensorFlow knowledge"}),"\n",(0,s.jsx)(n.li,{children:"ROS 2 Humble with perception packages"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"multimodal-perception-architecture",children:"Multimodal Perception Architecture"}),"\n",(0,s.jsx)(n.h3,{id:"sensor-integration-framework",children:"Sensor Integration Framework"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\nimport numpy as np\nfrom typing import Dict, List, Optional, Tuple\nimport torchvision.transforms as T\nfrom PIL import Image\n\nclass MultimodalSensorFusion:\n    def __init__(self):\n        # Initialize sensor encoders\n        self.vision_encoder = VisionEncoder()\n        self.language_encoder = LanguageEncoder()\n        self.depth_encoder = DepthEncoder()\n        self.audio_encoder = AudioEncoder()  # If available\n\n        # Fusion dimensions\n        self.vision_dim = 512\n        self.language_dim = 512\n        self.depth_dim = 256\n        self.fusion_dim = 1024\n\n        # Cross-modal attention modules\n        self.vision_language_attention = CrossModalAttention(self.vision_dim, self.language_dim)\n        self.vision_depth_attention = CrossModalAttention(self.vision_dim, self.depth_dim)\n\n    def forward(self, sensors: Dict[str, torch.Tensor], language_input: str):\n        \"\"\"Process multimodal sensor inputs with language context\"\"\"\n        outputs = {}\n\n        # Process individual modalities\n        if 'rgb' in sensors:\n            vision_features = self.vision_encoder(sensors['rgb'])\n            outputs['vision'] = vision_features\n\n        if 'depth' in sensors:\n            depth_features = self.depth_encoder(sensors['depth'])\n            outputs['depth'] = depth_features\n\n        if language_input:\n            language_features = self.language_encoder(language_input)\n            outputs['language'] = language_features\n\n        # Cross-modal fusion\n        if 'vision' in outputs and 'language' in outputs:\n            fused_vl = self.vision_language_attention(\n                outputs['vision'], outputs['language']\n            )\n            outputs['vision_language'] = fused_vl\n\n        if 'vision' in outputs and 'depth' in outputs:\n            fused_vd = self.vision_depth_attention(\n                outputs['vision'], outputs['depth']\n            )\n            outputs['vision_depth'] = fused_vd\n\n        return outputs\n\nclass VisionEncoder(nn.Module):\n    def __init__(self, backbone='resnet50'):\n        super().__init__()\n\n        if backbone == 'resnet50':\n            from torchvision.models import resnet50\n            self.backbone = resnet50(pretrained=True)\n            self.feature_dim = 2048\n            self.backbone.fc = nn.Identity()  # Remove classification layer\n        elif backbone == 'clip':\n            import clip\n            self.backbone, _ = clip.load(\"ViT-B/32\", device='cpu')\n            self.feature_dim = 512\n\n        # Spatial feature extraction\n        self.spatial_pool = nn.AdaptiveAvgPool2d((7, 7))\n\n    def forward(self, images):\n        features = self.backbone(images)\n\n        if len(features.shape) == 4:  # CNN features\n            features = self.spatial_pool(features)\n            # Reshape to (batch, channels, height*width)\n            batch_size, channels, h, w = features.shape\n            features = features.view(batch_size, channels, h*w)\n            features = features.transpose(1, 2)  # (batch, h*w, channels)\n\n        return features\n\nclass DepthEncoder(nn.Module):\n    def __init__(self, input_channels=1):\n        super().__init__()\n\n        # Simple CNN for depth processing\n        self.conv_layers = nn.Sequential(\n            nn.Conv2d(input_channels, 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.ReLU(),\n        )\n\n        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.feature_dim = 128\n\n    def forward(self, depth_maps):\n        features = self.conv_layers(depth_maps)\n        features = self.global_pool(features)\n        features = features.view(features.size(0), -1)  # Flatten\n        return features\n\nclass LanguageEncoder(nn.Module):\n    def __init__(self, model_name='bert-base-uncased'):\n        super().__init__()\n\n        from transformers import AutoTokenizer, AutoModel\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.backbone = AutoModel.from_pretrained(model_name)\n        self.feature_dim = self.backbone.config.hidden_size\n\n    def forward(self, texts):\n        if isinstance(texts, str):\n            texts = [texts]\n\n        inputs = self.tokenizer(\n            texts,\n            return_tensors='pt',\n            padding=True,\n            truncation=True,\n            max_length=128\n        )\n\n        outputs = self.backbone(**inputs)\n        # Use [CLS] token representation\n        return outputs.last_hidden_state[:, 0, :]\n"})}),"\n",(0,s.jsx)(n.h2,{id:"cross-modal-attention-mechanisms",children:"Cross-Modal Attention Mechanisms"}),"\n",(0,s.jsx)(n.h3,{id:"vision-language-attention",children:"Vision-Language Attention"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class VisionLanguageAttention(nn.Module):\n    def __init__(self, vision_dim, language_dim, hidden_dim=512):\n        super().__init__()\n\n        self.vision_dim = vision_dim\n        self.language_dim = language_dim\n        self.hidden_dim = hidden_dim\n\n        # Projection layers\n        self.vision_proj = nn.Linear(vision_dim, hidden_dim)\n        self.language_proj = nn.Linear(language_dim, hidden_dim)\n\n        # Attention mechanism\n        self.attention = nn.MultiheadAttention(\n            embed_dim=hidden_dim,\n            num_heads=8,\n            dropout=0.1,\n            batch_first=True\n        )\n\n        # Output projection\n        self.output_proj = nn.Linear(hidden_dim, hidden_dim)\n\n    def forward(self, vision_features, language_features):\n        """\n        vision_features: (batch_size, num_patches, vision_dim)\n        language_features: (batch_size, language_dim)\n        """\n        batch_size = vision_features.size(0)\n\n        # Project features\n        vision_proj = self.vision_proj(vision_features)  # (B, num_patches, H)\n        language_proj = self.language_proj(language_features).unsqueeze(1)  # (B, 1, H)\n\n        # Concatenate vision and language features\n        all_features = torch.cat([language_proj, vision_proj], dim=1)  # (B, 1+num_patches, H)\n\n        # Self-attention\n        attended_features, attention_weights = self.attention(\n            all_features, all_features, all_features\n        )\n\n        # Extract attended language features (first position)\n        attended_language = attended_features[:, 0, :]  # (B, H)\n\n        # Extract attended vision features (remaining positions)\n        attended_vision = attended_features[:, 1:, :]  # (B, num_patches, H)\n\n        # Apply output projection\n        attended_language = self.output_proj(attended_language)\n        attended_vision = self.output_proj(attended_vision)\n\n        return attended_vision, attended_language, attention_weights\n\nclass CrossModalAttention(nn.Module):\n    def __init__(self, dim1, dim2, hidden_dim=512):\n        super().__init__()\n\n        self.proj1 = nn.Linear(dim1, hidden_dim)\n        self.proj2 = nn.Linear(dim2, hidden_dim)\n        self.attention = nn.MultiheadAttention(\n            embed_dim=hidden_dim,\n            num_heads=8,\n            dropout=0.1,\n            batch_first=True\n        )\n        self.norm = nn.LayerNorm(hidden_dim)\n\n    def forward(self, features1, features2):\n        # Project to common dimension\n        f1_proj = self.proj1(features1)\n        f2_proj = self.proj2(features2)\n\n        # Cross attention: features1 attending to features2\n        attended_f1, _ = self.attention(f1_proj, f2_proj, f2_proj)\n        attended_f1 = self.norm(attended_f1 + f1_proj)\n\n        # Cross attention: features2 attending to features1\n        attended_f2, _ = self.attention(f2_proj, f1_proj, f1_proj)\n        attended_f2 = self.norm(attended_f2 + f2_proj)\n\n        return attended_f1, attended_f2\n'})}),"\n",(0,s.jsx)(n.h2,{id:"spatial-temporal-perception",children:"Spatial-Temporal Perception"}),"\n",(0,s.jsx)(n.h3,{id:"spatio-temporal-feature-integration",children:"Spatio-Temporal Feature Integration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class SpatioTemporalPerceiver(nn.Module):\n    def __init__(self, feature_dim=512, num_heads=8, num_layers=2):\n        super().__init__()\n\n        self.feature_dim = feature_dim\n        self.num_heads = num_heads\n\n        # Transformer layers for spatio-temporal processing\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=feature_dim,\n            nhead=num_heads,\n            dim_feedforward=feature_dim * 4,\n            dropout=0.1,\n            batch_first=True\n        )\n        self.temporal_transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n\n        # Spatial attention\n        self.spatial_attention = nn.MultiheadAttention(\n            embed_dim=feature_dim,\n            num_heads=num_heads,\n            dropout=0.1,\n            batch_first=True\n        )\n\n    def forward(self, spatial_features, temporal_features=None):\n        """\n        spatial_features: (batch, num_patches, feature_dim)\n        temporal_features: (batch, time_steps, feature_dim) or None\n        """\n        batch_size, num_patches, feature_dim = spatial_features.shape\n\n        if temporal_features is not None:\n            # Process temporal dimension\n            attended_temporal = self.temporal_transformer(temporal_features)\n\n            # Aggregate temporal information\n            temporal_summary = attended_temporal.mean(dim=1, keepdim=True)  # (B, 1, F)\n\n            # Add temporal context to spatial features\n            spatial_with_temporal = spatial_features + temporal_summary\n        else:\n            spatial_with_temporal = spatial_features\n\n        # Apply spatial attention\n        attended_spatial, attention_weights = self.spatial_attention(\n            spatial_with_temporal, spatial_with_temporal, spatial_with_temporal\n        )\n\n        return attended_spatial, attention_weights\n'})}),"\n",(0,s.jsx)(n.h2,{id:"multimodal-object-detection",children:"Multimodal Object Detection"}),"\n",(0,s.jsx)(n.h3,{id:"vision-language-object-detection",children:"Vision-Language Object Detection"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class VisionLanguageObjectDetector(nn.Module):\n    def __init__(self, num_classes=80):\n        super().__init__()\n\n        # Vision backbone\n        from torchvision.models.detection import fasterrcnn_resnet50_fpn\n        self.vision_detector = fasterrcnn_resnet50_fpn(pretrained=True)\n\n        # Language encoder for class descriptions\n        self.language_encoder = LanguageEncoder()\n\n        # Fusion module for vision-language grounding\n        self.grounding_module = nn.Sequential(\n            nn.Linear(1024, 512),  # Assuming concatenated features\n            nn.ReLU(),\n            nn.Linear(512, 1),\n            nn.Sigmoid()\n        )\n\n        self.num_classes = num_classes\n\n    def forward(self, images, class_descriptions=None):\n        # Get vision-based detections\n        vision_detections = self.vision_detector(images)\n\n        if class_descriptions is not None:\n            # Encode class descriptions\n            class_features = self.language_encoder(class_descriptions)\n\n            # Ground detections with language\n            grounded_detections = self.ground_detections(\n                vision_detections, class_features\n            )\n\n            return grounded_detections\n\n        return vision_detections\n\n    def ground_detections(self, vision_detections, class_features):\n        """Ground vision detections with language descriptions"""\n        results = []\n\n        for detection in vision_detections:\n            boxes = detection[\'boxes\']  # (N, 4)\n            labels = detection[\'labels\']  # (N,)\n            scores = detection[\'scores\']  # (N,)\n\n            # For each detection, compute grounding score with class descriptions\n            grounded_scores = []\n            for i in range(len(boxes)):\n                # Get region features (simplified - in practice, extract from backbone)\n                region_features = self.extract_region_features(boxes[i])\n\n                # Compute grounding score\n                grounding_score = self.compute_grounding_score(\n                    region_features, class_features\n                )\n\n                grounded_scores.append(grounding_score)\n\n            # Update scores with grounding information\n            detection[\'grounding_scores\'] = torch.tensor(grounded_scores)\n            results.append(detection)\n\n        return results\n\n    def extract_region_features(self, box):\n        """Extract features for a specific region (simplified)"""\n        # In practice, this would use ROI pooling on the backbone features\n        return torch.randn(512)  # Placeholder\n\n    def compute_grounding_score(self, region_features, class_features):\n        """Compute grounding score between region and class descriptions"""\n        # Compute similarity (dot product or cosine similarity)\n        similarity = torch.dot(region_features, class_features[0])  # Simplified\n        return torch.sigmoid(similarity)\n'})}),"\n",(0,s.jsx)(n.h2,{id:"multimodal-perception-node",children:"Multimodal Perception Node"}),"\n",(0,s.jsx)(n.h3,{id:"ros-2-integration",children:"ROS 2 Integration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, PointCloud2\nfrom geometry_msgs.msg import Point\nfrom std_msgs.msg import String\nfrom vision_msgs.msg import Detection2DArray, Detection2D\nfrom cv_bridge import CvBridge\nimport numpy as np\n\nclass MultimodalPerceptionNode(Node):\n    def __init__(self):\n        super().__init__(\'multimodal_perception_node\')\n\n        # Initialize CV bridge\n        self.cv_bridge = CvBridge()\n\n        # Publishers and subscribers\n        self.rgb_sub = self.create_subscription(\n            Image, \'/camera/rgb/image_raw\', self.rgb_callback, 10\n        )\n        self.depth_sub = self.create_subscription(\n            Image, \'/camera/depth/image_raw\', self.depth_callback, 10\n        )\n        self.pointcloud_sub = self.create_subscription(\n            PointCloud2, \'/points2\', self.pointcloud_callback, 10\n        )\n        self.command_sub = self.create_subscription(\n            String, \'/perception_command\', self.command_callback, 10\n        )\n\n        self.detection_pub = self.create_publisher(\n            Detection2DArray, \'/multimodal_detections\', 10\n        )\n\n        # Initialize multimodal perception system\n        self.perception_system = MultimodalSensorFusion()\n\n        # Storage for sensor data\n        self.latest_rgb = None\n        self.latest_depth = None\n        self.latest_command = None\n\n    def rgb_callback(self, msg):\n        """Process RGB image"""\n        try:\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding=\'bgr8\')\n            # Convert to tensor and preprocess\n            transform = T.Compose([\n                T.Resize((224, 224)),\n                T.ToTensor(),\n                T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n            ])\n            self.latest_rgb = transform(cv_image).unsqueeze(0)\n\n            # Process if we have command\n            if self.latest_command:\n                self.process_multimodal_request()\n\n        except Exception as e:\n            self.get_logger().error(f\'Error processing RGB image: {e}\')\n\n    def depth_callback(self, msg):\n        """Process depth image"""\n        try:\n            cv_depth = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding=\'32FC1\')\n            # Convert to tensor\n            depth_tensor = torch.from_numpy(cv_depth).unsqueeze(0).unsqueeze(0).float()\n            self.latest_depth = depth_tensor\n\n            # Process if we have other data\n            if self.latest_rgb is not None and self.latest_command is not None:\n                self.process_multimodal_request()\n\n        except Exception as e:\n            self.get_logger().error(f\'Error processing depth image: {e}\')\n\n    def pointcloud_callback(self, msg):\n        """Process point cloud data"""\n        # Extract 3D information from point cloud\n        # This would typically involve segmentation and object detection\n        pass\n\n    def command_callback(self, msg):\n        """Process perception command"""\n        self.latest_command = msg.data\n\n        # Process if we have sensor data\n        if self.latest_rgb is not None:\n            self.process_multimodal_request()\n\n    def process_multimodal_request(self):\n        """Process multimodal perception request"""\n        if (self.latest_rgb is None or\n            self.latest_depth is None or\n            self.latest_command is None):\n            return\n\n        try:\n            # Prepare sensor data\n            sensors = {\n                \'rgb\': self.latest_rgb,\n                \'depth\': self.latest_depth\n            }\n\n            # Process with multimodal perception system\n            perception_results = self.perception_system(\n                sensors, self.latest_command\n            )\n\n            # Publish results\n            self.publish_perception_results(perception_results)\n\n            # Clear command after processing\n            self.latest_command = None\n\n        except Exception as e:\n            self.get_logger().error(f\'Error in multimodal processing: {e}\')\n\n    def publish_perception_results(self, results):\n        """Publish perception results"""\n        detection_array = Detection2DArray()\n        detection_array.header.stamp = self.get_clock().now().to_msg()\n        detection_array.header.frame_id = \'camera_rgb_optical_frame\'\n\n        # Convert results to Detection2DArray message\n        # This would depend on your specific output format\n        for key, value in results.items():\n            if key.startswith(\'vision\'):\n                # Create detection message\n                detection = Detection2D()\n                # Fill in detection details based on your results\n                detection_array.detections.append(detection)\n\n        self.detection_pub.publish(detection_array)\n'})}),"\n",(0,s.jsx)(n.h2,{id:"scene-understanding-and-grounding",children:"Scene Understanding and Grounding"}),"\n",(0,s.jsx)(n.h3,{id:"multimodal-scene-graph",children:"Multimodal Scene Graph"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class MultimodalSceneGraph:\n    def __init__(self):\n        self.objects = {}\n        self.relations = []\n        self.attributes = {}\n\n    def add_object(self, obj_id, visual_features, language_description):\n        \"\"\"Add object with visual and language features\"\"\"\n        self.objects[obj_id] = {\n            'visual_features': visual_features,\n            'language_description': language_description,\n            'bbox': None,\n            'category': None\n        }\n\n    def add_relation(self, obj1_id, obj2_id, relation_type, confidence=1.0):\n        \"\"\"Add relation between objects\"\"\"\n        self.relations.append({\n            'subject': obj1_id,\n            'object': obj2_id,\n            'relation': relation_type,\n            'confidence': confidence\n        })\n\n    def ground_language_in_scene(self, language_query):\n        \"\"\"Ground language query in the scene\"\"\"\n        # Match language descriptions to objects\n        matches = []\n\n        for obj_id, obj_data in self.objects.items():\n            # Compute similarity between query and object description\n            similarity = self.compute_language_similarity(\n                language_query, obj_data['language_description']\n            )\n\n            if similarity > 0.5:  # Threshold\n                matches.append({\n                    'object_id': obj_id,\n                    'similarity': similarity,\n                    'bbox': obj_data['bbox']\n                })\n\n        return sorted(matches, key=lambda x: x['similarity'], reverse=True)\n\n    def compute_language_similarity(self, query, description):\n        \"\"\"Compute similarity between language query and description\"\"\"\n        # This would use a language model or embedding similarity\n        # For now, return a simple similarity score\n        query_words = set(query.lower().split())\n        desc_words = set(description.lower().split())\n\n        intersection = len(query_words.intersection(desc_words))\n        union = len(query_words.union(desc_words))\n\n        return intersection / union if union > 0 else 0.0\n\nclass SceneUnderstandingModule(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        # Object detection and segmentation\n        self.object_detector = VisionLanguageObjectDetector()\n\n        # Spatial reasoning module\n        self.spatial_reasoner = SpatioTemporalPerceiver()\n\n        # Scene graph builder\n        self.scene_graph = MultimodalSceneGraph()\n\n    def forward(self, images, language_query):\n        \"\"\"Process image and language to build scene understanding\"\"\"\n        # Detect objects in image\n        detections = self.object_detector(images)\n\n        # Extract visual features for detected objects\n        visual_features = self.extract_object_features(images, detections)\n\n        # Build scene graph\n        for i, detection in enumerate(detections):\n            obj_id = f'obj_{i}'\n            self.scene_graph.add_object(\n                obj_id,\n                visual_features[i],\n                detection.get('description', 'unknown')\n            )\n\n        # Ground language query in scene\n        grounded_objects = self.scene_graph.ground_language_in_scene(language_query)\n\n        return {\n            'detections': detections,\n            'grounded_objects': grounded_objects,\n            'scene_graph': self.scene_graph\n        }\n\n    def extract_object_features(self, images, detections):\n        \"\"\"Extract features for detected objects\"\"\"\n        # This would typically use ROI pooling or similar techniques\n        features = []\n        for detection in detections:\n            # Extract features for each detected object\n            features.append(torch.randn(512))  # Placeholder\n        return features\n"})}),"\n",(0,s.jsx)(n.h2,{id:"exercise-tasks",children:"Exercise Tasks"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Implement the multimodal sensor fusion architecture"}),"\n",(0,s.jsx)(n.li,{children:"Create cross-modal attention mechanisms for vision-language integration"}),"\n",(0,s.jsx)(n.li,{children:"Build a spatio-temporal perception module"}),"\n",(0,s.jsx)(n.li,{children:"Implement vision-language object detection"}),"\n",(0,s.jsx)(n.li,{children:"Create a ROS 2 node for multimodal perception"}),"\n",(0,s.jsx)(n.li,{children:"Test the system with various sensor inputs and language queries"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"evaluation-metrics",children:"Evaluation Metrics"}),"\n",(0,s.jsx)(n.h3,{id:"multimodal-perception-evaluation",children:"Multimodal Perception Evaluation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class MultimodalEvaluation:\n    def __init__(self):\n        self.metrics = {\n            \'vision_grounding_accuracy\': [],\n            \'language_alignment_score\': [],\n            \'spatial_reasoning_accuracy\': [],\n            \'temporal_consistency\': []\n        }\n\n    def evaluate_vision_grounding(self, predicted_objects, ground_truth_objects):\n        """Evaluate how well vision detections align with language"""\n        correct = 0\n        total = len(ground_truth_objects)\n\n        for gt_obj in ground_truth_objects:\n            for pred_obj in predicted_objects:\n                if self.objects_match(gt_obj, pred_obj):\n                    correct += 1\n                    break\n\n        accuracy = correct / total if total > 0 else 0\n        self.metrics[\'vision_grounding_accuracy\'].append(accuracy)\n        return accuracy\n\n    def evaluate_language_alignment(self, vision_features, language_features):\n        """Evaluate alignment between vision and language features"""\n        # Compute similarity score\n        similarity = torch.cosine_similarity(\n            vision_features.flatten(),\n            language_features.flatten(),\n            dim=0\n        )\n        score = similarity.item()\n        self.metrics[\'language_alignment_score\'].append(score)\n        return score\n\n    def objects_match(self, obj1, obj2):\n        """Check if two objects match based on spatial overlap and semantic similarity"""\n        # Check spatial overlap (IoU)\n        iou = self.calculate_iou(obj1[\'bbox\'], obj2[\'bbox\'])\n\n        # Check semantic similarity\n        semantic_sim = self.calculate_semantic_similarity(\n            obj1[\'description\'],\n            obj2[\'description\']\n        )\n\n        return iou > 0.5 and semantic_sim > 0.7\n\n    def calculate_iou(self, bbox1, bbox2):\n        """Calculate Intersection over Union"""\n        # Implementation of IoU calculation\n        pass\n\n    def calculate_semantic_similarity(self, desc1, desc2):\n        """Calculate semantic similarity between descriptions"""\n        # Use language model embeddings or simple word overlap\n        pass\n'})}),"\n",(0,s.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,s.jsx)(n.h3,{id:"common-issues",children:"Common Issues"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Feature dimension mismatches"}),": Ensure all modalities project to compatible dimensions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Memory issues"}),": Use appropriate batch sizes for multimodal processing"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Synchronization problems"}),": Ensure sensor data alignment in time"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Grounding failures"}),": Improve language-vision alignment training"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:"In this lab, you implemented multimodal perception systems that integrate vision, language, and other sensory inputs. You created cross-modal attention mechanisms, built scene understanding modules, and integrated the system with ROS 2. This enables robots to understand complex scenes by combining multiple sensory modalities with natural language understanding."})]})}function u(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}}}]);