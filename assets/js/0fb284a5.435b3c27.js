"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[5903],{2367(e,n,i){i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>p,frontMatter:()=>o,metadata:()=>a,toc:()=>l});var s=i(4848),t=i(8453);const o={sidebar_label:"Lab 3.2: Perception Systems"},r="Lab Exercise 3.2: Perception Systems in AI-Robot Brain",a={id:"modules/lab-exercises/lab-3-2-perception-systems",title:"Lab Exercise 3.2: Perception Systems in AI-Robot Brain",description:"This lab exercise covers implementing perception systems for robot AI using NVIDIA Isaac.",source:"@site/docs/modules/lab-exercises/lab-3-2-perception-systems.md",sourceDirName:"modules/lab-exercises",slug:"/modules/lab-exercises/lab-3-2-perception-systems",permalink:"/hackathon-ai-book/modules/lab-exercises/lab-3-2-perception-systems",draft:!1,unlisted:!1,editUrl:"https://github.com/sanilahmed/hackathon-ai-book/tree/main/docs/modules/lab-exercises/lab-3-2-perception-systems.md",tags:[],version:"current",frontMatter:{sidebar_label:"Lab 3.2: Perception Systems"},sidebar:"tutorialSidebar",previous:{title:"Lab 3.1: Isaac Sim Setup",permalink:"/hackathon-ai-book/modules/lab-exercises/lab-3-1-isaac-sim-setup"},next:{title:"Lab 3.3: Planning and Control",permalink:"/hackathon-ai-book/modules/lab-exercises/lab-3-3-planning-control"}},c={},l=[{value:"Objectives",id:"objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Perception System Overview",id:"perception-system-overview",level:2},{value:"Key Components",id:"key-components",level:3},{value:"Perception Pipeline",id:"perception-pipeline",level:3},{value:"Sensor Setup in Isaac Sim",id:"sensor-setup-in-isaac-sim",level:2},{value:"Camera Configuration",id:"camera-configuration",level:3},{value:"LIDAR Configuration",id:"lidar-configuration",level:3},{value:"Isaac ROS Perception Integration",id:"isaac-ros-perception-integration",level:2},{value:"Isaac ROS Sensor Bridge",id:"isaac-ros-sensor-bridge",level:3},{value:"Computer Vision Processing",id:"computer-vision-processing",level:2},{value:"Object Detection Node",id:"object-detection-node",level:3},{value:"NVIDIA Isaac Perception Tools",id:"nvidia-isaac-perception-tools",level:2},{value:"Isaac ROS Perception Packages",id:"isaac-ros-perception-packages",level:3},{value:"3D Perception and Point Cloud Processing",id:"3d-perception-and-point-cloud-processing",level:2},{value:"Point Cloud Processing Node",id:"point-cloud-processing-node",level:3},{value:"Perception Performance Optimization",id:"perception-performance-optimization",level:2},{value:"GPU-Accelerated Processing",id:"gpu-accelerated-processing",level:3},{value:"Perception Evaluation",id:"perception-evaluation",level:2},{value:"Accuracy Metrics Node",id:"accuracy-metrics-node",level:3},{value:"Exercise Tasks",id:"exercise-tasks",level:2},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues",id:"common-issues",level:3},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"lab-exercise-32-perception-systems-in-ai-robot-brain",children:"Lab Exercise 3.2: Perception Systems in AI-Robot Brain"}),"\n",(0,s.jsx)(n.p,{children:"This lab exercise covers implementing perception systems for robot AI using NVIDIA Isaac."}),"\n",(0,s.jsx)(n.h2,{id:"objectives",children:"Objectives"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Set up perception sensors in simulation"}),"\n",(0,s.jsx)(n.li,{children:"Implement computer vision pipelines"}),"\n",(0,s.jsx)(n.li,{children:"Integrate perception with ROS 2"}),"\n",(0,s.jsx)(n.li,{children:"Test perception accuracy and performance"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Isaac Sim installed and configured"}),"\n",(0,s.jsx)(n.li,{children:"ROS 2 Humble with perception packages"}),"\n",(0,s.jsx)(n.li,{children:"Basic Python and OpenCV knowledge"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"perception-system-overview",children:"Perception System Overview"}),"\n",(0,s.jsx)(n.h3,{id:"key-components",children:"Key Components"}),"\n",(0,s.jsx)(n.p,{children:"A perception system typically includes:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cameras"}),": RGB, depth, stereo"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LIDAR"}),": 2D and 3D scanning"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"IMU"}),": Inertial measurement"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Other sensors"}),": Force/torque, tactile, etc."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"perception-pipeline",children:"Perception Pipeline"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Raw Sensor Data \u2192 Preprocessing \u2192 Feature Extraction \u2192 Object Detection \u2192 Decision Making\n"})}),"\n",(0,s.jsx)(n.h2,{id:"sensor-setup-in-isaac-sim",children:"Sensor Setup in Isaac Sim"}),"\n",(0,s.jsx)(n.h3,{id:"camera-configuration",children:"Camera Configuration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from omni.isaac.core.utils.prims import get_prim_at_path, define_prim\nfrom omni.isaac.sensor import Camera\nfrom pxr import Gf\nimport numpy as np\n\nclass CameraSetup:\n    def __init__(self, prim_path, position, orientation):\n        self.prim_path = prim_path\n        self.position = position\n        self.orientation = orientation\n\n        # Create camera prim\n        define_prim(prim_path, "Camera")\n        self.camera = Camera(\n            prim_path=prim_path,\n            frequency=30,  # Hz\n            resolution=(640, 480)\n        )\n\n        # Set camera properties\n        self.camera.set_position(position)\n        self.camera.set_orientation(orientation)\n\n    def get_rgb_image(self):\n        return self.camera.get_rgb()\n\n    def get_depth_image(self):\n        return self.camera.get_depth()\n\n    def get_point_cloud(self):\n        return self.camera.get_point_cloud()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"lidar-configuration",children:"LIDAR Configuration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from omni.isaac.range_sensor import LidarRtx\nfrom pxr import Gf\n\nclass LidarSetup:\n    def __init__(self, prim_path, position, orientation):\n        self.lidar = LidarRtx(\n            prim_path=prim_path,\n            translation=position,\n            orientation=orientation,\n            config="Example_Rotary",\n            rotation_frequency=20,\n            horizontal_samples=1080,\n            vertical_samples=32,\n            horizontal_fov=360,\n            vertical_fov=45\n        )\n\n        # Enable different types of data\n        self.lidar.add_ground_truth_to_frame()\n        self.lidar.add_position_channels_to_frame()\n\n    def get_lidar_data(self):\n        return self.lidar.get_sensor_reading()\n\n    def get_point_cloud(self):\n        return self.lidar.get_point_cloud()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"isaac-ros-perception-integration",children:"Isaac ROS Perception Integration"}),"\n",(0,s.jsx)(n.h3,{id:"isaac-ros-sensor-bridge",children:"Isaac ROS Sensor Bridge"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo, PointCloud2, LaserScan\nfrom cv_bridge import CvBridge\nimport numpy as np\n\nclass IsaacPerceptionBridge(Node):\n    def __init__(self):\n        super().__init__('isaac_perception_bridge')\n\n        # Initialize CV bridge\n        self.cv_bridge = CvBridge()\n\n        # Publishers for different sensor types\n        self.rgb_pub = self.create_publisher(Image, '/camera/rgb/image_raw', 10)\n        self.depth_pub = self.create_publisher(Image, '/camera/depth/image_raw', 10)\n        self.camera_info_pub = self.create_publisher(CameraInfo, '/camera/rgb/camera_info', 10)\n        self.lidar_pub = self.create_publisher(LaserScan, '/scan', 10)\n\n        # Timer for publishing sensor data\n        self.timer = self.create_timer(0.033, self.publish_sensor_data)  # ~30Hz\n\n        # Isaac Sim sensor references\n        self.isaac_camera = None\n        self.isaac_lidar = None\n\n    def publish_sensor_data(self):\n        # Publish RGB image\n        if self.isaac_camera:\n            rgb_image = self.isaac_camera.get_rgb()\n            if rgb_image is not None:\n                ros_image = self.cv_bridge.cv2_to_imgmsg(rgb_image, encoding=\"bgr8\")\n                ros_image.header.stamp = self.get_clock().now().to_msg()\n                ros_image.header.frame_id = \"camera_rgb_optical_frame\"\n                self.rgb_pub.publish(ros_image)\n\n        # Publish depth image\n        if self.isaac_camera:\n            depth_image = self.isaac_camera.get_depth()\n            if depth_image is not None:\n                ros_depth = self.cv_bridge.cv2_to_imgmsg(depth_image, encoding=\"32FC1\")\n                ros_depth.header.stamp = self.get_clock().now().to_msg()\n                ros_depth.header.frame_id = \"camera_depth_optical_frame\"\n                self.depth_pub.publish(ros_depth)\n\n        # Publish LIDAR scan\n        if self.isaac_lidar:\n            lidar_data = self.isaac_lidar.get_lidar_data()\n            if lidar_data:\n                scan_msg = self.create_laser_scan_msg(lidar_data)\n                self.lidar_pub.publish(scan_msg)\n\n    def create_laser_scan_msg(self, lidar_data):\n        scan = LaserScan()\n        scan.header.stamp = self.get_clock().now().to_msg()\n        scan.header.frame_id = \"laser_frame\"\n        scan.angle_min = -np.pi\n        scan.angle_max = np.pi\n        scan.angle_increment = 2 * np.pi / len(lidar_data)\n        scan.time_increment = 0.0\n        scan.scan_time = 0.1\n        scan.range_min = 0.1\n        scan.range_max = 50.0\n        scan.ranges = lidar_data\n        return scan\n"})}),"\n",(0,s.jsx)(n.h2,{id:"computer-vision-processing",children:"Computer Vision Processing"}),"\n",(0,s.jsx)(n.h3,{id:"object-detection-node",children:"Object Detection Node"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom vision_msgs.msg import Detection2DArray, Detection2D, ObjectHypothesisWithPose\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\n\nclass ObjectDetectionNode(Node):\n    def __init__(self):\n        super().__init__('object_detection_node')\n\n        self.cv_bridge = CvBridge()\n\n        # Subscribe to camera image\n        self.image_sub = self.create_subscription(\n            Image, '/camera/rgb/image_raw', self.image_callback, 10\n        )\n\n        # Publish detection results\n        self.detection_pub = self.create_publisher(\n            Detection2DArray, '/object_detections', 10\n        )\n\n        # Initialize detection model (using OpenCV DNN as example)\n        # In practice, you'd use a trained model like YOLO or SSD\n        self.detector = self.initialize_detector()\n\n    def initialize_detector(self):\n        # This would initialize your object detection model\n        # For example, using OpenCV's DNN module with a pre-trained model\n        return None  # Placeholder\n\n    def image_callback(self, msg):\n        # Convert ROS image to OpenCV\n        cv_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n\n        # Perform object detection\n        detections = self.detect_objects(cv_image)\n\n        # Publish results\n        detection_msg = self.create_detection_message(detections, msg.header)\n        self.detection_pub.publish(detection_msg)\n\n    def detect_objects(self, image):\n        # Perform object detection on the image\n        # This is a simplified example - in practice, you'd use a trained model\n        detections = []\n\n        # Example: detect colored objects\n        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n        # Define color ranges for detection\n        lower_red = np.array([0, 50, 50])\n        upper_red = np.array([10, 255, 255])\n        mask_red = cv2.inRange(hsv, lower_red, upper_red)\n\n        # Find contours\n        contours, _ = cv2.findContours(mask_red, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        for contour in contours:\n            area = cv2.contourArea(contour)\n            if area > 100:  # Filter small contours\n                x, y, w, h = cv2.boundingRect(contour)\n                detections.append({\n                    'class': 'red_object',\n                    'confidence': 0.8,\n                    'bbox': (x, y, w, h)\n                })\n\n        return detections\n\n    def create_detection_message(self, detections, header):\n        detection_array = Detection2DArray()\n        detection_array.header = header\n\n        for det in detections:\n            detection = Detection2D()\n            detection.header = header\n\n            # Set bounding box\n            bbox = detection.bbox\n            bbox.center.x = det['bbox'][0] + det['bbox'][2] / 2\n            bbox.center.y = det['bbox'][1] + det['bbox'][3] / 2\n            bbox.size_x = det['bbox'][2]\n            bbox.size_y = det['bbox'][3]\n\n            # Set hypothesis\n            hypothesis = ObjectHypothesisWithPose()\n            hypothesis.hypothesis.class_id = det['class']\n            hypothesis.hypothesis.score = det['confidence']\n            detection.results.append(hypothesis)\n\n            detection_array.detections.append(detection)\n\n        return detection_array\n"})}),"\n",(0,s.jsx)(n.h2,{id:"nvidia-isaac-perception-tools",children:"NVIDIA Isaac Perception Tools"}),"\n",(0,s.jsx)(n.h3,{id:"isaac-ros-perception-packages",children:"Isaac ROS Perception Packages"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Example of using Isaac ROS perception packages\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, PointCloud2\nfrom stereo_msgs.msg import DisparityImage\nfrom message_filters import ApproximateTimeSynchronizer, Subscriber\n\nclass IsaacPerceptionPipeline(Node):\n    def __init__(self):\n        super().__init__('isaac_perception_pipeline')\n\n        # Subscribe to stereo pair\n        left_sub = Subscriber(self, Image, '/camera/left/image_rect_color')\n        right_sub = Subscriber(self, Image, '/camera/right/image_rect_color')\n\n        # Synchronize stereo images\n        ts = ApproximateTimeSynchronizer(\n            [left_sub, right_sub],\n            queue_size=10,\n            slop=0.1\n        )\n        ts.registerCallback(self.stereo_callback)\n\n        # Publishers\n        self.disparity_pub = self.create_publisher(DisparityImage, '/disparity', 10)\n        self.pointcloud_pub = self.create_publisher(PointCloud2, '/points2', 10)\n\n    def stereo_callback(self, left_msg, right_msg):\n        # Process stereo images to generate disparity and point cloud\n        # This would use Isaac's optimized stereo processing\n        pass\n"})}),"\n",(0,s.jsx)(n.h2,{id:"3d-perception-and-point-cloud-processing",children:"3D Perception and Point Cloud Processing"}),"\n",(0,s.jsx)(n.h3,{id:"point-cloud-processing-node",children:"Point Cloud Processing Node"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import PointCloud2, PointField\nfrom std_msgs.msg import Header\nimport numpy as np\nimport sensor_msgs_py.point_cloud2 as pc2\n\nclass PointCloudProcessor(Node):\n    def __init__(self):\n        super().__init__('point_cloud_processor')\n\n        # Subscribe to point cloud\n        self.pc_sub = self.create_subscription(\n            PointCloud2, '/points2', self.pc_callback, 10\n        )\n\n        # Publisher for processed point cloud\n        self.processed_pc_pub = self.create_publisher(\n            PointCloud2, '/processed_points', 10\n        )\n\n    def pc_callback(self, msg):\n        # Convert PointCloud2 to numpy array\n        points_list = []\n        for point in pc2.read_points(msg, field_names=(\"x\", \"y\", \"z\"), skip_nans=True):\n            points_list.append([point[0], point[1], point[2]])\n\n        points = np.array(points_list)\n\n        # Process point cloud (example: ground plane removal)\n        processed_points = self.remove_ground_plane(points)\n\n        # Convert back to PointCloud2 and publish\n        header = Header()\n        header.stamp = self.get_clock().now().to_msg()\n        header.frame_id = msg.header.frame_id\n\n        fields = [\n            PointField(name='x', offset=0, datatype=PointField.FLOAT32, count=1),\n            PointField(name='y', offset=4, datatype=PointField.FLOAT32, count=1),\n            PointField(name='z', offset=8, datatype=PointField.FLOAT32, count=1)\n        ]\n\n        processed_msg = pc2.create_cloud(header, fields, processed_points)\n        self.processed_pc_pub.publish(processed_msg)\n\n    def remove_ground_plane(self, points):\n        # Simple ground plane removal using RANSAC or height threshold\n        # This is a simplified example\n        ground_height = 0.1  # Adjust based on robot height\n        return points[points[:, 2] > ground_height]\n"})}),"\n",(0,s.jsx)(n.h2,{id:"perception-performance-optimization",children:"Perception Performance Optimization"}),"\n",(0,s.jsx)(n.h3,{id:"gpu-accelerated-processing",children:"GPU-Accelerated Processing"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import torch\nimport torchvision.transforms as T\nfrom PIL import Image as PILImage\n\nclass GPUPerceptionProcessor:\n    def __init__(self):\n        # Check for GPU availability\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n        # Load models to GPU\n        self.detection_model = self.load_detection_model().to(self.device)\n        self.segmentation_model = self.load_segmentation_model().to(self.device)\n\n        # Set models to evaluation mode\n        self.detection_model.eval()\n        self.segmentation_model.eval()\n\n    def load_detection_model(self):\n        # Load a pre-trained detection model (e.g., YOLO, SSD)\n        # This would be your specific model\n        return None  # Placeholder\n\n    def load_segmentation_model(self):\n        # Load a pre-trained segmentation model\n        return None  # Placeholder\n\n    def process_image_gpu(self, image):\n        # Convert image to tensor and move to GPU\n        transform = T.Compose([T.ToTensor()])\n        image_tensor = transform(image).unsqueeze(0).to(self.device)\n\n        # Run inference\n        with torch.no_grad():\n            detection_results = self.detection_model(image_tensor)\n            segmentation_results = self.segmentation_model(image_tensor)\n\n        return detection_results, segmentation_results\n"})}),"\n",(0,s.jsx)(n.h2,{id:"perception-evaluation",children:"Perception Evaluation"}),"\n",(0,s.jsx)(n.h3,{id:"accuracy-metrics-node",children:"Accuracy Metrics Node"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom vision_msgs.msg import Detection2DArray\nfrom std_msgs.msg import Float32\nimport numpy as np\n\nclass PerceptionEvaluator(Node):\n    def __init__(self):\n        super().__init__('perception_evaluator')\n\n        # Subscribers\n        self.detection_sub = self.create_subscription(\n            Detection2DArray, '/object_detections', self.detection_callback, 10\n        )\n        self.ground_truth_sub = self.create_subscription(\n            Detection2DArray, '/ground_truth', self.ground_truth_callback, 10\n        )\n\n        # Publishers for metrics\n        self.precision_pub = self.create_publisher(Float32, '/perception_precision', 10)\n        self.recall_pub = self.create_publisher(Float32, '/perception_recall', 10)\n\n        # Storage for evaluation\n        self.ground_truth = None\n        self.detection_results = None\n\n    def detection_callback(self, msg):\n        self.detection_results = msg\n        if self.ground_truth is not None:\n            self.evaluate_perception()\n\n    def ground_truth_callback(self, msg):\n        self.ground_truth = msg\n\n    def evaluate_perception(self):\n        # Calculate precision and recall\n        # This is a simplified evaluation\n        if not self.ground_truth.detections or not self.detection_results.detections:\n            return\n\n        # Calculate IoU between detections and ground truth\n        true_positives = 0\n        false_positives = 0\n        false_negatives = 0\n\n        # Simple evaluation logic (in practice, you'd use proper matching algorithms)\n        for det in self.detection_results.detections:\n            matched = False\n            for gt in self.ground_truth.detections:\n                if self.calculate_iou(det.bbox, gt.bbox) > 0.5:\n                    true_positives += 1\n                    matched = True\n                    break\n            if not matched:\n                false_positives += 1\n\n        false_negatives = len(self.ground_truth.detections) - true_positives\n\n        # Calculate metrics\n        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n\n        # Publish metrics\n        precision_msg = Float32()\n        precision_msg.data = precision\n        self.precision_pub.publish(precision_msg)\n\n        recall_msg = Float32()\n        recall_msg.data = recall\n        self.recall_pub.publish(recall_msg)\n\n    def calculate_iou(self, bbox1, bbox2):\n        # Calculate Intersection over Union\n        # Simplified implementation\n        return 0.0  # Placeholder\n"})}),"\n",(0,s.jsx)(n.h2,{id:"exercise-tasks",children:"Exercise Tasks"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Set up camera and LIDAR sensors in Isaac Sim"}),"\n",(0,s.jsx)(n.li,{children:"Implement a ROS 2 node to bridge Isaac Sim sensor data"}),"\n",(0,s.jsx)(n.li,{children:"Create an object detection node that processes camera images"}),"\n",(0,s.jsx)(n.li,{children:"Implement point cloud processing for 3D perception"}),"\n",(0,s.jsx)(n.li,{children:"Set up GPU-accelerated perception processing"}),"\n",(0,s.jsx)(n.li,{children:"Create an evaluation node to measure perception accuracy"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,s.jsx)(n.h3,{id:"common-issues",children:"Common Issues"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Sensor data not publishing"}),": Check Isaac Sim extension settings"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Performance issues"}),": Monitor GPU and CPU usage"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Synchronization problems"}),": Verify time stamps and QoS settings"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Memory issues"}),": Process data in batches or reduce resolution"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:"In this lab, you learned to set up and implement perception systems for robot AI. You configured sensors in Isaac Sim, created ROS 2 bridges, implemented computer vision pipelines, and evaluated perception performance. These skills are essential for building intelligent robotic systems."})]})}function p(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453(e,n,i){i.d(n,{R:()=>r,x:()=>a});var s=i(6540);const t={},o=s.createContext(t);function r(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);